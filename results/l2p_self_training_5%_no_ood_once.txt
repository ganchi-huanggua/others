(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2023-12-20 20:05:47,024 [trainer.py] => config: ./exps/l2p_self_training.json
2023-12-20 20:05:47,024 [trainer.py] => prefix:  
2023-12-20 20:05:47,024 [trainer.py] => dataset: cifar224
2023-12-20 20:05:47,024 [trainer.py] => memory_size: 0
2023-12-20 20:05:47,024 [trainer.py] => memory_per_class: 0
2023-12-20 20:05:47,024 [trainer.py] => fixed_memory: False
2023-12-20 20:05:47,024 [trainer.py] => shuffle: True
2023-12-20 20:05:47,024 [trainer.py] => init_cls: 10
2023-12-20 20:05:47,024 [trainer.py] => increment: 10
2023-12-20 20:05:47,024 [trainer.py] => model_name: l2p_self_training
2023-12-20 20:05:47,024 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2023-12-20 20:05:47,024 [trainer.py] => get_original_backbone: True
2023-12-20 20:05:47,024 [trainer.py] => device: [device(type='cuda', index=5)]
2023-12-20 20:05:47,024 [trainer.py] => seed: 1993
2023-12-20 20:05:47,024 [trainer.py] => tuned_epoch: 5
2023-12-20 20:05:47,024 [trainer.py] => init_lr: 0.001875
2023-12-20 20:05:47,025 [trainer.py] => batch_size: 16
2023-12-20 20:05:47,025 [trainer.py] => weight_decay: 0
2023-12-20 20:05:47,025 [trainer.py] => min_lr: 1e-05
2023-12-20 20:05:47,025 [trainer.py] => optimizer: adam
2023-12-20 20:05:47,025 [trainer.py] => scheduler: constant
2023-12-20 20:05:47,025 [trainer.py] => reinit_optimizer: True
2023-12-20 20:05:47,025 [trainer.py] => global_pool: token
2023-12-20 20:05:47,025 [trainer.py] => head_type: prompt
2023-12-20 20:05:47,025 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2023-12-20 20:05:47,025 [trainer.py] => pretrained: True
2023-12-20 20:05:47,025 [trainer.py] => drop: 0.0
2023-12-20 20:05:47,025 [trainer.py] => drop_path: 0.0
2023-12-20 20:05:47,025 [trainer.py] => prompt_pool: True
2023-12-20 20:05:47,025 [trainer.py] => pool_size: 10
2023-12-20 20:05:47,025 [trainer.py] => length: 5
2023-12-20 20:05:47,025 [trainer.py] => top_k: 5
2023-12-20 20:05:47,025 [trainer.py] => initializer: uniform
2023-12-20 20:05:47,025 [trainer.py] => prompt_key: True
2023-12-20 20:05:47,025 [trainer.py] => prompt_key_init: uniform
2023-12-20 20:05:47,025 [trainer.py] => use_prompt_mask: False
2023-12-20 20:05:47,025 [trainer.py] => shared_prompt_pool: False
2023-12-20 20:05:47,025 [trainer.py] => shared_prompt_key: False
2023-12-20 20:05:47,025 [trainer.py] => batchwise_prompt: True
2023-12-20 20:05:47,025 [trainer.py] => embedding_key: cls
2023-12-20 20:05:47,025 [trainer.py] => predefined_key: 
2023-12-20 20:05:47,025 [trainer.py] => pull_constraint: True
2023-12-20 20:05:47,026 [trainer.py] => pull_constraint_coeff: 0.1
2023-12-20 20:05:47,026 [trainer.py] => semi_supervised_mode: True
2023-12-20 20:05:47,026 [trainer.py] => labeled_ratio: 0.05
2023-12-20 20:05:47,026 [trainer.py] => unlabeled_data_distribution_mode: no_ood
2023-12-20 20:05:47,026 [trainer.py] => confidence_threshold: 0.9
2023-12-20 20:05:47,026 [trainer.py] => seeing_once: True
Files already downloaded and verified
Files already downloaded and verified
2023-12-20 20:05:49,163 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-20 20:05:51,651 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2023-12-20 20:05:51,652 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2023-12-20 20:05:55,103 [l2p_self_training.py] => 85,940,836 model total parameters.
2023-12-20 20:05:55,104 [l2p_self_training.py] => 122,980 model training parameters.
2023-12-20 20:05:55,104 [l2p_self_training.py] => prompt.prompt: 38400
2023-12-20 20:05:55,104 [l2p_self_training.py] => prompt.prompt_key: 7680
2023-12-20 20:05:55,104 [l2p_self_training.py] => head.weight: 76800
2023-12-20 20:05:55,104 [l2p_self_training.py] => head.bias: 100
2023-12-20 20:05:55,105 [trainer.py] => All params: 171816392
2023-12-20 20:05:55,106 [trainer.py] => Trainable params: 122980
2023-12-20 20:05:55,106 [l2p_self_training.py] => Learning on 0-10
Task 0, Epoch 4/5, Self_training_Iteration: 0 => Loss 0.549, Train_accy 84.00:  80%|██████████████████████████████████████████████████████████████████████▍                 | 4/5 [00:11<00:02,  2.76s/it]tensor([8, 6, 2, 3, 3, 9, 2, 8, 1, 4, 4, 0, 6, 4, 4, 7], device='cuda:5')
tensor([9, 7, 4, 7, 2, 4, 9, 6, 2, 5, 3, 7, 5, 0, 8, 0], device='cuda:5')
tensor([2, 6, 3, 6, 6, 0, 0, 8, 5, 3, 8, 5, 2, 5, 9, 0], device='cuda:5')
tensor([5, 5, 3, 3, 1, 4, 0, 4, 3, 7, 6, 9, 6, 3, 0, 3], device='cuda:5')
tensor([1, 9, 5, 2, 5, 1, 7, 0, 0, 9, 3, 3, 5, 7, 5, 1], device='cuda:5')
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.03s/it]
2023-12-20 20:06:15,986 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40
2023-12-20 20:06:15,988 [l2p_self_training.py] => Here are still 4758 unlabeled samples left
2023-12-20 20:06:15,988 [l2p_self_training.py] => pseudo labeling start
2023-12-20 20:06:43,870 [l2p_self_training.py] => 1885 unlabeled samples will be pseudo labeled
2023-12-20 20:06:43,870 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9915119363395225
Task 0, Epoch 4/5, Self_training_Iteration: 1 => Loss 0.032, Train_accy 91.83:  80%|██████████████████████████████████████████████████████████████████████▍                 | 4/5 [01:10<00:17, 17.49s/it]tensor([1, 9, 9, 0, 6, 7, 3, 5, 9, 6, 5, 1, 7, 0, 1, 3], device='cuda:5')
tensor([1, 9, 0, 2, 7, 0, 2, 2, 4, 9, 6, 9, 7, 4, 8, 6], device='cuda:5')
tensor([3, 0, 0, 9, 8, 1, 4, 3, 8, 2, 2, 4, 0, 8, 9, 6], device='cuda:5')
tensor([5, 8, 6, 4, 6, 8, 6, 3, 2, 6, 4, 2, 0, 1, 7, 7], device='cuda:5')
tensor([6, 1, 4, 9, 8, 5, 7, 5, 1, 7, 5, 5, 3, 7, 3, 5], device='cuda:5')
Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.018, Train_accy 92.10, Test_accy 90.50: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [01:33<00:00, 18.78s/it]
2023-12-20 20:08:17,802 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.018, Train_accy 92.10, Test_accy 90.50
2023-12-20 20:08:17,806 [l2p_self_training.py] => Here are still 2873 unlabeled samples left
2023-12-20 20:08:17,806 [l2p_self_training.py] => pseudo labeling start
2023-12-20 20:08:35,203 [l2p_self_training.py] => 1602 unlabeled samples will be pseudo labeled
2023-12-20 20:08:35,203 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9069912609238452
Task 0, Epoch 4/5, Self_training_Iteration: 2 => Loss 0.218, Train_accy 83.27:  80%|██████████████████████████████████████████████████████████████████████▍                 | 4/5 [01:00<00:15, 15.12s/it]tensor([5, 3, 1, 1, 9, 2, 7, 4, 9, 3, 1, 6, 0, 7, 2, 2], device='cuda:5')
tensor([2, 9, 4, 8, 1, 2, 5, 0, 5, 4, 0, 3, 5, 1, 1, 6], device='cuda:5')
tensor([7, 6, 4, 1, 6, 1, 0, 7, 2, 1, 6, 0, 9, 9, 1, 6], device='cuda:5')
tensor([6, 8, 3, 7, 5, 4, 6, 6, 9, 2, 8, 1, 0, 0, 4, 5], device='cuda:5')
tensor([9, 4, 8, 5, 9, 3, 1, 3, 9, 5, 3, 0, 1, 4, 9, 4], device='cuda:5')
Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.182, Train_accy 84.21, Test_accy 90.80: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [01:21<00:00, 16.37s/it]
2023-12-20 20:09:57,088 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.182, Train_accy 84.21, Test_accy 90.80
2023-12-20 20:09:57,090 [l2p_self_training.py] => Here are still 1271 unlabeled samples left
2023-12-20 20:09:57,091 [l2p_self_training.py] => pseudo labeling start
2023-12-20 20:10:05,022 [l2p_self_training.py] => 558 unlabeled samples will be pseudo labeled
2023-12-20 20:10:05,023 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.910394265232975
Task 0, Epoch 4/5, Self_training_Iteration: 3 => Loss 0.123, Train_accy 84.95:  80%|██████████████████████████████████████████████████████████████████████▍                 | 4/5 [00:22<00:05,  5.56s/it]tensor([6, 7, 9, 9, 7, 3, 8, 9, 0, 8, 0, 5, 8, 7, 0, 1], device='cuda:5')
tensor([7, 2, 2, 9, 0, 7, 0, 3, 6, 3, 2, 0, 4, 5, 5, 8], device='cuda:5')
tensor([6, 0, 7, 2, 9, 8, 7, 1, 7, 4, 7, 5, 1, 0, 8, 2], device='cuda:5')
tensor([5, 8, 4, 6, 7, 4, 5, 6, 2, 5, 2, 4, 1, 2, 7, 0], device='cuda:5')
tensor([7, 1, 8, 7, 1, 6, 5, 1, 9, 7, 4, 7, 7, 3, 9, 0], device='cuda:5')
Task 0, Epoch 5/5, Self_training_Iteration: 3 => Loss 0.163, Train_accy 84.41, Test_accy 92.50: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:34<00:00,  6.84s/it]
2023-12-20 20:10:39,230 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 3 => Loss 0.163, Train_accy 84.41, Test_accy 92.50
2023-12-20 20:10:39,233 [l2p_self_training.py] => Here are still 713 unlabeled samples left
2023-12-20 20:10:39,233 [l2p_self_training.py] => pseudo labeling start
2023-12-20 20:10:43,966 [l2p_self_training.py] => 291 unlabeled samples will be pseudo labeled
2023-12-20 20:10:43,967 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.8934707903780069
Task 0, Epoch 4/5, Self_training_Iteration: 4 => Loss 0.181, Train_accy 83.16:  80%|██████████████████████████████████████████████████████████████████████▍                 | 4/5 [00:12<00:03,  3.16s/it]tensor([6, 5, 9, 3, 6, 4, 0, 6, 3, 7, 0, 1, 8, 5, 1, 3], device='cuda:5')
tensor([0, 2, 5, 5, 4, 8, 4, 6, 9, 9, 2, 7, 3, 0, 6, 1], device='cuda:5')
tensor([8, 5, 7, 7, 6, 2, 5, 8, 0, 6, 3, 9, 2, 4, 7, 5], device='cuda:5')
tensor([3, 7, 3, 6, 6, 8, 7, 7, 4, 3, 1, 5, 3, 4, 1, 6], device='cuda:5')
tensor([8, 9, 0, 3, 6, 9, 3, 2, 1, 3, 4, 5, 4, 8, 1, 4], device='cuda:5')
Task 0, Epoch 5/5, Self_training_Iteration: 4 => Loss 0.169, Train_accy 82.47, Test_accy 91.60: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:22<00:00,  4.43s/it]
2023-12-20 20:11:06,103 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 4 => Loss 0.169, Train_accy 82.47, Test_accy 91.60
2023-12-20 20:11:06,105 [l2p_self_training.py] => Here are still 422 unlabeled samples left
2023-12-20 20:11:06,105 [l2p_self_training.py] => pseudo labeling start
2023-12-20 20:11:09,068 [l2p_self_training.py] => 189 unlabeled samples will be pseudo labeled
2023-12-20 20:11:09,069 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.8412698412698413
2023-12-20 20:11:15,464 [trainer.py] => No NME accuracy.
2023-12-20 20:11:15,464 [trainer.py] => CNN: {'total': 91.6, '00-09': 91.6, 'old': 0, 'new': 91.6}
2023-12-20 20:11:15,464 [trainer.py] => CNN top1 curve: [91.6]
2023-12-20 20:11:15,464 [trainer.py] => CNN top5 curve: [99.3]

Average Accuracy (CNN): 91.6
2023-12-20 20:11:15,465 [trainer.py] => Average Accuracy (CNN): 91.6 

2023-12-20 20:11:15,466 [trainer.py] => All params: 171816392
2023-12-20 20:11:15,468 [trainer.py] => Trainable params: 122980
2023-12-20 20:11:15,468 [l2p_self_training.py] => Learning on 10-20
Task 1, Epoch 4/5, Self_training_Iteration: 0 => Loss 0.099, Train_accy 87.60:  80%|██████████████████████████████████████████████████████████████████████▍                 | 4/5 [00:11<00:02,  2.77s/it]tensor([13,  5,  9,  5,  3,  3,  3,  9, 16,  8,  6,  5,  1,  1,  5, 13],
       device='cuda:5')
tensor([ 5,  3,  3, 15,  7,  1, 11,  0, 18, 13, 15, 13, 16, 13,  4,  6],
       device='cuda:5')
tensor([12,  0,  3,  6,  3,  4, 12,  8,  2, 11, 13, 13,  1,  7, 15,  8],
       device='cuda:5')
tensor([ 4,  7, 16,  4,  6,  7,  8,  7,  0,  0, 15,  5,  8, 16, 19,  8],
       device='cuda:5')
tensor([16,  3, 19,  5,  7,  8, 16,  2,  6,  8, 18,  6, 11,  6,  3,  2],
       device='cuda:5')
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.045, Train_accy 87.60, Test_accy 75.85: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.20s/it]
2023-12-20 20:11:41,506 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.045, Train_accy 87.60, Test_accy 75.85
2023-12-20 20:11:41,510 [l2p_self_training.py] => Here are still 4761 unlabeled samples left
2023-12-20 20:11:41,510 [l2p_self_training.py] => pseudo labeling start
2023-12-20 20:12:09,942 [l2p_self_training.py] => 1933 unlabeled samples will be pseudo labeled
2023-12-20 20:12:09,942 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.578375581996896
Task 1, Epoch 4/5, Self_training_Iteration: 1 => Loss inf, Train_accy 44.02:  80%|████████████████████████████████████████████████████████████████████████                  | 4/5 [01:12<00:18, 18.16s/it]tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:5')
tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:5')
tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:5')
tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:5')
tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:5')
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 41.13, Test_accy 5.00: 100%|██████████████████████████████████████████████████████████████████████████| 5/5 [01:42<00:00, 20.60s/it]
2023-12-20 20:13:52,940 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 41.13, Test_accy 5.00
2023-12-20 20:13:52,944 [l2p_self_training.py] => Here are still 2828 unlabeled samples left
2023-12-20 20:13:52,945 [l2p_self_training.py] => pseudo labeling start
2023-12-20 20:14:10,024 [l2p_self_training.py] => 2828 unlabeled samples will be pseudo labeled
2023-12-20 20:14:10,025 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.0
2023-12-20 20:14:22,272 [trainer.py] => No NME accuracy.
2023-12-20 20:14:22,272 [trainer.py] => CNN: {'total': 5.0, '00-09': 10.0, '10-19': 0.0, 'old': 10.0, 'new': 0.0}
2023-12-20 20:14:22,273 [trainer.py] => CNN top1 curve: [91.6, 5.0]
2023-12-20 20:14:22,273 [trainer.py] => CNN top5 curve: [99.3, 28.5]

Average Accuracy (CNN): 48.3
2023-12-20 20:14:22,273 [trainer.py] => Average Accuracy (CNN): 48.3