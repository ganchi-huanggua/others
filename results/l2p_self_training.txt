1. 有标签数据训练分类器，给无标签数据打伪标签，只用最新打好伪标签的数据重新训练，之前打好伪标签的数据或有标签数据不在参与训练，而且训练后只在之前没有打过伪标签的无标签数据上打伪标签，以此迭代
2. 有标签数据训练分类器，给无标签数据打伪标签，用打好伪标签的数据和之前打好伪标签和有标签数据重训练，但是在训练后还是只在没有打过伪标签的无标签数据上打伪标签
3. 有标签数据训练分类器，给无标签数据打伪标签，用打好伪标签的数据和之前打好伪标签和有标签数据重训练，而且在训练后对所有无标签数据打伪标签
在3下进行实验

原始情况：
(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2023-12-25 10:16:59,290 [trainer.py] => config: ./exps/l2p_self_training.json
2023-12-25 10:16:59,291 [trainer.py] => prefix:  
2023-12-25 10:16:59,291 [trainer.py] => dataset: cifar224
2023-12-25 10:16:59,291 [trainer.py] => memory_size: 0
2023-12-25 10:16:59,291 [trainer.py] => memory_per_class: 0
2023-12-25 10:16:59,291 [trainer.py] => fixed_memory: False
2023-12-25 10:16:59,291 [trainer.py] => shuffle: True
2023-12-25 10:16:59,291 [trainer.py] => init_cls: 10
2023-12-25 10:16:59,291 [trainer.py] => increment: 10
2023-12-25 10:16:59,291 [trainer.py] => model_name: l2p_self_training
2023-12-25 10:16:59,291 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2023-12-25 10:16:59,291 [trainer.py] => get_original_backbone: True
2023-12-25 10:16:59,291 [trainer.py] => device: [device(type='cuda', index=0)]
2023-12-25 10:16:59,291 [trainer.py] => seed: 1993
2023-12-25 10:16:59,291 [trainer.py] => tuned_epoch: 5
2023-12-25 10:16:59,291 [trainer.py] => init_lr: 0.001875
2023-12-25 10:16:59,291 [trainer.py] => batch_size: 16
2023-12-25 10:16:59,291 [trainer.py] => weight_decay: 0
2023-12-25 10:16:59,291 [trainer.py] => min_lr: 1e-05
2023-12-25 10:16:59,291 [trainer.py] => optimizer: adam
2023-12-25 10:16:59,291 [trainer.py] => scheduler: constant
2023-12-25 10:16:59,291 [trainer.py] => reinit_optimizer: True
2023-12-25 10:16:59,291 [trainer.py] => global_pool: token
2023-12-25 10:16:59,291 [trainer.py] => head_type: prompt
2023-12-25 10:16:59,291 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2023-12-25 10:16:59,292 [trainer.py] => pretrained: True
2023-12-25 10:16:59,292 [trainer.py] => drop: 0.0
2023-12-25 10:16:59,292 [trainer.py] => drop_path: 0.0
2023-12-25 10:16:59,292 [trainer.py] => prompt_pool: True
2023-12-25 10:16:59,292 [trainer.py] => pool_size: 10
2023-12-25 10:16:59,292 [trainer.py] => length: 5
2023-12-25 10:16:59,292 [trainer.py] => top_k: 5
2023-12-25 10:16:59,292 [trainer.py] => initializer: uniform
2023-12-25 10:16:59,292 [trainer.py] => prompt_key: True
2023-12-25 10:16:59,292 [trainer.py] => prompt_key_init: uniform
2023-12-25 10:16:59,292 [trainer.py] => use_prompt_mask: False
2023-12-25 10:16:59,292 [trainer.py] => shared_prompt_pool: False
2023-12-25 10:16:59,292 [trainer.py] => shared_prompt_key: False
2023-12-25 10:16:59,292 [trainer.py] => batchwise_prompt: True
2023-12-25 10:16:59,292 [trainer.py] => embedding_key: cls
2023-12-25 10:16:59,292 [trainer.py] => predefined_key: 
2023-12-25 10:16:59,292 [trainer.py] => pull_constraint: True
2023-12-25 10:16:59,292 [trainer.py] => pull_constraint_coeff: 0.1
2023-12-25 10:16:59,292 [trainer.py] => semi_supervised_mode: True
2023-12-25 10:16:59,292 [trainer.py] => labeled_ratio: 0.05
2023-12-25 10:16:59,292 [trainer.py] => unlabeled_data_distribution_mode: no_ood
2023-12-25 10:16:59,292 [trainer.py] => confidence_threshold: 0.9
2023-12-25 10:16:59,292 [trainer.py] => max_self_training_iteration: 5
Files already downloaded and verified
Files already downloaded and verified
2023-12-25 10:17:01,248 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-25 10:17:03,029 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2023-12-25 10:17:03,029 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2023-12-25 10:17:05,313 [l2p_self_training.py] => 85,940,836 model total parameters.
2023-12-25 10:17:05,314 [l2p_self_training.py] => 122,980 model training parameters.
2023-12-25 10:17:05,314 [l2p_self_training.py] => prompt.prompt: 38400
2023-12-25 10:17:05,314 [l2p_self_training.py] => prompt.prompt_key: 7680
2023-12-25 10:17:05,314 [l2p_self_training.py] => head.weight: 76800
2023-12-25 10:17:05,315 [l2p_self_training.py] => head.bias: 100
2023-12-25 10:17:05,316 [trainer.py] => All params: 171816392
2023-12-25 10:17:05,317 [trainer.py] => Trainable params: 122980
2023-12-25 10:17:05,317 [l2p_self_training.py] => Learning on 0-10
250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.04s/it]
2023-12-25 10:17:26,299 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40
              precision    recall  f1-score   support

           0       0.78      0.99      0.87       100
           1       0.84      1.00      0.91       100
           2       1.00      0.87      0.93       100
           3       0.85      1.00      0.92       100
           4       1.00      0.87      0.93       100
           5       1.00      0.83      0.91       100
           6       0.85      0.88      0.87       100
           7       0.79      0.97      0.87       100
           8       0.94      0.68      0.79       100
           9       1.00      0.81      0.90       100

    accuracy                           0.89      1000
   macro avg       0.91      0.89      0.89      1000
weighted avg       0.91      0.89      0.89      1000

2023-12-25 10:17:32,562 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:18:00,574 [l2p_self_training.py] => 1833 unlabeled samples will be pseudo labeled
2023-12-25 10:18:00,574 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9896344789961812
2083
Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.043, Train_accy 89.73, Test_accy 93.20: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [01:43<00:00, 20.66s/it]
2023-12-25 10:19:44,141 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.043, Train_accy 89.73, Test_accy 93.20
              precision    recall  f1-score   support

           0       0.95      0.94      0.94       100
           1       0.96      0.99      0.98       100
           2       0.95      0.96      0.96       100
           3       0.75      1.00      0.86       100
           4       0.92      0.94      0.93       100
           5       0.99      0.96      0.97       100
           6       0.96      0.92      0.94       100
           7       0.94      0.96      0.95       100
           8       1.00      0.76      0.86       100
           9       0.98      0.89      0.93       100

    accuracy                           0.93      1000
   macro avg       0.94      0.93      0.93      1000
weighted avg       0.94      0.93      0.93      1000

2023-12-25 10:19:50,795 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:20:19,309 [l2p_self_training.py] => 3366 unlabeled samples will be pseudo labeled
2023-12-25 10:20:19,310 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9506833036244801
3616
Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.044, Train_accy 89.13, Test_accy 93.50: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [02:54<00:00, 34.88s/it]
2023-12-25 10:23:14,001 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.044, Train_accy 89.13, Test_accy 93.50
              precision    recall  f1-score   support

           0       0.94      0.92      0.93       100
           1       0.98      0.99      0.99       100
           2       0.92      0.98      0.95       100
           3       0.81      1.00      0.89       100
           4       0.96      0.91      0.93       100
           5       0.99      0.94      0.96       100
           6       0.89      0.93      0.91       100
           7       0.96      0.90      0.93       100
           8       0.98      0.85      0.91       100
           9       0.97      0.93      0.95       100

    accuracy                           0.94      1000
   macro avg       0.94      0.93      0.94      1000
weighted avg       0.94      0.94      0.94      1000

2023-12-25 10:23:20,313 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:23:48,807 [l2p_self_training.py] => 3319 unlabeled samples will be pseudo labeled
2023-12-25 10:23:48,807 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9647484181982525
3569
Task 0, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.030, Train_accy 90.89, Test_accy 94.40: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [02:52<00:00, 34.41s/it]
2023-12-25 10:26:41,055 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.030, Train_accy 90.89, Test_accy 94.40
              precision    recall  f1-score   support

           0       0.93      0.88      0.90       100
           1       0.99      0.97      0.98       100
           2       0.94      0.98      0.96       100
           3       0.92      0.99      0.95       100
           4       0.95      0.91      0.93       100
           5       0.98      0.97      0.97       100
           6       0.86      0.96      0.91       100
           7       0.99      0.92      0.95       100
           8       0.95      0.91      0.93       100
           9       0.96      0.95      0.95       100

    accuracy                           0.94      1000
   macro avg       0.95      0.94      0.94      1000
weighted avg       0.95      0.94      0.94      1000

2023-12-25 10:26:47,402 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:27:15,987 [l2p_self_training.py] => 3667 unlabeled samples will be pseudo labeled
2023-12-25 10:27:15,987 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9645486773929642
3917
Task 0, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.050, Train_accy 91.22, Test_accy 93.80: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [03:08<00:00, 37.65s/it]
2023-12-25 10:30:24,376 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.050, Train_accy 91.22, Test_accy 93.80
              precision    recall  f1-score   support

           0       0.99      0.87      0.93       100
           1       1.00      0.97      0.98       100
           2       1.00      0.86      0.92       100
           3       0.99      0.99      0.99       100
           4       0.86      0.96      0.91       100
           5       0.96      0.97      0.97       100
           6       0.87      0.96      0.91       100
           7       0.99      0.89      0.94       100
           8       0.84      0.98      0.90       100
           9       0.93      0.93      0.93       100

    accuracy                           0.94      1000
   macro avg       0.94      0.94      0.94      1000
weighted avg       0.94      0.94      0.94      1000

2023-12-25 10:30:30,775 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:30:59,387 [l2p_self_training.py] => 3680 unlabeled samples will be pseudo labeled
2023-12-25 10:30:59,388 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9578804347826086
2023-12-25 10:31:05,996 [trainer.py] => No NME accuracy.
2023-12-25 10:31:05,997 [trainer.py] => CNN: {'total': 93.8, '00-09': 93.8, 'old': 0, 'new': 93.8}
2023-12-25 10:31:05,997 [trainer.py] => CNN top1 curve: [93.8]
2023-12-25 10:31:05,997 [trainer.py] => CNN top5 curve: [99.8]

Average Accuracy (CNN): 93.8
2023-12-25 10:31:05,997 [trainer.py] => Average Accuracy (CNN): 93.8 

2023-12-25 10:31:06,000 [trainer.py] => All params: 171816392
2023-12-25 10:31:06,002 [trainer.py] => Trainable params: 122980
2023-12-25 10:31:06,002 [l2p_self_training.py] => Learning on 10-20
250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.014, Train_accy 89.20, Test_accy 74.70: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.16s/it]
2023-12-25 10:31:31,871 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.014, Train_accy 89.20, Test_accy 74.70
              precision    recall  f1-score   support

           0       0.94      0.85      0.89       100
           1       0.96      0.98      0.97       100
           2       0.94      0.87      0.90       100
           3       0.92      0.97      0.94       100
           4       0.76      0.96      0.85       100
           5       0.67      0.96      0.79       100
           6       0.72      0.97      0.83       100
           7       0.71      0.89      0.79       100
           8       0.24      0.98      0.39       100
           9       0.81      0.92      0.86       100
          10       0.95      0.57      0.71       100
          11       1.00      0.79      0.88       100
          12       1.00      0.35      0.52       100
          13       1.00      0.98      0.99       100
          14       0.93      0.27      0.42       100
          15       0.99      0.77      0.87       100
          16       1.00      0.67      0.80       100
          17       1.00      0.20      0.33       100
          18       1.00      0.77      0.87       100
          19       1.00      0.22      0.36       100

    accuracy                           0.75      2000
   macro avg       0.88      0.75      0.75      2000
weighted avg       0.88      0.75      0.75      2000

2023-12-25 10:31:44,145 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:32:12,785 [l2p_self_training.py] => 2209 unlabeled samples will be pseudo labeled
2023-12-25 10:32:12,786 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.5450430058850159
2459
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 36.19, Test_accy 5.00: 100%|██████████████████████████████████████████████████████████████████████████| 5/5 [02:07<00:00, 25.58s/it]
2023-12-25 10:34:21,132 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 36.19, Test_accy 5.00
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           1       0.00      0.00      0.00       100
           2       0.00      0.00      0.00       100
           3       0.00      0.00      0.00       100
           4       0.00      0.00      0.00       100
           5       0.00      0.00      0.00       100
           6       0.00      0.00      0.00       100
           7       0.00      0.00      0.00       100
           8       0.05      1.00      0.10       100
           9       0.00      0.00      0.00       100
          10       0.00      0.00      0.00       100
          11       0.00      0.00      0.00       100
          12       0.00      0.00      0.00       100
          13       0.00      0.00      0.00       100
          14       0.00      0.00      0.00       100
          15       0.00      0.00      0.00       100
          16       0.00      0.00      0.00       100
          17       0.00      0.00      0.00       100
          18       0.00      0.00      0.00       100
          19       0.00      0.00      0.00       100

    accuracy                           0.05      2000
   macro avg       0.00      0.05      0.00      2000
weighted avg       0.00      0.05      0.00      2000

2023-12-25 10:34:33,419 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:35:02,330 [l2p_self_training.py] => 4761 unlabeled samples will be pseudo labeled
2023-12-25 10:35:02,330 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.0
5011
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss inf, Train_accy 0.78, Test_accy 5.00: 100%|███████████████████████████████████████████████████████████████████████████| 5/5 [04:05<00:00, 49.02s/it]
2023-12-25 10:39:07,636 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss inf, Train_accy 0.78, Test_accy 5.00
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           1       0.00      0.00      0.00       100
           2       0.00      0.00      0.00       100
           3       0.00      0.00      0.00       100
           4       0.00      0.00      0.00       100
           5       0.00      0.00      0.00       100
           6       0.00      0.00      0.00       100
           7       0.00      0.00      0.00       100
           8       0.05      1.00      0.10       100
           9       0.00      0.00      0.00       100
          10       0.00      0.00      0.00       100
          11       0.00      0.00      0.00       100
          12       0.00      0.00      0.00       100
          13       0.00      0.00      0.00       100
          14       0.00      0.00      0.00       100
          15       0.00      0.00      0.00       100
          16       0.00      0.00      0.00       100
          17       0.00      0.00      0.00       100
          18       0.00      0.00      0.00       100
          19       0.00      0.00      0.00       100

    accuracy                           0.05      2000
   macro avg       0.00      0.05      0.00      2000
weighted avg       0.00      0.05      0.00      2000

2023-12-25 10:39:19,879 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:39:48,630 [l2p_self_training.py] => 4761 unlabeled samples will be pseudo labeled
2023-12-25 10:39:48,630 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.0
5011
Task 1, Epoch 1/5, Self_training_Iteration: 3 => Loss inf, Train_accy 0.74:  20%|██████████████████▏                                                                        | 1/5 [00:46<03:05, 46.25s/it]                                                                     | 1/5 [00:20<01:20, 20.05s/it]Task 1, Epoch 1/5, Self_training_Iteration: 1 => Loss inf, Train_accy 48.68:  20%|██████████████████                                                                        | 1/5 [00:25<01:40, 25.08s/it]

======================================================================================
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
======================================================================================

task 1不变，将刚开始进入task 2时1%打错的伪标签舍弃，不作为模型新一轮自训练的训练样本，将剩下的打对伪标签的样本作为模型新一轮自训练的样本
(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2023-12-25 10:45:42,503 [trainer.py] => config: ./exps/l2p_self_training.json
2023-12-25 10:45:42,503 [trainer.py] => prefix:  
2023-12-25 10:45:42,504 [trainer.py] => dataset: cifar224
2023-12-25 10:45:42,504 [trainer.py] => memory_size: 0
2023-12-25 10:45:42,504 [trainer.py] => memory_per_class: 0
2023-12-25 10:45:42,504 [trainer.py] => fixed_memory: False
2023-12-25 10:45:42,504 [trainer.py] => shuffle: True
2023-12-25 10:45:42,504 [trainer.py] => init_cls: 10
2023-12-25 10:45:42,504 [trainer.py] => increment: 10
2023-12-25 10:45:42,504 [trainer.py] => model_name: l2p_self_training
2023-12-25 10:45:42,504 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2023-12-25 10:45:42,504 [trainer.py] => get_original_backbone: True
2023-12-25 10:45:42,504 [trainer.py] => device: [device(type='cuda', index=4)]
2023-12-25 10:45:42,504 [trainer.py] => seed: 1993
2023-12-25 10:45:42,504 [trainer.py] => tuned_epoch: 5
2023-12-25 10:45:42,504 [trainer.py] => init_lr: 0.001875
2023-12-25 10:45:42,504 [trainer.py] => batch_size: 16
2023-12-25 10:45:42,504 [trainer.py] => weight_decay: 0
2023-12-25 10:45:42,504 [trainer.py] => min_lr: 1e-05
2023-12-25 10:45:42,504 [trainer.py] => optimizer: adam
2023-12-25 10:45:42,504 [trainer.py] => scheduler: constant
2023-12-25 10:45:42,504 [trainer.py] => reinit_optimizer: True
2023-12-25 10:45:42,504 [trainer.py] => global_pool: token
2023-12-25 10:45:42,504 [trainer.py] => head_type: prompt
2023-12-25 10:45:42,504 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2023-12-25 10:45:42,504 [trainer.py] => pretrained: True
2023-12-25 10:45:42,504 [trainer.py] => drop: 0.0
2023-12-25 10:45:42,504 [trainer.py] => drop_path: 0.0
2023-12-25 10:45:42,505 [trainer.py] => prompt_pool: True
2023-12-25 10:45:42,505 [trainer.py] => pool_size: 10
2023-12-25 10:45:42,505 [trainer.py] => length: 5
2023-12-25 10:45:42,505 [trainer.py] => top_k: 5
2023-12-25 10:45:42,505 [trainer.py] => initializer: uniform
2023-12-25 10:45:42,505 [trainer.py] => prompt_key: True
2023-12-25 10:45:42,505 [trainer.py] => prompt_key_init: uniform
2023-12-25 10:45:42,505 [trainer.py] => use_prompt_mask: False
2023-12-25 10:45:42,505 [trainer.py] => shared_prompt_pool: False
2023-12-25 10:45:42,505 [trainer.py] => shared_prompt_key: False
2023-12-25 10:45:42,505 [trainer.py] => batchwise_prompt: True
2023-12-25 10:45:42,505 [trainer.py] => embedding_key: cls
2023-12-25 10:45:42,505 [trainer.py] => predefined_key: 
2023-12-25 10:45:42,505 [trainer.py] => pull_constraint: True
2023-12-25 10:45:42,505 [trainer.py] => pull_constraint_coeff: 0.1
2023-12-25 10:45:42,505 [trainer.py] => semi_supervised_mode: True
2023-12-25 10:45:42,505 [trainer.py] => labeled_ratio: 0.05
2023-12-25 10:45:42,505 [trainer.py] => unlabeled_data_distribution_mode: no_ood
2023-12-25 10:45:42,505 [trainer.py] => confidence_threshold: 0.9
2023-12-25 10:45:42,505 [trainer.py] => max_self_training_iteration: 5
Files already downloaded and verified
Files already downloaded and verified
2023-12-25 10:45:44,519 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-25 10:45:46,232 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2023-12-25 10:45:46,232 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2023-12-25 10:45:48,494 [l2p_self_training.py] => 85,940,836 model total parameters.
2023-12-25 10:45:48,495 [l2p_self_training.py] => 122,980 model training parameters.
2023-12-25 10:45:48,495 [l2p_self_training.py] => prompt.prompt: 38400
2023-12-25 10:45:48,495 [l2p_self_training.py] => prompt.prompt_key: 7680
2023-12-25 10:45:48,495 [l2p_self_training.py] => head.weight: 76800
2023-12-25 10:45:48,495 [l2p_self_training.py] => head.bias: 100
2023-12-25 10:45:48,497 [trainer.py] => All params: 171816392
2023-12-25 10:45:48,498 [trainer.py] => Trainable params: 122980
2023-12-25 10:45:48,498 [l2p_self_training.py] => Learning on 0-10
250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.01s/it]
2023-12-25 10:46:09,369 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40
              precision    recall  f1-score   support

           0       0.78      0.99      0.87       100
           1       0.84      1.00      0.91       100
           2       1.00      0.87      0.93       100
           3       0.85      1.00      0.92       100
           4       1.00      0.87      0.93       100
           5       1.00      0.83      0.91       100
           6       0.85      0.88      0.87       100
           7       0.79      0.97      0.87       100
           8       0.94      0.68      0.79       100
           9       1.00      0.81      0.90       100

    accuracy                           0.89      1000
   macro avg       0.91      0.89      0.89      1000
weighted avg       0.91      0.89      0.89      1000

2023-12-25 10:46:15,545 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:46:43,233 [l2p_self_training.py] => 1833 unlabeled samples will be pseudo labeled
2023-12-25 10:46:43,234 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9896344789961812
2083
Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.043, Train_accy 89.73, Test_accy 93.20: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [01:41<00:00, 20.34s/it]
2023-12-25 10:48:25,109 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.043, Train_accy 89.73, Test_accy 93.20
              precision    recall  f1-score   support

           0       0.95      0.94      0.94       100
           1       0.96      0.99      0.98       100
           2       0.95      0.96      0.96       100
           3       0.75      1.00      0.86       100
           4       0.92      0.94      0.93       100
           5       0.99      0.96      0.97       100
           6       0.96      0.92      0.94       100
           7       0.94      0.96      0.95       100
           8       1.00      0.76      0.86       100
           9       0.98      0.89      0.93       100

    accuracy                           0.93      1000
   macro avg       0.94      0.93      0.93      1000
weighted avg       0.94      0.93      0.93      1000

2023-12-25 10:48:31,361 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:48:59,398 [l2p_self_training.py] => 3366 unlabeled samples will be pseudo labeled
2023-12-25 10:48:59,399 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9506833036244801
3616
Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.044, Train_accy 89.13, Test_accy 93.50: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [02:51<00:00, 34.23s/it]
2023-12-25 10:51:50,947 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.044, Train_accy 89.13, Test_accy 93.50
              precision    recall  f1-score   support

           0       0.94      0.92      0.93       100
           1       0.98      0.99      0.99       100
           2       0.92      0.98      0.95       100
           3       0.81      1.00      0.89       100
           4       0.96      0.91      0.93       100
           5       0.99      0.94      0.96       100
           6       0.89      0.93      0.91       100
           7       0.96      0.90      0.93       100
           8       0.98      0.85      0.91       100
           9       0.97      0.93      0.95       100

    accuracy                           0.94      1000
   macro avg       0.94      0.93      0.94      1000
weighted avg       0.94      0.94      0.94      1000

2023-12-25 10:51:57,261 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:52:25,494 [l2p_self_training.py] => 3319 unlabeled samples will be pseudo labeled
2023-12-25 10:52:25,495 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9647484181982525
3569
Task 0, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.030, Train_accy 90.89, Test_accy 94.40: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [02:49<00:00, 33.92s/it]
2023-12-25 10:55:15,216 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.030, Train_accy 90.89, Test_accy 94.40
              precision    recall  f1-score   support

           0       0.93      0.88      0.90       100
           1       0.99      0.97      0.98       100
           2       0.94      0.98      0.96       100
           3       0.92      0.99      0.95       100
           4       0.95      0.91      0.93       100
           5       0.98      0.97      0.97       100
           6       0.86      0.96      0.91       100
           7       0.99      0.92      0.95       100
           8       0.95      0.91      0.93       100
           9       0.96      0.95      0.95       100

    accuracy                           0.94      1000
   macro avg       0.95      0.94      0.94      1000
weighted avg       0.95      0.94      0.94      1000

2023-12-25 10:55:21,503 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:55:49,712 [l2p_self_training.py] => 3667 unlabeled samples will be pseudo labeled
2023-12-25 10:55:49,712 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9645486773929642
3917
Task 0, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.050, Train_accy 91.22, Test_accy 93.80: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [03:06<00:00, 37.21s/it]
2023-12-25 10:58:55,950 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.050, Train_accy 91.22, Test_accy 93.80
              precision    recall  f1-score   support

           0       0.99      0.87      0.93       100
           1       1.00      0.97      0.98       100
           2       1.00      0.86      0.92       100
           3       0.99      0.99      0.99       100
           4       0.86      0.96      0.91       100
           5       0.96      0.97      0.97       100
           6       0.87      0.96      0.91       100
           7       0.99      0.89      0.94       100
           8       0.84      0.98      0.90       100
           9       0.93      0.93      0.93       100

    accuracy                           0.94      1000
   macro avg       0.94      0.94      0.94      1000
weighted avg       0.94      0.94      0.94      1000

2023-12-25 10:59:02,231 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:59:30,237 [l2p_self_training.py] => 3680 unlabeled samples will be pseudo labeled
2023-12-25 10:59:30,238 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9578804347826086
2023-12-25 10:59:36,652 [trainer.py] => No NME accuracy.
2023-12-25 10:59:36,653 [trainer.py] => CNN: {'total': 93.8, '00-09': 93.8, 'old': 0, 'new': 93.8}
2023-12-25 10:59:36,653 [trainer.py] => CNN top1 curve: [93.8]
2023-12-25 10:59:36,653 [trainer.py] => CNN top5 curve: [99.8]

Average Accuracy (CNN): 93.8
2023-12-25 10:59:36,653 [trainer.py] => Average Accuracy (CNN): 93.8 

2023-12-25 10:59:36,654 [trainer.py] => All params: 171816392
2023-12-25 10:59:36,655 [trainer.py] => Trainable params: 122980
2023-12-25 10:59:36,655 [l2p_self_training.py] => Learning on 10-20
250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.014, Train_accy 89.20, Test_accy 74.70: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.16s/it]
2023-12-25 11:00:02,500 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.014, Train_accy 89.20, Test_accy 74.70
              precision    recall  f1-score   support

           0       0.94      0.85      0.89       100
           1       0.96      0.98      0.97       100
           2       0.94      0.87      0.90       100
           3       0.92      0.97      0.94       100
           4       0.76      0.96      0.85       100
           5       0.67      0.96      0.79       100
           6       0.72      0.97      0.83       100
           7       0.71      0.89      0.79       100
           8       0.24      0.98      0.39       100
           9       0.81      0.92      0.86       100
          10       0.95      0.57      0.71       100
          11       1.00      0.79      0.88       100
          12       1.00      0.35      0.52       100
          13       1.00      0.98      0.99       100
          14       0.93      0.27      0.42       100
          15       0.99      0.77      0.87       100
          16       1.00      0.67      0.80       100
          17       1.00      0.20      0.33       100
          18       1.00      0.77      0.87       100
          19       1.00      0.22      0.36       100

    accuracy                           0.75      2000
   macro avg       0.88      0.75      0.75      2000
weighted avg       0.88      0.75      0.75      2000

2023-12-25 11:00:14,526 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:00:42,389 [l2p_self_training.py] => 2209 unlabeled samples will be pseudo labeled
2023-12-25 11:00:42,389 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.5450430058850159
250
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.023, Train_accy 90.00, Test_accy 77.40: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.19s/it]
2023-12-25 11:01:08,870 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.023, Train_accy 90.00, Test_accy 77.40
              precision    recall  f1-score   support

           0       0.99      0.86      0.92       100
           1       0.97      0.98      0.98       100
           2       0.94      0.88      0.91       100
           3       0.93      0.97      0.95       100
           4       0.77      0.96      0.86       100
           5       0.70      0.96      0.81       100
           6       0.75      0.97      0.85       100
           7       0.80      0.86      0.83       100
           8       0.26      0.99      0.41       100
           9       0.79      0.91      0.85       100
          10       0.97      0.66      0.79       100
          11       0.99      0.88      0.93       100
          12       0.98      0.50      0.66       100
          13       1.00      0.97      0.98       100
          14       0.94      0.31      0.47       100
          15       1.00      0.65      0.79       100
          16       1.00      0.65      0.79       100
          17       0.97      0.29      0.45       100
          18       1.00      0.80      0.89       100
          19       0.98      0.43      0.60       100

    accuracy                           0.77      2000
   macro avg       0.89      0.77      0.78      2000
weighted avg       0.89      0.77      0.78      2000

2023-12-25 11:01:20,952 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:01:48,841 [l2p_self_training.py] => 2263 unlabeled samples will be pseudo labeled
2023-12-25 11:01:48,841 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.6168802474591251
250
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.023, Train_accy 90.80, Test_accy 78.65: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:26<00:00,  5.21s/it]
2023-12-25 11:02:15,123 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.023, Train_accy 90.80, Test_accy 78.65
              precision    recall  f1-score   support

           0       0.99      0.85      0.91       100
           1       0.97      0.97      0.97       100
           2       0.94      0.88      0.91       100
           3       0.92      0.96      0.94       100
           4       0.77      0.96      0.86       100
           5       0.75      0.94      0.83       100
           6       0.76      0.97      0.85       100
           7       0.79      0.86      0.82       100
           8       0.28      0.99      0.43       100
           9       0.83      0.91      0.87       100
          10       0.89      0.73      0.80       100
          11       0.99      0.87      0.93       100
          12       1.00      0.54      0.70       100
          13       1.00      0.96      0.98       100
          14       0.96      0.26      0.41       100
          15       0.97      0.74      0.84       100
          16       1.00      0.72      0.84       100
          17       0.97      0.36      0.53       100
          18       0.98      0.84      0.90       100
          19       0.98      0.42      0.59       100

    accuracy                           0.79      2000
   macro avg       0.89      0.79      0.80      2000
weighted avg       0.89      0.79      0.80      2000

2023-12-25 11:02:27,124 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:02:54,981 [l2p_self_training.py] => 2520 unlabeled samples will be pseudo labeled
2023-12-25 11:02:54,981 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.6547619047619048
250
Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.138, Train_accy 94.00, Test_accy 79.35: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.19s/it]
2023-12-25 11:03:21,351 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.138, Train_accy 94.00, Test_accy 79.35
              precision    recall  f1-score   support

           0       0.99      0.85      0.91       100
           1       0.97      0.97      0.97       100
           2       0.93      0.89      0.91       100
           3       0.94      0.96      0.95       100
           4       0.79      0.96      0.86       100
           5       0.72      0.94      0.82       100
           6       0.80      0.97      0.88       100
           7       0.79      0.86      0.82       100
           8       0.29      0.99      0.44       100
           9       0.80      0.92      0.86       100
          10       0.94      0.59      0.72       100
          11       0.99      0.96      0.97       100
          12       0.98      0.56      0.71       100
          13       1.00      0.98      0.99       100
          14       0.93      0.38      0.54       100
          15       0.98      0.80      0.88       100
          16       1.00      0.72      0.84       100
          17       0.97      0.29      0.45       100
          18       0.97      0.87      0.92       100
          19       0.98      0.41      0.58       100

    accuracy                           0.79      2000
   macro avg       0.89      0.79      0.80      2000
weighted avg       0.89      0.79      0.80      2000

2023-12-25 11:03:33,323 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:04:01,225 [l2p_self_training.py] => 2668 unlabeled samples will be pseudo labeled
2023-12-25 11:04:01,225 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.6754122938530734
250
Task 1, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.175, Train_accy 95.20, Test_accy 80.35: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.19s/it]
2023-12-25 11:04:27,416 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.175, Train_accy 95.20, Test_accy 80.35
              precision    recall  f1-score   support

           0       0.99      0.85      0.91       100
           1       0.96      0.95      0.95       100
           2       0.95      0.88      0.91       100
           3       0.95      0.97      0.96       100
           4       0.83      0.96      0.89       100
           5       0.77      0.94      0.85       100
           6       0.81      0.97      0.88       100
           7       0.79      0.86      0.82       100
           8       0.29      0.99      0.44       100
           9       0.89      0.93      0.91       100
          10       0.85      0.81      0.83       100
          11       0.99      0.97      0.98       100
          12       1.00      0.54      0.70       100
          13       1.00      0.98      0.99       100
          14       0.88      0.44      0.59       100
          15       0.99      0.80      0.88       100
          16       1.00      0.69      0.82       100
          17       1.00      0.34      0.51       100
          18       0.99      0.83      0.90       100
          19       0.95      0.37      0.53       100

    accuracy                           0.80      2000
   macro avg       0.89      0.80      0.81      2000
weighted avg       0.89      0.80      0.81      2000

2023-12-25 11:04:39,483 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:05:07,289 [l2p_self_training.py] => 2735 unlabeled samples will be pseudo labeled
2023-12-25 11:05:07,289 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.6976234003656308
2023-12-25 11:05:19,623 [trainer.py] => No NME accuracy.
2023-12-25 11:05:19,623 [trainer.py] => CNN: {'total': 80.35, '00-09': 93.0, '10-19': 67.7, 'old': 93.0, 'new': 67.7}
2023-12-25 11:05:19,623 [trainer.py] => CNN top1 curve: [93.8, 80.35]
2023-12-25 11:05:19,623 [trainer.py] => CNN top5 curve: [99.8, 98.7]

Average Accuracy (CNN): 87.07499999999999
2023-12-25 11:05:19,623 [trainer.py] => Average Accuracy (CNN): 87.07499999999999 

2023-12-25 11:05:19,625 [trainer.py] => All params: 171816392
2023-12-25 11:05:19,627 [trainer.py] => Trainable params: 122980
2023-12-25 11:05:19,627 [l2p_self_training.py] => Learning on 20-30
250
Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.109, Train_accy 92.40, Test_accy 68.20: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:31<00:00,  6.38s/it]
2023-12-25 11:05:51,682 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.109, Train_accy 92.40, Test_accy 68.20
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.84      0.86      0.85       100
           1       0.91      0.96      0.94       100
           2       0.91      0.88      0.89       100
           3       0.92      0.96      0.94       100
           4       0.75      0.96      0.84       100
           5       0.68      0.94      0.79       100
           6       0.51      0.97      0.66       100
           7       0.76      0.86      0.81       100
           8       0.18      0.99      0.31       100
           9       0.72      0.94      0.82       100
          10       0.55      0.81      0.66       100
          11       0.66      0.97      0.79       100
          12       0.97      0.57      0.72       100
          13       0.51      0.98      0.67       100
          14       0.88      0.45      0.60       100
          15       0.95      0.80      0.87       100
          16       0.97      0.67      0.79       100
          17       0.95      0.36      0.52       100
          18       0.99      0.85      0.91       100
          19       0.95      0.40      0.56       100
          20       1.00      0.46      0.63       100
          21       1.00      0.04      0.08       100
          22       1.00      0.39      0.56       100
          23       1.00      0.81      0.90       100
          24       1.00      0.37      0.54       100
          25       1.00      0.48      0.65       100
          26       0.98      0.40      0.57       100
          27       0.00      0.00      0.00       100
          28       1.00      0.33      0.50       100
          29       1.00      1.00      1.00       100

    accuracy                           0.68      3000
   macro avg       0.82      0.68      0.68      3000
weighted avg       0.82      0.68      0.68      3000

2023-12-25 11:06:09,398 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:06:37,186 [l2p_self_training.py] => 2217 unlabeled samples will be pseudo labeled
2023-12-25 11:06:37,186 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.37302661253946773
250
Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.107, Train_accy 92.40, Test_accy 70.37: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:31<00:00,  6.38s/it]
2023-12-25 11:07:09,580 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.107, Train_accy 92.40, Test_accy 70.37
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.88      0.86      0.87       100
           1       0.94      0.96      0.95       100
           2       0.93      0.87      0.90       100
           3       0.92      0.96      0.94       100
           4       0.75      0.96      0.84       100
           5       0.69      0.94      0.80       100
           6       0.53      0.97      0.69       100
           7       0.77      0.85      0.81       100
           8       0.18      0.99      0.31       100
           9       0.78      0.93      0.85       100
          10       0.60      0.81      0.69       100
          11       0.73      0.97      0.83       100
          12       0.97      0.56      0.71       100
          13       0.51      0.98      0.67       100
          14       0.87      0.47      0.61       100
          15       0.94      0.79      0.86       100
          16       0.97      0.67      0.79       100
          17       0.97      0.35      0.51       100
          18       0.99      0.85      0.91       100
          19       0.95      0.39      0.55       100
          20       1.00      0.54      0.70       100
          21       1.00      0.06      0.11       100
          22       1.00      0.57      0.73       100
          23       1.00      0.80      0.89       100
          24       1.00      0.49      0.66       100
          25       1.00      0.37      0.54       100
          26       0.99      0.68      0.80       100
          27       0.00      0.00      0.00       100
          28       1.00      0.49      0.66       100
          29       1.00      0.98      0.99       100

    accuracy                           0.70      3000
   macro avg       0.83      0.70      0.71      3000
weighted avg       0.83      0.70      0.71      3000

2023-12-25 11:07:27,291 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:07:55,212 [l2p_self_training.py] => 2293 unlabeled samples will be pseudo labeled
2023-12-25 11:07:55,212 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.4679459223724379
250
Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.248, Train_accy 97.60, Test_accy 71.50: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:31<00:00,  6.36s/it]
2023-12-25 11:08:27,563 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.248, Train_accy 97.60, Test_accy 71.50
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.85      0.86      0.86       100
           1       0.94      0.95      0.95       100
           2       0.91      0.87      0.89       100
           3       0.91      0.95      0.93       100
           4       0.79      0.96      0.86       100
           5       0.69      0.94      0.80       100
           6       0.53      0.97      0.69       100
           7       0.76      0.84      0.80       100
           8       0.19      0.99      0.31       100
           9       0.77      0.94      0.85       100
          10       0.70      0.81      0.75       100
          11       0.76      0.97      0.85       100
          12       0.97      0.56      0.71       100
          13       0.52      0.98      0.68       100
          14       0.87      0.46      0.60       100
          15       0.95      0.78      0.86       100
          16       0.97      0.67      0.79       100
          17       1.00      0.34      0.51       100
          18       0.99      0.85      0.91       100
          19       0.95      0.38      0.54       100
          20       1.00      0.44      0.61       100
          21       1.00      0.10      0.18       100
          22       0.99      0.67      0.80       100
          23       1.00      0.79      0.88       100
          24       0.97      0.58      0.72       100
          25       1.00      0.48      0.65       100
          26       0.97      0.62      0.76       100
          27       0.00      0.00      0.00       100
          28       0.99      0.74      0.85       100
          29       1.00      0.96      0.98       100

    accuracy                           0.71      3000
   macro avg       0.83      0.71      0.72      3000
weighted avg       0.83      0.71      0.72      3000

2023-12-25 11:08:45,311 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:09:13,153 [l2p_self_training.py] => 2381 unlabeled samples will be pseudo labeled
2023-12-25 11:09:13,153 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.5300293994120118
250
Task 2, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.133, Train_accy 94.80, Test_accy 72.33: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:31<00:00,  6.39s/it]
2023-12-25 11:09:45,490 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.133, Train_accy 94.80, Test_accy 72.33
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.88      0.86      0.87       100
           1       0.94      0.95      0.95       100
           2       0.90      0.88      0.89       100
           3       0.91      0.95      0.93       100
           4       0.76      0.95      0.84       100
           5       0.69      0.94      0.80       100
           6       0.59      0.96      0.73       100
           7       0.78      0.83      0.81       100
           8       0.19      0.99      0.32       100
           9       0.78      0.93      0.85       100
          10       0.60      0.81      0.69       100
          11       0.84      0.97      0.90       100
          12       0.97      0.57      0.72       100
          13       0.54      0.98      0.69       100
          14       0.87      0.47      0.61       100
          15       0.95      0.79      0.86       100
          16       0.97      0.67      0.79       100
          17       0.97      0.34      0.50       100
          18       0.99      0.85      0.91       100
          19       0.95      0.42      0.58       100
          20       1.00      0.60      0.75       100
          21       1.00      0.15      0.26       100
          22       0.98      0.64      0.78       100
          23       1.00      0.81      0.90       100
          24       0.99      0.75      0.85       100
          25       1.00      0.50      0.67       100
          26       0.99      0.66      0.79       100
          27       0.00      0.00      0.00       100
          28       1.00      0.48      0.65       100
          29       1.00      1.00      1.00       100

    accuracy                           0.72      3000
   macro avg       0.83      0.72      0.73      3000
weighted avg       0.83      0.72      0.73      3000

2023-12-25 11:10:03,319 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:10:31,206 [l2p_self_training.py] => 2514 unlabeled samples will be pseudo labeled
2023-12-25 11:10:31,207 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.5437549721559268
250
Task 2, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.176, Train_accy 96.00, Test_accy 74.40: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:31<00:00,  6.32s/it]
2023-12-25 11:11:03,412 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.176, Train_accy 96.00, Test_accy 74.40
              precision    recall  f1-score   support

           0       0.90      0.85      0.88       100
           1       0.93      0.95      0.94       100
           2       0.88      0.88      0.88       100
           3       0.93      0.94      0.94       100
           4       0.78      0.96      0.86       100
           5       0.72      0.94      0.82       100
           6       0.57      0.98      0.72       100
           7       0.81      0.83      0.82       100
           8       0.19      0.99      0.32       100
           9       0.82      0.92      0.87       100
          10       0.79      0.81      0.80       100
          11       0.84      0.97      0.90       100
          12       0.97      0.57      0.72       100
          13       0.60      0.97      0.74       100
          14       0.85      0.47      0.61       100
          15       0.96      0.75      0.84       100
          16       0.97      0.66      0.79       100
          17       1.00      0.33      0.50       100
          18       0.99      0.84      0.91       100
          19       0.95      0.42      0.58       100
          20       1.00      0.61      0.76       100
          21       0.97      0.33      0.49       100
          22       0.99      0.71      0.83       100
          23       1.00      0.78      0.88       100
          24       0.99      0.66      0.79       100
          25       1.00      0.58      0.73       100
          26       0.96      0.76      0.85       100
          27       1.00      0.01      0.02       100
          28       0.93      0.87      0.90       100
          29       1.00      0.98      0.99       100

    accuracy                           0.74      3000
   macro avg       0.88      0.74      0.75      3000
weighted avg       0.88      0.74      0.75      3000

2023-12-25 11:11:21,139 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:11:49,347 [l2p_self_training.py] => 2664 unlabeled samples will be pseudo labeled
2023-12-25 11:11:49,347 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.6418918918918919
2023-12-25 11:12:07,745 [trainer.py] => No NME accuracy.
2023-12-25 11:12:07,745 [trainer.py] => CNN: {'total': 74.4, '00-09': 92.4, '10-19': 67.9, '20-29': 62.9, 'old': 80.15, 'new': 62.9}
2023-12-25 11:12:07,745 [trainer.py] => CNN top1 curve: [93.8, 80.35, 74.4]
2023-12-25 11:12:07,745 [trainer.py] => CNN top5 curve: [99.8, 98.7, 98.07]

Average Accuracy (CNN): 82.85
2023-12-25 11:12:07,745 [trainer.py] => Average Accuracy (CNN): 82.85 

2023-12-25 11:12:07,747 [trainer.py] => All params: 171816392
2023-12-25 11:12:07,748 [trainer.py] => Trainable params: 122980
2023-12-25 11:12:07,748 [l2p_self_training.py] => Learning on 30-40
250
Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.080, Train_accy 93.20, Test_accy 66.22: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:37<00:00,  7.51s/it]
2023-12-25 11:12:45,360 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.080, Train_accy 93.20, Test_accy 66.22
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.76      0.85      0.80       100
           1       0.70      0.94      0.80       100
           2       0.68      0.91      0.78       100
           3       0.88      0.92      0.90       100
           4       0.70      0.95      0.81       100
           5       0.61      0.94      0.74       100
           6       0.29      0.98      0.45       100
           7       0.78      0.83      0.81       100
           8       0.16      0.99      0.27       100
           9       0.78      0.93      0.85       100
          10       0.72      0.83      0.77       100
          11       0.78      0.99      0.87       100
          12       0.95      0.58      0.72       100
          13       0.54      0.97      0.69       100
          14       0.83      0.48      0.61       100
          15       0.50      0.78      0.61       100
          16       0.77      0.66      0.71       100
          17       0.97      0.33      0.49       100
          18       1.00      0.84      0.91       100
          19       0.96      0.43      0.59       100
          20       0.76      0.65      0.70       100
          21       0.97      0.35      0.51       100
          22       0.97      0.73      0.83       100
          23       1.00      0.78      0.88       100
          24       0.96      0.66      0.78       100
          25       1.00      0.60      0.75       100
          26       0.90      0.76      0.83       100
          27       1.00      0.01      0.02       100
          28       0.85      0.88      0.86       100
          29       0.97      0.98      0.98       100
          30       1.00      0.02      0.04       100
          31       0.00      0.00      0.00       100
          32       1.00      0.14      0.25       100
          33       1.00      0.94      0.97       100
          34       0.99      0.87      0.93       100
          35       1.00      0.18      0.31       100
          36       1.00      0.75      0.86       100
          37       1.00      0.04      0.08       100
          38       0.97      0.90      0.93       100
          39       1.00      0.12      0.21       100

    accuracy                           0.66      4000
   macro avg       0.82      0.66      0.65      4000
weighted avg       0.82      0.66      0.65      4000

2023-12-25 11:13:08,934 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:13:36,841 [l2p_self_training.py] => 2486 unlabeled samples will be pseudo labeled
2023-12-25 11:13:36,842 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.39702333065164924
250
Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.099, Train_accy 92.40, Test_accy 66.62: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:37<00:00,  7.57s/it]
2023-12-25 11:14:15,300 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.099, Train_accy 92.40, Test_accy 66.62
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.76      0.85      0.80       100
           1       0.70      0.93      0.80       100
           2       0.69      0.92      0.79       100
           3       0.88      0.92      0.90       100
           4       0.72      0.95      0.82       100
           5       0.63      0.94      0.75       100
           6       0.30      0.98      0.45       100
           7       0.78      0.83      0.81       100
           8       0.16      0.99      0.27       100
           9       0.76      0.93      0.84       100
          10       0.75      0.83      0.79       100
          11       0.76      0.99      0.86       100
          12       0.98      0.58      0.73       100
          13       0.55      0.97      0.70       100
          14       0.85      0.46      0.60       100
          15       0.50      0.78      0.61       100
          16       0.79      0.65      0.71       100
          17       0.97      0.33      0.49       100
          18       1.00      0.84      0.91       100
          19       0.96      0.43      0.59       100
          20       0.79      0.66      0.72       100
          21       0.97      0.36      0.53       100
          22       0.97      0.72      0.83       100
          23       1.00      0.78      0.88       100
          24       0.97      0.67      0.79       100
          25       1.00      0.60      0.75       100
          26       0.92      0.77      0.84       100
          27       1.00      0.01      0.02       100
          28       0.85      0.88      0.86       100
          29       0.97      0.98      0.98       100
          30       1.00      0.04      0.08       100
          31       0.00      0.00      0.00       100
          32       0.96      0.22      0.36       100
          33       1.00      0.94      0.97       100
          34       0.99      0.80      0.88       100
          35       1.00      0.25      0.40       100
          36       1.00      0.84      0.91       100
          37       1.00      0.03      0.06       100
          38       0.96      0.92      0.94       100
          39       1.00      0.08      0.15       100

    accuracy                           0.67      4000
   macro avg       0.82      0.67      0.65      4000
weighted avg       0.82      0.67      0.65      4000

2023-12-25 11:14:38,812 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:15:06,780 [l2p_self_training.py] => 2521 unlabeled samples will be pseudo labeled
2023-12-25 11:15:06,781 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.40658468861562874
250
Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.092, Train_accy 93.20, Test_accy 66.80: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:38<00:00,  7.60s/it]
2023-12-25 11:15:45,149 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.092, Train_accy 93.20, Test_accy 66.80
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.78      0.85      0.81       100
           1       0.71      0.93      0.81       100
           2       0.68      0.92      0.78       100
           3       0.88      0.91      0.89       100
           4       0.73      0.94      0.82       100
           5       0.62      0.94      0.75       100
           6       0.29      0.98      0.45       100
           7       0.77      0.83      0.80       100
           8       0.15      0.98      0.27       100
           9       0.79      0.93      0.85       100
          10       0.75      0.82      0.78       100
          11       0.76      0.99      0.86       100
          12       0.98      0.58      0.73       100
          13       0.55      0.97      0.70       100
          14       0.85      0.46      0.60       100
          15       0.50      0.77      0.61       100
          16       0.82      0.63      0.71       100
          17       0.97      0.33      0.49       100
          18       1.00      0.84      0.91       100
          19       0.95      0.42      0.58       100
          20       0.78      0.66      0.71       100
          21       0.97      0.34      0.50       100
          22       0.97      0.72      0.83       100
          23       1.00      0.78      0.88       100
          24       0.97      0.67      0.79       100
          25       1.00      0.59      0.74       100
          26       0.93      0.77      0.84       100
          27       1.00      0.01      0.02       100
          28       0.85      0.87      0.86       100
          29       0.99      0.98      0.98       100
          30       1.00      0.03      0.06       100
          31       0.00      0.00      0.00       100
          32       0.96      0.23      0.37       100
          33       1.00      0.94      0.97       100
          34       0.99      0.88      0.93       100
          35       1.00      0.30      0.46       100
          36       1.00      0.85      0.92       100
          37       1.00      0.05      0.10       100
          38       0.95      0.92      0.93       100
          39       1.00      0.11      0.20       100

    accuracy                           0.67      4000
   macro avg       0.82      0.67      0.66      4000
weighted avg       0.82      0.67      0.66      4000

2023-12-25 11:16:08,904 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:16:36,801 [l2p_self_training.py] => 2592 unlabeled samples will be pseudo labeled
2023-12-25 11:16:36,801 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.44367283950617287
250
Task 3, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.172, Train_accy 93.20, Test_accy 67.65: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:37<00:00,  7.53s/it]
2023-12-25 11:17:15,002 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.172, Train_accy 93.20, Test_accy 67.65
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.80      0.86      0.83       100
           1       0.74      0.94      0.83       100
           2       0.72      0.92      0.81       100
           3       0.89      0.90      0.90       100
           4       0.73      0.94      0.82       100
           5       0.64      0.94      0.76       100
           6       0.30      0.98      0.46       100
           7       0.78      0.83      0.80       100
           8       0.16      0.99      0.27       100
           9       0.80      0.92      0.86       100
          10       0.75      0.82      0.78       100
          11       0.76      0.99      0.86       100
          12       0.97      0.58      0.72       100
          13       0.54      0.97      0.70       100
          14       0.88      0.45      0.60       100
          15       0.54      0.80      0.64       100
          16       0.85      0.55      0.67       100
          17       1.00      0.32      0.48       100
          18       1.00      0.83      0.91       100
          19       0.95      0.41      0.57       100
          20       0.78      0.66      0.71       100
          21       0.97      0.34      0.50       100
          22       0.97      0.69      0.81       100
          23       1.00      0.77      0.87       100
          24       0.97      0.68      0.80       100
          25       1.00      0.58      0.73       100
          26       0.93      0.75      0.83       100
          27       1.00      0.01      0.02       100
          28       0.85      0.87      0.86       100
          29       0.99      0.98      0.98       100
          30       1.00      0.03      0.06       100
          31       0.00      0.00      0.00       100
          32       0.79      0.48      0.60       100
          33       1.00      0.95      0.97       100
          34       0.99      0.77      0.87       100
          35       1.00      0.40      0.57       100
          36       1.00      0.90      0.95       100
          37       1.00      0.15      0.26       100
          38       0.94      0.92      0.93       100
          39       1.00      0.19      0.32       100

    accuracy                           0.68      4000
   macro avg       0.82      0.68      0.67      4000
weighted avg       0.82      0.68      0.67      4000

2023-12-25 11:17:38,486 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:18:06,350 [l2p_self_training.py] => 2658 unlabeled samples will be pseudo labeled
2023-12-25 11:18:06,350 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.5026335590669676
250
Task 3, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.162, Train_accy 94.80, Test_accy 68.15: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:37<00:00,  7.55s/it]
2023-12-25 11:18:44,646 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.162, Train_accy 94.80, Test_accy 68.15
              precision    recall  f1-score   support

           0       0.79      0.86      0.82       100
           1       0.72      0.94      0.82       100
           2       0.74      0.92      0.82       100
           3       0.87      0.90      0.89       100
           4       0.73      0.94      0.82       100
           5       0.64      0.94      0.76       100
           6       0.30      0.98      0.46       100
           7       0.78      0.84      0.81       100
           8       0.16      0.99      0.27       100
           9       0.80      0.93      0.86       100
          10       0.75      0.82      0.78       100
          11       0.79      0.98      0.87       100
          12       0.97      0.57      0.72       100
          13       0.56      0.97      0.71       100
          14       0.88      0.45      0.60       100
          15       0.54      0.79      0.64       100
          16       0.84      0.59      0.69       100
          17       0.97      0.32      0.48       100
          18       1.00      0.83      0.91       100
          19       0.95      0.41      0.57       100
          20       0.79      0.68      0.73       100
          21       0.97      0.35      0.51       100
          22       0.97      0.69      0.81       100
          23       1.00      0.78      0.88       100
          24       0.99      0.68      0.80       100
          25       1.00      0.59      0.74       100
          26       0.94      0.74      0.83       100
          27       1.00      0.01      0.02       100
          28       0.87      0.87      0.87       100
          29       0.99      0.98      0.98       100
          30       0.88      0.07      0.13       100
          31       1.00      0.04      0.08       100
          32       0.84      0.31      0.45       100
          33       1.00      0.96      0.98       100
          34       0.99      0.84      0.91       100
          35       1.00      0.35      0.52       100
          36       1.00      0.88      0.94       100
          37       1.00      0.21      0.35       100
          38       0.94      0.92      0.93       100
          39       0.92      0.34      0.50       100

    accuracy                           0.68      4000
   macro avg       0.85      0.68      0.68      4000
weighted avg       0.85      0.68      0.68      4000

2023-12-25 11:19:08,205 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:19:36,281 [l2p_self_training.py] => 2589 unlabeled samples will be pseudo labeled
2023-12-25 11:19:36,281 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.4982618771726535
2023-12-25 11:20:00,351 [trainer.py] => No NME accuracy.
2023-12-25 11:20:00,351 [trainer.py] => CNN: {'total': 68.15, '00-09': 92.4, '10-19': 67.3, '20-29': 63.7, '30-39': 49.2, 'old': 74.47, 'new': 49.2}
2023-12-25 11:20:00,351 [trainer.py] => CNN top1 curve: [93.8, 80.35, 74.4, 68.15]
2023-12-25 11:20:00,351 [trainer.py] => CNN top5 curve: [99.8, 98.7, 98.07, 97.35]

Average Accuracy (CNN): 79.175
2023-12-25 11:20:00,351 [trainer.py] => Average Accuracy (CNN): 79.175 



======================================================================================
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
======================================================================================

第一个任务不打伪标签，只拿有标签数据训练模型，第二个任务开始打伪标签
(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2023-12-25 10:56:05,455 [trainer.py] => config: ./exps/l2p_self_training.json
2023-12-25 10:56:05,455 [trainer.py] => prefix:  
2023-12-25 10:56:05,455 [trainer.py] => dataset: cifar224
2023-12-25 10:56:05,456 [trainer.py] => memory_size: 0
2023-12-25 10:56:05,456 [trainer.py] => memory_per_class: 0
2023-12-25 10:56:05,456 [trainer.py] => fixed_memory: False
2023-12-25 10:56:05,456 [trainer.py] => shuffle: True
2023-12-25 10:56:05,456 [trainer.py] => init_cls: 10
2023-12-25 10:56:05,456 [trainer.py] => increment: 10
2023-12-25 10:56:05,456 [trainer.py] => model_name: l2p_self_training
2023-12-25 10:56:05,456 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2023-12-25 10:56:05,456 [trainer.py] => get_original_backbone: True
2023-12-25 10:56:05,456 [trainer.py] => device: [device(type='cuda', index=5)]
2023-12-25 10:56:05,456 [trainer.py] => seed: 1993
2023-12-25 10:56:05,456 [trainer.py] => tuned_epoch: 5
2023-12-25 10:56:05,456 [trainer.py] => init_lr: 0.001875
2023-12-25 10:56:05,456 [trainer.py] => batch_size: 16
2023-12-25 10:56:05,456 [trainer.py] => weight_decay: 0
2023-12-25 10:56:05,456 [trainer.py] => min_lr: 1e-05
2023-12-25 10:56:05,456 [trainer.py] => optimizer: adam
2023-12-25 10:56:05,456 [trainer.py] => scheduler: constant
2023-12-25 10:56:05,456 [trainer.py] => reinit_optimizer: True
2023-12-25 10:56:05,456 [trainer.py] => global_pool: token
2023-12-25 10:56:05,456 [trainer.py] => head_type: prompt
2023-12-25 10:56:05,456 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2023-12-25 10:56:05,456 [trainer.py] => pretrained: True
2023-12-25 10:56:05,456 [trainer.py] => drop: 0.0
2023-12-25 10:56:05,456 [trainer.py] => drop_path: 0.0
2023-12-25 10:56:05,456 [trainer.py] => prompt_pool: True
2023-12-25 10:56:05,456 [trainer.py] => pool_size: 10
2023-12-25 10:56:05,456 [trainer.py] => length: 5
2023-12-25 10:56:05,456 [trainer.py] => top_k: 5
2023-12-25 10:56:05,457 [trainer.py] => initializer: uniform
2023-12-25 10:56:05,457 [trainer.py] => prompt_key: True
2023-12-25 10:56:05,457 [trainer.py] => prompt_key_init: uniform
2023-12-25 10:56:05,457 [trainer.py] => use_prompt_mask: False
2023-12-25 10:56:05,457 [trainer.py] => shared_prompt_pool: False
2023-12-25 10:56:05,457 [trainer.py] => shared_prompt_key: False
2023-12-25 10:56:05,457 [trainer.py] => batchwise_prompt: True
2023-12-25 10:56:05,457 [trainer.py] => embedding_key: cls
2023-12-25 10:56:05,457 [trainer.py] => predefined_key: 
2023-12-25 10:56:05,457 [trainer.py] => pull_constraint: True
2023-12-25 10:56:05,457 [trainer.py] => pull_constraint_coeff: 0.1
2023-12-25 10:56:05,457 [trainer.py] => semi_supervised_mode: True
2023-12-25 10:56:05,457 [trainer.py] => labeled_ratio: 0.05
2023-12-25 10:56:05,457 [trainer.py] => unlabeled_data_distribution_mode: no_ood
2023-12-25 10:56:05,457 [trainer.py] => confidence_threshold: 0.9
2023-12-25 10:56:05,457 [trainer.py] => max_self_training_iteration: 5
Files already downloaded and verified
Files already downloaded and verified
2023-12-25 10:56:07,412 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-25 10:56:09,113 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2023-12-25 10:56:09,114 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2023-12-25 10:56:11,561 [l2p_self_training.py] => 85,940,836 model total parameters.
2023-12-25 10:56:11,562 [l2p_self_training.py] => 122,980 model training parameters.
2023-12-25 10:56:11,562 [l2p_self_training.py] => prompt.prompt: 38400
2023-12-25 10:56:11,562 [l2p_self_training.py] => prompt.prompt_key: 7680
2023-12-25 10:56:11,562 [l2p_self_training.py] => head.weight: 76800
2023-12-25 10:56:11,562 [l2p_self_training.py] => head.bias: 100
2023-12-25 10:56:11,564 [trainer.py] => All params: 171816392
2023-12-25 10:56:11,565 [trainer.py] => Trainable params: 122980
2023-12-25 10:56:11,565 [l2p_self_training.py] => Learning on 0-10
250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.02s/it]
2023-12-25 10:56:32,484 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40
              precision    recall  f1-score   support

           0       0.78      0.99      0.87       100
           1       0.84      1.00      0.91       100
           2       1.00      0.87      0.93       100
           3       0.85      1.00      0.92       100
           4       1.00      0.87      0.93       100
           5       1.00      0.83      0.91       100
           6       0.85      0.88      0.87       100
           7       0.79      0.97      0.87       100
           8       0.94      0.68      0.79       100
           9       1.00      0.81      0.90       100

    accuracy                           0.89      1000
   macro avg       0.91      0.89      0.89      1000
weighted avg       0.91      0.89      0.89      1000

2023-12-25 10:56:44,852 [trainer.py] => No NME accuracy.
2023-12-25 10:56:44,852 [trainer.py] => CNN: {'total': 89.3, '00-09': 89.3, 'old': 0, 'new': 89.3}
2023-12-25 10:56:44,852 [trainer.py] => CNN top1 curve: [89.3]
2023-12-25 10:56:44,853 [trainer.py] => CNN top5 curve: [98.5]

Average Accuracy (CNN): 89.3
2023-12-25 10:56:44,853 [trainer.py] => Average Accuracy (CNN): 89.3 

2023-12-25 10:56:44,855 [trainer.py] => All params: 171816392
2023-12-25 10:56:44,856 [trainer.py] => Trainable params: 122980
2023-12-25 10:56:44,857 [l2p_self_training.py] => Learning on 10-20
250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.393, Train_accy 84.80, Test_accy 84.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.10s/it]
2023-12-25 10:57:10,414 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.393, Train_accy 84.80, Test_accy 84.40
              precision    recall  f1-score   support

           0       0.82      0.98      0.89       100
           1       0.94      0.96      0.95       100
           2       1.00      0.82      0.90       100
           3       0.83      0.99      0.90       100
           4       1.00      0.68      0.81       100
           5       1.00      0.58      0.73       100
           6       0.89      0.85      0.87       100
           7       0.87      0.80      0.83       100
           8       0.90      0.52      0.66       100
           9       0.99      0.70      0.82       100
          10       0.57      0.96      0.71       100
          11       0.99      0.92      0.95       100
          12       0.69      0.92      0.79       100
          13       0.99      0.96      0.97       100
          14       0.74      0.75      0.74       100
          15       1.00      0.95      0.97       100
          16       0.91      0.92      0.92       100
          17       0.57      0.88      0.69       100
          18       0.95      0.91      0.93       100
          19       0.87      0.83      0.85       100

    accuracy                           0.84      2000
   macro avg       0.88      0.84      0.85      2000
weighted avg       0.88      0.84      0.85      2000

2023-12-25 10:57:22,574 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:57:50,966 [l2p_self_training.py] => 1934 unlabeled samples will be pseudo labeled
2023-12-25 10:57:50,966 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9865563598759048
2184
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 90.89, Test_accy 83.20: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [01:54<00:00, 22.87s/it]
2023-12-25 10:59:45,540 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 90.89, Test_accy 83.20
              precision    recall  f1-score   support

           0       0.65      1.00      0.79       100
           1       0.82      0.95      0.88       100
           2       0.99      0.92      0.95       100
           3       0.65      0.99      0.79       100
           4       1.00      0.53      0.69       100
           5       0.95      0.69      0.80       100
           6       0.69      0.82      0.75       100
           7       0.63      0.90      0.74       100
           8       0.59      0.80      0.68       100
           9       0.99      0.66      0.79       100
          10       0.71      0.89      0.79       100
          11       0.93      0.97      0.95       100
          12       0.94      0.83      0.88       100
          13       1.00      0.96      0.98       100
          14       0.97      0.59      0.73       100
          15       1.00      0.93      0.96       100
          16       0.96      0.91      0.93       100
          17       0.98      0.57      0.72       100
          18       0.99      0.93      0.96       100
          19       0.96      0.80      0.87       100

    accuracy                           0.83      2000
   macro avg       0.87      0.83      0.83      2000
weighted avg       0.87      0.83      0.83      2000

2023-12-25 10:59:57,742 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:00:26,126 [l2p_self_training.py] => 2288 unlabeled samples will be pseudo labeled
2023-12-25 11:00:26,126 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9768356643356644
2538
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss inf, Train_accy 89.91, Test_accy 39.15: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [02:09<00:00, 25.98s/it]
2023-12-25 11:02:36,329 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss inf, Train_accy 89.91, Test_accy 39.15
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.86      0.82      0.84       100
           1       0.48      0.97      0.64       100
           2       0.67      0.95      0.79       100
           3       0.19      1.00      0.32       100
           4       0.96      0.25      0.40       100
           5       0.68      0.50      0.57       100
           6       0.36      0.93      0.52       100
           7       0.31      0.87      0.45       100
           8       0.28      0.84      0.41       100
           9       0.81      0.70      0.75       100
          10       0.00      0.00      0.00       100
          11       0.00      0.00      0.00       100
          12       0.00      0.00      0.00       100
          13       0.00      0.00      0.00       100
          14       0.00      0.00      0.00       100
          15       0.00      0.00      0.00       100
          16       0.00      0.00      0.00       100
          17       0.00      0.00      0.00       100
          18       0.00      0.00      0.00       100
          19       0.00      0.00      0.00       100

    accuracy                           0.39      2000
   macro avg       0.28      0.39      0.29      2000
weighted avg       0.28      0.39      0.29      2000

2023-12-25 11:02:48,588 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:03:17,003 [l2p_self_training.py] => 327 unlabeled samples will be pseudo labeled
2023-12-25 11:03:17,003 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.0
577
Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss inf, Train_accy 24.09, Test_accy 14.55: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:40<00:00,  8.19s/it]
2023-12-25 11:03:58,190 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss inf, Train_accy 24.09, Test_accy 14.55
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           1       0.09      0.99      0.16       100
           2       1.00      0.01      0.02       100
           3       0.14      1.00      0.25       100
           4       0.00      0.00      0.00       100
           5       0.00      0.00      0.00       100
           6       0.12      0.04      0.06       100
           7       0.92      0.44      0.59       100
           8       0.41      0.43      0.42       100
           9       0.00      0.00      0.00       100
          10       0.00      0.00      0.00       100
          11       0.00      0.00      0.00       100
          12       0.00      0.00      0.00       100
          13       0.00      0.00      0.00       100
          14       0.00      0.00      0.00       100
          15       0.00      0.00      0.00       100
          16       0.00      0.00      0.00       100
          17       0.00      0.00      0.00       100
          18       0.00      0.00      0.00       100
          19       0.00      0.00      0.00       100

    accuracy                           0.15      2000
   macro avg       0.13      0.15      0.08      2000
weighted avg       0.13      0.15      0.08      2000

2023-12-25 11:04:10,525 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:04:38,903 [l2p_self_training.py] => 0 unlabeled samples will be pseudo labeled
2023-12-25 11:04:38,903 [l2p_self_training.py] => pseudo labeling finish
/home/lhz/code/LAMDA-PILOT/utils/toolkit.py:82: RuntimeWarning: invalid value encountered in scalar divide
  pseudo_accuracy = np.sum(pseudo_labels == ground_truth) / len(ground_truth)
Pseudo Accuracy:  nan
Traceback (most recent call last):
  File "/home/lhz/code/LAMDA-PILOT/main.py", line 25, in <module>
    main()
  File "/home/lhz/code/LAMDA-PILOT/main.py", line 11, in main
    train(args)
  File "/home/lhz/code/LAMDA-PILOT/trainer.py", line 18, in train
    _train(args)
  File "/home/lhz/code/LAMDA-PILOT/trainer.py", line 74, in _train
    model.incremental_train(data_manager)
  File "/home/lhz/code/LAMDA-PILOT/models/l2p_self_training.py", line 76, in incremental_train
    self._train(self.train_labeled_dataset, self.train_unlabeled_dataset, self.test_loader)
  File "/home/lhz/code/LAMDA-PILOT/models/l2p_self_training.py", line 92, in _train
    self._init_train(train_labeled_dataset, train_unlabeled_dataset, test_loader, optimizer, scheduler)
  File "/home/lhz/code/LAMDA-PILOT/models/l2p_self_training.py", line 252, in _init_train
    train_unlabeled_dataset = self._pseudo_label(unlabeled_dataset)
  File "/home/lhz/code/LAMDA-PILOT/models/l2p_self_training.py", line 335, in _pseudo_label
    confusion_matrix_on_pseudo_labels(pseudo_label_dataset.labels[selected_unlabeled_data_idx], pseudo_label)
  File "/home/lhz/code/LAMDA-PILOT/utils/toolkit.py", line 86, in confusion_matrix_on_pseudo_labels
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
  File "/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/seaborn/matrix.py", line 446, in heatmap
    plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,
  File "/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/seaborn/matrix.py", line 163, in __init__
    self._determine_cmap_params(plot_data, vmin, vmax,
  File "/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/seaborn/matrix.py", line 202, in _determine_cmap_params
    vmin = np.nanmin(calc_data)
  File "<__array_function__ internals>", line 200, in nanmin
  File "/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/numpy/lib/nanfunctions.py", line 343, in nanmin
    res = np.fmin.reduce(a, axis=axis, out=out, **kwargs)
ValueError: zero-size array to reduction operation fmin which has no identity