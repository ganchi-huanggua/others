1. 有标签数据训练分类器，给无标签数据打伪标签，只用最新打好伪标签的数据重新训练，之前打好伪标签的数据或有标签数据不在参与训练，而且训练后只在之前没有打过伪标签的无标签数据上打伪标签，以此迭代
2. 有标签数据训练分类器，给无标签数据打伪标签，用打好伪标签的数据和之前打好伪标签和有标签数据重训练，但是在训练后还是只在没有打过伪标签的无标签数据上打伪标签
3. 有标签数据训练分类器，给无标签数据打伪标签，用打好伪标签的数据和之前打好伪标签和有标签数据重训练，而且在训练后对所有无标签数据打伪标签
在3下进行实验

原始情况：
(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2023-12-25 10:16:59,290 [trainer.py] => config: ./exps/l2p_self_training.json
2023-12-25 10:16:59,291 [trainer.py] => prefix:  
2023-12-25 10:16:59,291 [trainer.py] => dataset: cifar224
2023-12-25 10:16:59,291 [trainer.py] => memory_size: 0
2023-12-25 10:16:59,291 [trainer.py] => memory_per_class: 0
2023-12-25 10:16:59,291 [trainer.py] => fixed_memory: False
2023-12-25 10:16:59,291 [trainer.py] => shuffle: True
2023-12-25 10:16:59,291 [trainer.py] => init_cls: 10
2023-12-25 10:16:59,291 [trainer.py] => increment: 10
2023-12-25 10:16:59,291 [trainer.py] => model_name: l2p_self_training
2023-12-25 10:16:59,291 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2023-12-25 10:16:59,291 [trainer.py] => get_original_backbone: True
2023-12-25 10:16:59,291 [trainer.py] => device: [device(type='cuda', index=0)]
2023-12-25 10:16:59,291 [trainer.py] => seed: 1993
2023-12-25 10:16:59,291 [trainer.py] => tuned_epoch: 5
2023-12-25 10:16:59,291 [trainer.py] => init_lr: 0.001875
2023-12-25 10:16:59,291 [trainer.py] => batch_size: 16
2023-12-25 10:16:59,291 [trainer.py] => weight_decay: 0
2023-12-25 10:16:59,291 [trainer.py] => min_lr: 1e-05
2023-12-25 10:16:59,291 [trainer.py] => optimizer: adam
2023-12-25 10:16:59,291 [trainer.py] => scheduler: constant
2023-12-25 10:16:59,291 [trainer.py] => reinit_optimizer: True
2023-12-25 10:16:59,291 [trainer.py] => global_pool: token
2023-12-25 10:16:59,291 [trainer.py] => head_type: prompt
2023-12-25 10:16:59,291 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2023-12-25 10:16:59,292 [trainer.py] => pretrained: True
2023-12-25 10:16:59,292 [trainer.py] => drop: 0.0
2023-12-25 10:16:59,292 [trainer.py] => drop_path: 0.0
2023-12-25 10:16:59,292 [trainer.py] => prompt_pool: True
2023-12-25 10:16:59,292 [trainer.py] => pool_size: 10
2023-12-25 10:16:59,292 [trainer.py] => length: 5
2023-12-25 10:16:59,292 [trainer.py] => top_k: 5
2023-12-25 10:16:59,292 [trainer.py] => initializer: uniform
2023-12-25 10:16:59,292 [trainer.py] => prompt_key: True
2023-12-25 10:16:59,292 [trainer.py] => prompt_key_init: uniform
2023-12-25 10:16:59,292 [trainer.py] => use_prompt_mask: False
2023-12-25 10:16:59,292 [trainer.py] => shared_prompt_pool: False
2023-12-25 10:16:59,292 [trainer.py] => shared_prompt_key: False
2023-12-25 10:16:59,292 [trainer.py] => batchwise_prompt: True
2023-12-25 10:16:59,292 [trainer.py] => embedding_key: cls
2023-12-25 10:16:59,292 [trainer.py] => predefined_key: 
2023-12-25 10:16:59,292 [trainer.py] => pull_constraint: True
2023-12-25 10:16:59,292 [trainer.py] => pull_constraint_coeff: 0.1
2023-12-25 10:16:59,292 [trainer.py] => semi_supervised_mode: True
2023-12-25 10:16:59,292 [trainer.py] => labeled_ratio: 0.05
2023-12-25 10:16:59,292 [trainer.py] => unlabeled_data_distribution_mode: no_ood
2023-12-25 10:16:59,292 [trainer.py] => confidence_threshold: 0.9
2023-12-25 10:16:59,292 [trainer.py] => max_self_training_iteration: 5
Files already downloaded and verified
Files already downloaded and verified
2023-12-25 10:17:01,248 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-25 10:17:03,029 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2023-12-25 10:17:03,029 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2023-12-25 10:17:05,313 [l2p_self_training.py] => 85,940,836 model total parameters.
2023-12-25 10:17:05,314 [l2p_self_training.py] => 122,980 model training parameters.
2023-12-25 10:17:05,314 [l2p_self_training.py] => prompt.prompt: 38400
2023-12-25 10:17:05,314 [l2p_self_training.py] => prompt.prompt_key: 7680
2023-12-25 10:17:05,314 [l2p_self_training.py] => head.weight: 76800
2023-12-25 10:17:05,315 [l2p_self_training.py] => head.bias: 100
2023-12-25 10:17:05,316 [trainer.py] => All params: 171816392
2023-12-25 10:17:05,317 [trainer.py] => Trainable params: 122980
2023-12-25 10:17:05,317 [l2p_self_training.py] => Learning on 0-10
250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.04s/it]
2023-12-25 10:17:26,299 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40
              precision    recall  f1-score   support

           0       0.78      0.99      0.87       100
           1       0.84      1.00      0.91       100
           2       1.00      0.87      0.93       100
           3       0.85      1.00      0.92       100
           4       1.00      0.87      0.93       100
           5       1.00      0.83      0.91       100
           6       0.85      0.88      0.87       100
           7       0.79      0.97      0.87       100
           8       0.94      0.68      0.79       100
           9       1.00      0.81      0.90       100

    accuracy                           0.89      1000
   macro avg       0.91      0.89      0.89      1000
weighted avg       0.91      0.89      0.89      1000

2023-12-25 10:17:32,562 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:18:00,574 [l2p_self_training.py] => 1833 unlabeled samples will be pseudo labeled
2023-12-25 10:18:00,574 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9896344789961812
2083
Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.043, Train_accy 89.73, Test_accy 93.20: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [01:43<00:00, 20.66s/it]
2023-12-25 10:19:44,141 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.043, Train_accy 89.73, Test_accy 93.20
              precision    recall  f1-score   support

           0       0.95      0.94      0.94       100
           1       0.96      0.99      0.98       100
           2       0.95      0.96      0.96       100
           3       0.75      1.00      0.86       100
           4       0.92      0.94      0.93       100
           5       0.99      0.96      0.97       100
           6       0.96      0.92      0.94       100
           7       0.94      0.96      0.95       100
           8       1.00      0.76      0.86       100
           9       0.98      0.89      0.93       100

    accuracy                           0.93      1000
   macro avg       0.94      0.93      0.93      1000
weighted avg       0.94      0.93      0.93      1000

2023-12-25 10:19:50,795 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:20:19,309 [l2p_self_training.py] => 3366 unlabeled samples will be pseudo labeled
2023-12-25 10:20:19,310 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9506833036244801
3616
Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.044, Train_accy 89.13, Test_accy 93.50: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [02:54<00:00, 34.88s/it]
2023-12-25 10:23:14,001 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.044, Train_accy 89.13, Test_accy 93.50
              precision    recall  f1-score   support

           0       0.94      0.92      0.93       100
           1       0.98      0.99      0.99       100
           2       0.92      0.98      0.95       100
           3       0.81      1.00      0.89       100
           4       0.96      0.91      0.93       100
           5       0.99      0.94      0.96       100
           6       0.89      0.93      0.91       100
           7       0.96      0.90      0.93       100
           8       0.98      0.85      0.91       100
           9       0.97      0.93      0.95       100

    accuracy                           0.94      1000
   macro avg       0.94      0.93      0.94      1000
weighted avg       0.94      0.94      0.94      1000

2023-12-25 10:23:20,313 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:23:48,807 [l2p_self_training.py] => 3319 unlabeled samples will be pseudo labeled
2023-12-25 10:23:48,807 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9647484181982525
3569
Task 0, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.030, Train_accy 90.89, Test_accy 94.40: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [02:52<00:00, 34.41s/it]
2023-12-25 10:26:41,055 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.030, Train_accy 90.89, Test_accy 94.40
              precision    recall  f1-score   support

           0       0.93      0.88      0.90       100
           1       0.99      0.97      0.98       100
           2       0.94      0.98      0.96       100
           3       0.92      0.99      0.95       100
           4       0.95      0.91      0.93       100
           5       0.98      0.97      0.97       100
           6       0.86      0.96      0.91       100
           7       0.99      0.92      0.95       100
           8       0.95      0.91      0.93       100
           9       0.96      0.95      0.95       100

    accuracy                           0.94      1000
   macro avg       0.95      0.94      0.94      1000
weighted avg       0.95      0.94      0.94      1000

2023-12-25 10:26:47,402 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:27:15,987 [l2p_self_training.py] => 3667 unlabeled samples will be pseudo labeled
2023-12-25 10:27:15,987 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9645486773929642
3917
Task 0, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.050, Train_accy 91.22, Test_accy 93.80: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [03:08<00:00, 37.65s/it]
2023-12-25 10:30:24,376 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.050, Train_accy 91.22, Test_accy 93.80
              precision    recall  f1-score   support

           0       0.99      0.87      0.93       100
           1       1.00      0.97      0.98       100
           2       1.00      0.86      0.92       100
           3       0.99      0.99      0.99       100
           4       0.86      0.96      0.91       100
           5       0.96      0.97      0.97       100
           6       0.87      0.96      0.91       100
           7       0.99      0.89      0.94       100
           8       0.84      0.98      0.90       100
           9       0.93      0.93      0.93       100

    accuracy                           0.94      1000
   macro avg       0.94      0.94      0.94      1000
weighted avg       0.94      0.94      0.94      1000

2023-12-25 10:30:30,775 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:30:59,387 [l2p_self_training.py] => 3680 unlabeled samples will be pseudo labeled
2023-12-25 10:30:59,388 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9578804347826086
2023-12-25 10:31:05,996 [trainer.py] => No NME accuracy.
2023-12-25 10:31:05,997 [trainer.py] => CNN: {'total': 93.8, '00-09': 93.8, 'old': 0, 'new': 93.8}
2023-12-25 10:31:05,997 [trainer.py] => CNN top1 curve: [93.8]
2023-12-25 10:31:05,997 [trainer.py] => CNN top5 curve: [99.8]

Average Accuracy (CNN): 93.8
2023-12-25 10:31:05,997 [trainer.py] => Average Accuracy (CNN): 93.8 

2023-12-25 10:31:06,000 [trainer.py] => All params: 171816392
2023-12-25 10:31:06,002 [trainer.py] => Trainable params: 122980
2023-12-25 10:31:06,002 [l2p_self_training.py] => Learning on 10-20
250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.014, Train_accy 89.20, Test_accy 74.70: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.16s/it]
2023-12-25 10:31:31,871 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.014, Train_accy 89.20, Test_accy 74.70
              precision    recall  f1-score   support

           0       0.94      0.85      0.89       100
           1       0.96      0.98      0.97       100
           2       0.94      0.87      0.90       100
           3       0.92      0.97      0.94       100
           4       0.76      0.96      0.85       100
           5       0.67      0.96      0.79       100
           6       0.72      0.97      0.83       100
           7       0.71      0.89      0.79       100
           8       0.24      0.98      0.39       100
           9       0.81      0.92      0.86       100
          10       0.95      0.57      0.71       100
          11       1.00      0.79      0.88       100
          12       1.00      0.35      0.52       100
          13       1.00      0.98      0.99       100
          14       0.93      0.27      0.42       100
          15       0.99      0.77      0.87       100
          16       1.00      0.67      0.80       100
          17       1.00      0.20      0.33       100
          18       1.00      0.77      0.87       100
          19       1.00      0.22      0.36       100

    accuracy                           0.75      2000
   macro avg       0.88      0.75      0.75      2000
weighted avg       0.88      0.75      0.75      2000

2023-12-25 10:31:44,145 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:32:12,785 [l2p_self_training.py] => 2209 unlabeled samples will be pseudo labeled
2023-12-25 10:32:12,786 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.5450430058850159
2459
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 36.19, Test_accy 5.00: 100%|██████████████████████████████████████████████████████████████████████████| 5/5 [02:07<00:00, 25.58s/it]
2023-12-25 10:34:21,132 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 36.19, Test_accy 5.00
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           1       0.00      0.00      0.00       100
           2       0.00      0.00      0.00       100
           3       0.00      0.00      0.00       100
           4       0.00      0.00      0.00       100
           5       0.00      0.00      0.00       100
           6       0.00      0.00      0.00       100
           7       0.00      0.00      0.00       100
           8       0.05      1.00      0.10       100
           9       0.00      0.00      0.00       100
          10       0.00      0.00      0.00       100
          11       0.00      0.00      0.00       100
          12       0.00      0.00      0.00       100
          13       0.00      0.00      0.00       100
          14       0.00      0.00      0.00       100
          15       0.00      0.00      0.00       100
          16       0.00      0.00      0.00       100
          17       0.00      0.00      0.00       100
          18       0.00      0.00      0.00       100
          19       0.00      0.00      0.00       100

    accuracy                           0.05      2000
   macro avg       0.00      0.05      0.00      2000
weighted avg       0.00      0.05      0.00      2000

2023-12-25 10:34:33,419 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:35:02,330 [l2p_self_training.py] => 4761 unlabeled samples will be pseudo labeled
2023-12-25 10:35:02,330 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.0
5011
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss inf, Train_accy 0.78, Test_accy 5.00: 100%|███████████████████████████████████████████████████████████████████████████| 5/5 [04:05<00:00, 49.02s/it]
2023-12-25 10:39:07,636 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss inf, Train_accy 0.78, Test_accy 5.00
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           1       0.00      0.00      0.00       100
           2       0.00      0.00      0.00       100
           3       0.00      0.00      0.00       100
           4       0.00      0.00      0.00       100
           5       0.00      0.00      0.00       100
           6       0.00      0.00      0.00       100
           7       0.00      0.00      0.00       100
           8       0.05      1.00      0.10       100
           9       0.00      0.00      0.00       100
          10       0.00      0.00      0.00       100
          11       0.00      0.00      0.00       100
          12       0.00      0.00      0.00       100
          13       0.00      0.00      0.00       100
          14       0.00      0.00      0.00       100
          15       0.00      0.00      0.00       100
          16       0.00      0.00      0.00       100
          17       0.00      0.00      0.00       100
          18       0.00      0.00      0.00       100
          19       0.00      0.00      0.00       100

    accuracy                           0.05      2000
   macro avg       0.00      0.05      0.00      2000
weighted avg       0.00      0.05      0.00      2000

2023-12-25 10:39:19,879 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:39:48,630 [l2p_self_training.py] => 4761 unlabeled samples will be pseudo labeled
2023-12-25 10:39:48,630 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.0
5011
Task 1, Epoch 1/5, Self_training_Iteration: 3 => Loss inf, Train_accy 0.74:  20%|██████████████████▏                                                                        | 1/5 [00:46<03:05, 46.25s/it]                                                                     | 1/5 [00:20<01:20, 20.05s/it]Task 1, Epoch 1/5, Self_training_Iteration: 1 => Loss inf, Train_accy 48.68:  20%|██████████████████                                                                        | 1/5 [00:25<01:40, 25.08s/it]

======================================================================================
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
======================================================================================

将打错的伪标签舍弃，不作为模型新一轮自训练的训练样本，将剩下的打对伪标签的样本作为模型新一轮自训练的样本
(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2023-12-25 23:29:36,716 [trainer.py] => config: ./exps/l2p_self_training.json
2023-12-25 23:29:36,716 [trainer.py] => prefix:  
2023-12-25 23:29:36,716 [trainer.py] => dataset: cifar224
2023-12-25 23:29:36,716 [trainer.py] => memory_size: 0
2023-12-25 23:29:36,716 [trainer.py] => memory_per_class: 0
2023-12-25 23:29:36,716 [trainer.py] => fixed_memory: False
2023-12-25 23:29:36,716 [trainer.py] => shuffle: True
2023-12-25 23:29:36,716 [trainer.py] => init_cls: 10
2023-12-25 23:29:36,716 [trainer.py] => increment: 10
2023-12-25 23:29:36,716 [trainer.py] => model_name: l2p_self_training
2023-12-25 23:29:36,716 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2023-12-25 23:29:36,716 [trainer.py] => get_original_backbone: True
2023-12-25 23:29:36,716 [trainer.py] => device: [device(type='cuda', index=5)]
2023-12-25 23:29:36,716 [trainer.py] => seed: 1993
2023-12-25 23:29:36,716 [trainer.py] => tuned_epoch: 5
2023-12-25 23:29:36,716 [trainer.py] => init_lr: 0.001875
2023-12-25 23:29:36,716 [trainer.py] => batch_size: 16
2023-12-25 23:29:36,717 [trainer.py] => weight_decay: 0
2023-12-25 23:29:36,717 [trainer.py] => min_lr: 1e-05
2023-12-25 23:29:36,717 [trainer.py] => optimizer: adam
2023-12-25 23:29:36,717 [trainer.py] => scheduler: constant
2023-12-25 23:29:36,717 [trainer.py] => reinit_optimizer: True
2023-12-25 23:29:36,717 [trainer.py] => global_pool: token
2023-12-25 23:29:36,717 [trainer.py] => head_type: prompt
2023-12-25 23:29:36,717 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2023-12-25 23:29:36,717 [trainer.py] => pretrained: True
2023-12-25 23:29:36,717 [trainer.py] => drop: 0.0
2023-12-25 23:29:36,717 [trainer.py] => drop_path: 0.0
2023-12-25 23:29:36,717 [trainer.py] => prompt_pool: True
2023-12-25 23:29:36,717 [trainer.py] => pool_size: 10
2023-12-25 23:29:36,717 [trainer.py] => length: 5
2023-12-25 23:29:36,717 [trainer.py] => top_k: 5
2023-12-25 23:29:36,717 [trainer.py] => initializer: uniform
2023-12-25 23:29:36,717 [trainer.py] => prompt_key: True
2023-12-25 23:29:36,717 [trainer.py] => prompt_key_init: uniform
2023-12-25 23:29:36,717 [trainer.py] => use_prompt_mask: False
2023-12-25 23:29:36,717 [trainer.py] => shared_prompt_pool: False
2023-12-25 23:29:36,717 [trainer.py] => shared_prompt_key: False
2023-12-25 23:29:36,717 [trainer.py] => batchwise_prompt: True
2023-12-25 23:29:36,717 [trainer.py] => embedding_key: cls
2023-12-25 23:29:36,717 [trainer.py] => predefined_key: 
2023-12-25 23:29:36,717 [trainer.py] => pull_constraint: True
2023-12-25 23:29:36,717 [trainer.py] => pull_constraint_coeff: 0.1
2023-12-25 23:29:36,717 [trainer.py] => semi_supervised_mode: True
2023-12-25 23:29:36,718 [trainer.py] => labeled_ratio: 0.05
2023-12-25 23:29:36,718 [trainer.py] => unlabeled_data_distribution_mode: no_ood
2023-12-25 23:29:36,718 [trainer.py] => confidence_threshold: 0.9
2023-12-25 23:29:36,718 [trainer.py] => max_self_training_iteration: 5
Files already downloaded and verified
Files already downloaded and verified
2023-12-25 23:29:38,797 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-25 23:29:41,073 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2023-12-25 23:29:41,074 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2023-12-25 23:29:44,512 [l2p_self_training.py] => 85,940,836 model total parameters.
2023-12-25 23:29:44,513 [l2p_self_training.py] => 122,980 model training parameters.
2023-12-25 23:29:44,514 [l2p_self_training.py] => prompt.prompt: 38400
2023-12-25 23:29:44,514 [l2p_self_training.py] => prompt.prompt_key: 7680
2023-12-25 23:29:44,514 [l2p_self_training.py] => head.weight: 76800
2023-12-25 23:29:44,514 [l2p_self_training.py] => head.bias: 100
2023-12-25 23:29:44,516 [trainer.py] => All params: 171816392
2023-12-25 23:29:44,518 [trainer.py] => Trainable params: 122980
2023-12-25 23:29:44,518 [l2p_self_training.py] => Learning on 0-10
250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40: 100%|█████████████| 5/5 [00:20<00:00,  4.06s/it]
2023-12-25 23:30:05,548 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40
              precision    recall  f1-score   support

           0       0.78      0.99      0.87       100
           1       0.84      1.00      0.91       100
           2       1.00      0.87      0.93       100
           3       0.85      1.00      0.92       100
           4       1.00      0.87      0.93       100
           5       1.00      0.83      0.91       100
           6       0.85      0.88      0.87       100
           7       0.79      0.97      0.87       100
           8       0.94      0.68      0.79       100
           9       1.00      0.81      0.90       100

    accuracy                           0.89      1000
   macro avg       0.91      0.89      0.89      1000
weighted avg       0.91      0.89      0.89      1000

2023-12-25 23:30:11,725 [l2p_self_training.py] => pseudo labeling start
[3 1 3 ... 6 0 6]
2023-12-25 23:30:39,760 [l2p_self_training.py] => 1833 unlabeled samples will be pseudo labeled
2023-12-25 23:30:39,760 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9896344789961812
2064
Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.025, Train_accy 90.94, Test_accy 94.30: 100%|█████████████| 5/5 [01:41<00:00, 20.32s/it]
2023-12-25 23:32:21,630 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.025, Train_accy 90.94, Test_accy 94.30
              precision    recall  f1-score   support

           0       0.85      1.00      0.92       100
           1       1.00      0.96      0.98       100
           2       0.92      0.99      0.95       100
           3       0.96      0.99      0.98       100
           4       0.92      0.92      0.92       100
           5       0.92      0.98      0.95       100
           6       0.98      0.89      0.93       100
           7       0.97      0.92      0.94       100
           8       0.96      0.86      0.91       100
           9       0.98      0.92      0.95       100

    accuracy                           0.94      1000
   macro avg       0.95      0.94      0.94      1000
weighted avg       0.95      0.94      0.94      1000

2023-12-25 23:32:27,953 [l2p_self_training.py] => pseudo labeling start
[3 1 8 ... 5 0 0]
2023-12-25 23:32:56,236 [l2p_self_training.py] => 3553 unlabeled samples will be pseudo labeled
2023-12-25 23:32:56,237 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9636926540951308
3674
Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.080, Train_accy 92.03, Test_accy 96.20: 100%|████████████| 5/5 [02:55<00:00, 35.16s/it]
2023-12-25 23:35:52,377 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.080, Train_accy 92.03, Test_accy 96.20
              precision    recall  f1-score   support

           0       0.98      1.00      0.99       100
           1       1.00      0.97      0.98       100
           2       0.96      0.99      0.98       100
           3       0.98      1.00      0.99       100
           4       0.93      0.98      0.96       100
           5       0.99      0.99      0.99       100
           6       0.94      0.96      0.95       100
           7       0.88      0.99      0.93       100
           8       0.99      0.82      0.90       100
           9       0.99      0.92      0.95       100

    accuracy                           0.96      1000
   macro avg       0.96      0.96      0.96      1000
weighted avg       0.96      0.96      0.96      1000

2023-12-25 23:35:58,750 [l2p_self_training.py] => pseudo labeling start
[7 9 2 ... 2 1 4]
2023-12-25 23:36:27,033 [l2p_self_training.py] => 3870 unlabeled samples will be pseudo labeled
2023-12-25 23:36:27,033 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9669250645994832
3992
Task 0, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.079, Train_accy 92.13, Test_accy 97.40: 100%|████████████| 5/5 [03:10<00:00, 38.00s/it]
2023-12-25 23:39:37,203 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.079, Train_accy 92.13, Test_accy 97.40
              precision    recall  f1-score   support

           0       0.96      0.99      0.98       100
           1       0.99      1.00      1.00       100
           2       0.94      1.00      0.97       100
           3       0.97      1.00      0.99       100
           4       0.97      0.96      0.96       100
           5       0.98      1.00      0.99       100
           6       1.00      0.93      0.96       100
           7       0.97      0.97      0.97       100
           8       1.00      0.90      0.95       100
           9       0.96      0.99      0.98       100

    accuracy                           0.97      1000
   macro avg       0.97      0.97      0.97      1000
weighted avg       0.97      0.97      0.97      1000

2023-12-25 23:39:43,500 [l2p_self_training.py] => pseudo labeling start
[6 5 4 ... 6 2 2]
2023-12-25 23:40:11,915 [l2p_self_training.py] => 3950 unlabeled samples will be pseudo labeled
2023-12-25 23:40:11,915 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9696202531645569
4080
Task 0, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.101, Train_accy 92.48, Test_accy 97.60: 100%|████████████| 5/5 [03:14<00:00, 38.94s/it]
2023-12-25 23:43:26,798 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.101, Train_accy 92.48, Test_accy 97.60
              precision    recall  f1-score   support

           0       1.00      0.97      0.98       100
           1       1.00      0.97      0.98       100
           2       0.96      1.00      0.98       100
           3       0.99      0.99      0.99       100
           4       0.98      0.98      0.98       100
           5       0.98      0.99      0.99       100
           6       0.99      0.97      0.98       100
           7       0.96      0.99      0.98       100
           8       0.99      0.92      0.95       100
           9       0.92      0.98      0.95       100

    accuracy                           0.98      1000
   macro avg       0.98      0.98      0.98      1000
weighted avg       0.98      0.98      0.98      1000

2023-12-25 23:43:33,204 [l2p_self_training.py] => pseudo labeling start
[9 3 0 ... 6 1 3]
2023-12-25 23:44:01,631 [l2p_self_training.py] => 3989 unlabeled samples will be pseudo labeled
2023-12-25 23:44:01,632 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9754324392078215
2023-12-25 23:44:08,084 [trainer.py] => No NME accuracy.
2023-12-25 23:44:08,084 [trainer.py] => CNN: {'total': 97.6, '00-09': 97.6, 'old': 0, 'new': 97.6}
2023-12-25 23:44:08,084 [trainer.py] => CNN top1 curve: [97.6]
2023-12-25 23:44:08,084 [trainer.py] => CNN top5 curve: [99.9]

Average Accuracy (CNN): 97.6
2023-12-25 23:44:08,084 [trainer.py] => Average Accuracy (CNN): 97.6 

2023-12-25 23:44:08,086 [trainer.py] => All params: 171816392
2023-12-25 23:44:08,088 [trainer.py] => Trainable params: 122980
2023-12-25 23:44:08,088 [l2p_self_training.py] => Learning on 10-20
250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.027, Train_accy 90.00, Test_accy 76.35: 100%|████████████| 5/5 [00:26<00:00,  5.21s/it]
2023-12-25 23:44:34,206 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.027, Train_accy 90.00, Test_accy 76.35
              precision    recall  f1-score   support

           0       0.99      0.97      0.98       100
           1       0.92      0.98      0.95       100
           2       0.75      1.00      0.85       100
           3       0.90      0.99      0.94       100
           4       0.88      0.98      0.93       100
           5       0.59      0.98      0.74       100
           6       0.75      0.97      0.84       100
           7       0.53      0.98      0.69       100
           8       0.33      0.95      0.49       100
           9       0.80      0.98      0.88       100
          10       0.98      0.40      0.57       100
          11       0.98      0.62      0.76       100
          12       0.98      0.59      0.74       100
          13       1.00      0.98      0.99       100
          14       1.00      0.23      0.37       100
          15       1.00      0.74      0.85       100
          16       1.00      0.59      0.74       100
          17       1.00      0.27      0.43       100
          18       1.00      0.83      0.91       100
          19       1.00      0.24      0.39       100

    accuracy                           0.76      2000
   macro avg       0.87      0.76      0.75      2000
weighted avg       0.87      0.76      0.75      2000

2023-12-25 23:44:46,558 [l2p_self_training.py] => pseudo labeling start
[10 13 12 ... 13 13 18]
2023-12-25 23:45:15,061 [l2p_self_training.py] => 2136 unlabeled samples will be pseudo labeled
2023-12-25 23:45:15,061 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.528558052434457
1379
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.135, Train_accy 94.42, Test_accy 85.50: 100%|████████████| 5/5 [01:17<00:00, 15.53s/it]
2023-12-25 23:46:33,219 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.135, Train_accy 94.42, Test_accy 85.50
              precision    recall  f1-score   support

           0       1.00      0.96      0.98       100
           1       0.96      0.98      0.97       100
           2       0.86      1.00      0.93       100
           3       0.93      1.00      0.97       100
           4       0.95      0.98      0.97       100
           5       0.74      0.96      0.84       100
           6       0.92      0.98      0.95       100
           7       0.65      0.98      0.78       100
           8       0.41      0.95      0.58       100
           9       0.89      0.97      0.93       100
          10       0.92      0.68      0.78       100
          11       0.99      0.93      0.96       100
          12       0.94      0.91      0.92       100
          13       1.00      0.98      0.99       100
          14       0.97      0.36      0.53       100
          15       1.00      0.92      0.96       100
          16       1.00      0.74      0.85       100
          17       0.98      0.49      0.65       100
          18       1.00      0.90      0.95       100
          19       0.98      0.43      0.60       100

    accuracy                           0.85      2000
   macro avg       0.90      0.86      0.85      2000
weighted avg       0.90      0.85      0.85      2000

2023-12-25 23:46:45,471 [l2p_self_training.py] => pseudo labeling start
[16 19 15 ... 12 11 13]
2023-12-25 23:47:13,995 [l2p_self_training.py] => 2802 unlabeled samples will be pseudo labeled
2023-12-25 23:47:13,995 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.7840827980014276
2447
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.152, Train_accy 94.73, Test_accy 90.80: 100%|████████████| 5/5 [02:06<00:00, 25.26s/it]
2023-12-25 23:49:20,634 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.152, Train_accy 94.73, Test_accy 90.80
              precision    recall  f1-score   support

           0       1.00      0.96      0.98       100
           1       0.98      0.98      0.98       100
           2       0.90      0.99      0.94       100
           3       0.95      1.00      0.98       100
           4       0.97      0.96      0.96       100
           5       0.86      0.95      0.90       100
           6       0.95      0.98      0.97       100
           7       0.74      0.98      0.84       100
           8       0.55      0.94      0.70       100
           9       0.90      0.97      0.93       100
          10       0.92      0.85      0.89       100
          11       0.99      0.98      0.98       100
          12       0.97      0.93      0.95       100
          13       1.00      0.98      0.99       100
          14       0.97      0.62      0.76       100
          15       0.93      0.95      0.94       100
          16       0.99      0.86      0.92       100
          17       0.98      0.64      0.78       100
          18       1.00      0.95      0.97       100
          19       1.00      0.69      0.82       100

    accuracy                           0.91      2000
   macro avg       0.93      0.91      0.91      2000
weighted avg       0.93      0.91      0.91      2000

2023-12-25 23:49:32,934 [l2p_self_training.py] => pseudo labeling start
[12 18 17 ... 10 18 12]
2023-12-25 23:50:01,398 [l2p_self_training.py] => 3180 unlabeled samples will be pseudo labeled
2023-12-25 23:50:01,398 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.8974842767295598
3104
Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.128, Train_accy 93.78, Test_accy 91.95: 100%|████████████| 5/5 [02:35<00:00, 31.19s/it]
2023-12-25 23:52:37,623 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.128, Train_accy 93.78, Test_accy 91.95
              precision    recall  f1-score   support

           0       1.00      0.95      0.97       100
           1       0.96      0.98      0.97       100
           2       0.94      0.99      0.97       100
           3       0.96      1.00      0.98       100
           4       0.98      0.94      0.96       100
           5       0.88      0.91      0.90       100
           6       0.97      0.98      0.98       100
           7       0.82      0.97      0.89       100
           8       0.64      0.93      0.76       100
           9       0.94      0.96      0.95       100
          10       0.93      0.76      0.84       100
          11       0.99      1.00      1.00       100
          12       1.00      0.95      0.97       100
          13       1.00      0.98      0.99       100
          14       0.97      0.66      0.79       100
          15       0.76      0.99      0.86       100
          16       0.96      0.96      0.96       100
          17       0.96      0.73      0.83       100
          18       1.00      0.95      0.97       100
          19       0.96      0.80      0.87       100

    accuracy                           0.92      2000
   macro avg       0.93      0.92      0.92      2000
weighted avg       0.93      0.92      0.92      2000

2023-12-25 23:52:49,824 [l2p_self_training.py] => pseudo labeling start
[19 16 15 ... 14 12 15]
2023-12-25 23:53:17,971 [l2p_self_training.py] => 3361 unlabeled samples will be pseudo labeled
2023-12-25 23:53:17,972 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9220470098185064
3349
Task 1, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.100, Train_accy 93.58, Test_accy 93.10: 100%|████████████| 5/5 [02:46<00:00, 33.25s/it]
2023-12-25 23:56:04,792 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.100, Train_accy 93.58, Test_accy 93.10
              precision    recall  f1-score   support

           0       1.00      0.94      0.97       100
           1       0.98      0.98      0.98       100
           2       0.96      0.99      0.98       100
           3       0.96      1.00      0.98       100
           4       0.99      0.94      0.96       100
           5       0.92      0.91      0.91       100
           6       0.97      0.97      0.97       100
           7       0.88      0.97      0.92       100
           8       0.64      0.94      0.76       100
           9       0.92      0.97      0.95       100
          10       0.85      0.94      0.90       100
          11       0.94      1.00      0.97       100
          12       0.96      0.95      0.95       100
          13       0.99      0.99      0.99       100
          14       1.00      0.59      0.74       100
          15       0.99      0.96      0.97       100
          16       0.94      0.94      0.94       100
          17       0.94      0.80      0.86       100
          18       0.98      0.98      0.98       100
          19       0.98      0.86      0.91       100

    accuracy                           0.93      2000
   macro avg       0.94      0.93      0.93      2000
weighted avg       0.94      0.93      0.93      2000

2023-12-25 23:56:17,119 [l2p_self_training.py] => pseudo labeling start
[17 18 11 ... 16 18 11]
2023-12-25 23:56:45,326 [l2p_self_training.py] => 3416 unlabeled samples will be pseudo labeled
2023-12-25 23:56:45,326 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9519906323185011
2023-12-25 23:56:57,724 [trainer.py] => No NME accuracy.
2023-12-25 23:56:57,724 [trainer.py] => CNN: {'total': 93.1, '00-09': 96.1, '10-19': 90.1, 'old': 96.1, 'new': 90.1}
2023-12-25 23:56:57,724 [trainer.py] => CNN top1 curve: [97.6, 93.1]
2023-12-25 23:56:57,724 [trainer.py] => CNN top5 curve: [99.9, 99.55]

Average Accuracy (CNN): 95.35
2023-12-25 23:56:57,725 [trainer.py] => Average Accuracy (CNN): 95.35 

2023-12-25 23:56:57,727 [trainer.py] => All params: 171816392
2023-12-25 23:56:57,728 [trainer.py] => Trainable params: 122980
2023-12-25 23:56:57,728 [l2p_self_training.py] => Learning on 20-30
250
Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.130, Train_accy 93.20, Test_accy 76.87: 100%|████████████| 5/5 [00:32<00:00,  6.46s/it]
2023-12-25 23:57:30,167 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.130, Train_accy 93.20, Test_accy 76.87
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.92      0.95      0.94       100
           1       0.97      0.99      0.98       100
           2       0.88      0.99      0.93       100
           3       0.95      1.00      0.98       100
           4       0.95      0.94      0.94       100
           5       0.85      0.92      0.88       100
           6       0.84      0.97      0.90       100
           7       0.76      0.97      0.85       100
           8       0.34      0.94      0.50       100
           9       0.76      0.97      0.85       100
          10       0.49      0.92      0.64       100
          11       0.48      1.00      0.65       100
          12       0.88      0.95      0.91       100
          13       0.48      0.99      0.64       100
          14       1.00      0.62      0.77       100
          15       0.91      0.96      0.94       100
          16       0.83      0.94      0.88       100
          17       0.88      0.80      0.84       100
          18       0.91      0.98      0.94       100
          19       0.89      0.85      0.87       100
          20       1.00      0.59      0.74       100
          21       0.00      0.00      0.00       100
          22       1.00      0.58      0.73       100
          23       1.00      0.75      0.86       100
          24       1.00      0.29      0.45       100
          25       1.00      0.65      0.79       100
          26       0.98      0.50      0.66       100
          27       0.00      0.00      0.00       100
          28       1.00      0.06      0.11       100
          29       1.00      0.99      0.99       100

    accuracy                           0.77      3000
   macro avg       0.80      0.77      0.74      3000
weighted avg       0.80      0.77      0.74      3000

2023-12-25 23:57:48,177 [l2p_self_training.py] => pseudo labeling start
[24 22 20 22 22 22 29 29 25 23 22 29 25 29 29 22 22 23 29 29 29 29 29 20
 26 22 29 29 26 22 23 29 25 29 23 29 20 29 22 22 29 23 23 20 29 25 29 25
 26 26 26 29 25 22 25 29 29 29 26 29 25 20 20 20 29 23 29 20 20 26 23 22
 25 23 25 25 29 23 20 23 25 22 23 29 22 29 29 29 25 23 22 25 20 29 23 22
 29 20 20 23 22 20 25 29 29 26 25 29 25 26 25 26 20 24 20 29 25 20 20 22
 20 26 26 25 29 29 20 29 29 29 25 29 20 29 26 25 22 26 22 20 29 29 23 23
 29 23 29 20 22 29 29 26 23 22 22 23 29 29 29 20 25 26 26 22 23 22 26 29
 25 25 23 26 20 22 26 29 22 22 25 25 20 29 25 25 29 29 25 22 20 25 26 24
 20 23 20 29 29 23 20 29 29 29 29 29 26 22 29 25 29 23 25 23 29 22 20 29
 23 22 29 29 29 29 29 29 24 22 29 22 29 29 29 29 22 26 29 26 29 22 20 29
 20 22 25 22 20 26 22 20 26 25 23 20 29 29 26 23 23 25 25 29 23 20 20 22
 20 26 29 29 23 20 29 23 29 20 20 23 22 20 29 23 20 29 29 29 29 29 29 23
 29 22 23 29 29 29 26 29 25 22 29 26 26 25 25 29 22 29 23 29 29 29 26 29
 20 29 23 29 23 20 23 29 29 22 25 29 29 20 26 29 29 29 23 20 29 20 20 29
 29 22 22 29 29 22 29 22 29 20 29 29 22 23 22 29 29 22 22 29 23 26 29 22
 23 23 29 29 29 26 22 23 23 29 25 20 20 26 26 22 29 29 22 29 29 29 23 22
 20 29 29 20 23 29 22 26 29 29 23 26 29 29 29 29 22 23 26 29 29 22 26 22
 25 26 26 25 22 25 29 29 26 26 29 20 29 29 29 29 22 29 29 25 29 23 29 29
 29 26 25 29 29 29 20 26 29 20 29 29 29 23 29 22 29 29 22 29 25 29 26 28
 25 20 29 23 23 29 24 23 22 29 20 22 29 29 23 26 23 22 29 29 20 29 24 29
 29 29 25 29 29 29 29 29 20 23 28 29 29 29 20 29 26 22 29 29 29 29 22 29
 29 29 26 29 20 20 29 23 22 20 20 29 26 23 23 23 29 29 23 29 22 29 20 22
 29 26 29 25 29 23 25 20 23 25 22 29 26 29 29 29 29 29 29 29 20 23 29 25
 29 29 29 29 29 22 20 29 26 25 20 29 22 22 20 29 29 29 29 22 23 23 29 29
 29 29 22 20 29 20 23 23 25 22 25 20 29 22 25 22 29 22 22 29 20 26 26 29
 26 29 23 29 22 25 23 29 29 29 22 22 22 29 29 29 29 29 23 22 20 22 29 23
 22 29 29 23 20 20 29 29 23 29 29 20 29 29 29 20 25 23 29 20 29 25 23 29
 23 26 22 29 29 20 20 20 29 29 29 22 25 22 25 29 20 25 22 29 23 25 20 23
 29 29 26 29 23 29 29 29 25 25 22 25 23 29 29 26 23 20 23 23 29 29 29 20
 25 29 29 25 29 23 29 29 25 29 20 20 22 26 23 26 29 25 25 20 23 29 20 29
 29 26 26 20 29 29 22 26 20 24 29 29 26 29 26 26 29 26 23 20 25 23 29 20
 20 25 22 22 29 20 23 25 22 29 29 20 26 20 29 29 29 20 29 29 23 20 29 24
 23 23 20 23 29 29 29 25 29 29 23 20 29 29 29 20 25 29 29 29 26 29 29 29
 29 26 29 29 29 23 26 22 29 24 26 29 22 29 23 29 25 26 29 26 23 29 29 26
 20 20 29 29 20 25 29 23 22 25 29 29 22 20 20 20 23 23 22 23 29 29 29 29
 20 29 22 29 25 29 20 26 29 29 29 29 26 23 29 29 25 29 29 29 29 25 25 29
 23 22 20 29 25 26 23 23 22 24 20 29 20 29 20 25 26 25 20 29 26 29 25 29
 22 29 29 23 29 25 29 20 20 26 25 29 25 25 22 26 29 29 20 25 23 23 20 25
 29 26 22 29 26 29 22 25 22 23 22 22 20 29 29 23 23 29 22 29 29 20 25 26
 20]
2023-12-25 23:58:16,373 [l2p_self_training.py] => 2501 unlabeled samples will be pseudo labeled
2023-12-25 23:58:16,374 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.3746501399440224
1187
Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.239, Train_accy 96.88, Test_accy 82.37: 100%|████████████| 5/5 [01:14<00:00, 14.91s/it]
2023-12-25 23:59:31,500 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.239, Train_accy 96.88, Test_accy 82.37
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.96      0.95      0.95       100
           1       0.97      0.99      0.98       100
           2       0.90      0.99      0.94       100
           3       0.94      1.00      0.97       100
           4       0.95      0.92      0.93       100
           5       0.85      0.92      0.88       100
           6       0.91      0.97      0.94       100
           7       0.86      0.95      0.90       100
           8       0.36      0.96      0.52       100
           9       0.88      0.95      0.91       100
          10       0.56      0.91      0.69       100
          11       0.67      1.00      0.80       100
          12       0.92      0.92      0.92       100
          13       0.49      0.99      0.65       100
          14       1.00      0.63      0.77       100
          15       0.96      0.96      0.96       100
          16       0.89      0.93      0.91       100
          17       0.92      0.80      0.86       100
          18       0.95      0.97      0.96       100
          19       0.92      0.85      0.89       100
          20       0.99      0.87      0.93       100
          21       0.00      0.00      0.00       100
          22       0.99      0.84      0.91       100
          23       1.00      0.83      0.91       100
          24       0.99      0.66      0.79       100
          25       1.00      0.80      0.89       100
          26       1.00      0.77      0.87       100
          27       0.00      0.00      0.00       100
          28       0.97      0.39      0.56       100
          29       1.00      0.99      0.99       100

    accuracy                           0.82      3000
   macro avg       0.83      0.82      0.81      3000
weighted avg       0.83      0.82      0.81      3000

2023-12-25 23:59:49,547 [l2p_self_training.py] => pseudo labeling start
[20 29 22 ... 22 20 29]
2023-12-26 00:00:17,792 [l2p_self_training.py] => 3185 unlabeled samples will be pseudo labeled
2023-12-26 00:00:17,793 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.6153846153846154
2210
Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.207, Train_accy 96.20, Test_accy 85.03: 100%|████████████| 5/5 [02:01<00:00, 24.29s/it]
2023-12-26 00:02:19,844 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.207, Train_accy 96.20, Test_accy 85.03
              precision    recall  f1-score   support

           0       0.98      0.91      0.94       100
           1       0.98      0.99      0.99       100
           2       0.91      0.99      0.95       100
           3       0.95      1.00      0.98       100
           4       0.99      0.92      0.95       100
           5       0.86      0.92      0.89       100
           6       0.96      0.96      0.96       100
           7       0.87      0.94      0.90       100
           8       0.36      0.95      0.52       100
           9       0.86      0.94      0.90       100
          10       0.71      0.91      0.80       100
          11       0.77      1.00      0.87       100
          12       0.94      0.92      0.93       100
          13       0.49      0.99      0.66       100
          14       1.00      0.61      0.76       100
          15       0.96      0.94      0.95       100
          16       0.92      0.92      0.92       100
          17       0.94      0.80      0.86       100
          18       0.97      0.96      0.96       100
          19       0.95      0.80      0.87       100
          20       0.99      0.92      0.95       100
          21       1.00      0.01      0.02       100
          22       0.95      0.95      0.95       100
          23       1.00      0.92      0.96       100
          24       0.98      0.82      0.89       100
          25       1.00      0.89      0.94       100
          26       0.95      0.90      0.92       100
          27       1.00      0.01      0.02       100
          28       1.00      0.73      0.84       100
          29       0.99      0.99      0.99       100

    accuracy                           0.85      3000
   macro avg       0.91      0.85      0.84      3000
weighted avg       0.91      0.85      0.84      3000

2023-12-26 00:02:38,000 [l2p_self_training.py] => pseudo labeling start
[26 20 20 ... 20 29 25]
2023-12-26 00:03:06,675 [l2p_self_training.py] => 3610 unlabeled samples will be pseudo labeled
2023-12-26 00:03:06,675 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.7362880886426593
2908
Task 2, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.194, Train_accy 95.98, Test_accy 85.40: 100%|████████████| 5/5 [02:33<00:00, 30.65s/it]
2023-12-26 00:05:40,605 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.194, Train_accy 95.98, Test_accy 85.40
              precision    recall  f1-score   support

           0       0.99      0.93      0.96       100
           1       1.00      0.98      0.99       100
           2       0.90      0.99      0.94       100
           3       0.95      1.00      0.98       100
           4       0.99      0.92      0.95       100
           5       0.88      0.91      0.89       100
           6       0.92      0.96      0.94       100
           7       0.86      0.94      0.90       100
           8       0.37      0.96      0.54       100
           9       0.89      0.96      0.92       100
          10       0.61      0.89      0.73       100
          11       0.87      0.99      0.93       100
          12       0.96      0.91      0.93       100
          13       0.53      0.99      0.69       100
          14       1.00      0.66      0.80       100
          15       0.94      0.96      0.95       100
          16       0.93      0.92      0.92       100
          17       0.97      0.78      0.87       100
          18       1.00      0.95      0.97       100
          19       0.98      0.82      0.89       100
          20       0.94      0.96      0.95       100
          21       1.00      0.17      0.29       100
          22       0.96      0.95      0.95       100
          23       1.00      0.91      0.95       100
          24       0.92      0.89      0.90       100
          25       0.89      0.96      0.92       100
          26       0.99      0.86      0.92       100
          27       1.00      0.01      0.02       100
          28       1.00      0.50      0.67       100
          29       1.00      0.99      0.99       100

    accuracy                           0.85      3000
   macro avg       0.91      0.85      0.84      3000
weighted avg       0.91      0.85      0.84      3000

2023-12-26 00:05:58,836 [l2p_self_training.py] => pseudo labeling start
[24 20 25 ... 29 20 25]
2023-12-26 00:06:27,575 [l2p_self_training.py] => 3450 unlabeled samples will be pseudo labeled
2023-12-26 00:06:27,576 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.7643478260869565
2887
Task 2, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.198, Train_accy 95.84, Test_accy 86.77: 100%|████████████| 5/5 [02:32<00:00, 30.59s/it]
2023-12-26 00:09:00,949 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 4 => Loss -0.198, Train_accy 95.84, Test_accy 86.77
              precision    recall  f1-score   support

           0       0.99      0.91      0.95       100
           1       1.00      0.95      0.97       100
           2       0.92      0.99      0.95       100
           3       0.95      1.00      0.98       100
           4       0.99      0.94      0.96       100
           5       0.89      0.90      0.90       100
           6       0.92      0.97      0.95       100
           7       0.85      0.93      0.89       100
           8       0.40      0.96      0.56       100
           9       0.88      0.96      0.92       100
          10       0.67      0.90      0.77       100
          11       0.85      0.99      0.92       100
          12       0.97      0.94      0.95       100
          13       0.57      0.99      0.73       100
          14       1.00      0.62      0.77       100
          15       0.96      0.95      0.95       100
          16       0.93      0.91      0.92       100
          17       0.97      0.77      0.86       100
          18       1.00      0.96      0.98       100
          19       0.98      0.81      0.89       100
          20       0.92      0.97      0.95       100
          21       1.00      0.28      0.44       100
          22       0.90      0.97      0.93       100
          23       1.00      0.94      0.97       100
          24       0.96      0.85      0.90       100
          25       0.96      0.98      0.97       100
          26       0.95      0.90      0.92       100
          27       0.89      0.16      0.27       100
          28       1.00      0.64      0.78       100
          29       1.00      0.99      0.99       100

    accuracy                           0.87      3000
   macro avg       0.91      0.87      0.86      3000
weighted avg       0.91      0.87      0.86      3000

2023-12-26 00:09:19,115 [l2p_self_training.py] => pseudo labeling start
[25 24 20 ... 29 25 25]
2023-12-26 00:09:47,547 [l2p_self_training.py] => 3404 unlabeled samples will be pseudo labeled
2023-12-26 00:09:47,547 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.8293184488836662
2023-12-26 00:10:06,156 [trainer.py] => No NME accuracy.
2023-12-26 00:10:06,156 [trainer.py] => CNN: {'total': 86.77, '00-09': 95.1, '10-19': 88.4, '20-29': 76.8, 'old': 91.75, 'new': 76.8}
2023-12-26 00:10:06,156 [trainer.py] => CNN top1 curve: [97.6, 93.1, 86.77]
2023-12-26 00:10:06,156 [trainer.py] => CNN top5 curve: [99.9, 99.55, 98.93]

Average Accuracy (CNN): 92.49
2023-12-26 00:10:06,156 [trainer.py] => Average Accuracy (CNN): 92.49 

======================================================================================
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
======================================================================================

第一个任务不打伪标签，只拿有标签数据训练模型，第二个任务开始打伪标签
(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2023-12-25 10:56:05,455 [trainer.py] => config: ./exps/l2p_self_training.json
2023-12-25 10:56:05,455 [trainer.py] => prefix:  
2023-12-25 10:56:05,455 [trainer.py] => dataset: cifar224
2023-12-25 10:56:05,456 [trainer.py] => memory_size: 0
2023-12-25 10:56:05,456 [trainer.py] => memory_per_class: 0
2023-12-25 10:56:05,456 [trainer.py] => fixed_memory: False
2023-12-25 10:56:05,456 [trainer.py] => shuffle: True
2023-12-25 10:56:05,456 [trainer.py] => init_cls: 10
2023-12-25 10:56:05,456 [trainer.py] => increment: 10
2023-12-25 10:56:05,456 [trainer.py] => model_name: l2p_self_training
2023-12-25 10:56:05,456 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2023-12-25 10:56:05,456 [trainer.py] => get_original_backbone: True
2023-12-25 10:56:05,456 [trainer.py] => device: [device(type='cuda', index=5)]
2023-12-25 10:56:05,456 [trainer.py] => seed: 1993
2023-12-25 10:56:05,456 [trainer.py] => tuned_epoch: 5
2023-12-25 10:56:05,456 [trainer.py] => init_lr: 0.001875
2023-12-25 10:56:05,456 [trainer.py] => batch_size: 16
2023-12-25 10:56:05,456 [trainer.py] => weight_decay: 0
2023-12-25 10:56:05,456 [trainer.py] => min_lr: 1e-05
2023-12-25 10:56:05,456 [trainer.py] => optimizer: adam
2023-12-25 10:56:05,456 [trainer.py] => scheduler: constant
2023-12-25 10:56:05,456 [trainer.py] => reinit_optimizer: True
2023-12-25 10:56:05,456 [trainer.py] => global_pool: token
2023-12-25 10:56:05,456 [trainer.py] => head_type: prompt
2023-12-25 10:56:05,456 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2023-12-25 10:56:05,456 [trainer.py] => pretrained: True
2023-12-25 10:56:05,456 [trainer.py] => drop: 0.0
2023-12-25 10:56:05,456 [trainer.py] => drop_path: 0.0
2023-12-25 10:56:05,456 [trainer.py] => prompt_pool: True
2023-12-25 10:56:05,456 [trainer.py] => pool_size: 10
2023-12-25 10:56:05,456 [trainer.py] => length: 5
2023-12-25 10:56:05,456 [trainer.py] => top_k: 5
2023-12-25 10:56:05,457 [trainer.py] => initializer: uniform
2023-12-25 10:56:05,457 [trainer.py] => prompt_key: True
2023-12-25 10:56:05,457 [trainer.py] => prompt_key_init: uniform
2023-12-25 10:56:05,457 [trainer.py] => use_prompt_mask: False
2023-12-25 10:56:05,457 [trainer.py] => shared_prompt_pool: False
2023-12-25 10:56:05,457 [trainer.py] => shared_prompt_key: False
2023-12-25 10:56:05,457 [trainer.py] => batchwise_prompt: True
2023-12-25 10:56:05,457 [trainer.py] => embedding_key: cls
2023-12-25 10:56:05,457 [trainer.py] => predefined_key: 
2023-12-25 10:56:05,457 [trainer.py] => pull_constraint: True
2023-12-25 10:56:05,457 [trainer.py] => pull_constraint_coeff: 0.1
2023-12-25 10:56:05,457 [trainer.py] => semi_supervised_mode: True
2023-12-25 10:56:05,457 [trainer.py] => labeled_ratio: 0.05
2023-12-25 10:56:05,457 [trainer.py] => unlabeled_data_distribution_mode: no_ood
2023-12-25 10:56:05,457 [trainer.py] => confidence_threshold: 0.9
2023-12-25 10:56:05,457 [trainer.py] => max_self_training_iteration: 5
Files already downloaded and verified
Files already downloaded and verified
2023-12-25 10:56:07,412 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-25 10:56:09,113 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2023-12-25 10:56:09,114 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2023-12-25 10:56:11,561 [l2p_self_training.py] => 85,940,836 model total parameters.
2023-12-25 10:56:11,562 [l2p_self_training.py] => 122,980 model training parameters.
2023-12-25 10:56:11,562 [l2p_self_training.py] => prompt.prompt: 38400
2023-12-25 10:56:11,562 [l2p_self_training.py] => prompt.prompt_key: 7680
2023-12-25 10:56:11,562 [l2p_self_training.py] => head.weight: 76800
2023-12-25 10:56:11,562 [l2p_self_training.py] => head.bias: 100
2023-12-25 10:56:11,564 [trainer.py] => All params: 171816392
2023-12-25 10:56:11,565 [trainer.py] => Trainable params: 122980
2023-12-25 10:56:11,565 [l2p_self_training.py] => Learning on 0-10
250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.02s/it]
2023-12-25 10:56:32,484 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40
              precision    recall  f1-score   support

           0       0.78      0.99      0.87       100
           1       0.84      1.00      0.91       100
           2       1.00      0.87      0.93       100
           3       0.85      1.00      0.92       100
           4       1.00      0.87      0.93       100
           5       1.00      0.83      0.91       100
           6       0.85      0.88      0.87       100
           7       0.79      0.97      0.87       100
           8       0.94      0.68      0.79       100
           9       1.00      0.81      0.90       100

    accuracy                           0.89      1000
   macro avg       0.91      0.89      0.89      1000
weighted avg       0.91      0.89      0.89      1000

2023-12-25 10:56:44,852 [trainer.py] => No NME accuracy.
2023-12-25 10:56:44,852 [trainer.py] => CNN: {'total': 89.3, '00-09': 89.3, 'old': 0, 'new': 89.3}
2023-12-25 10:56:44,852 [trainer.py] => CNN top1 curve: [89.3]
2023-12-25 10:56:44,853 [trainer.py] => CNN top5 curve: [98.5]

Average Accuracy (CNN): 89.3
2023-12-25 10:56:44,853 [trainer.py] => Average Accuracy (CNN): 89.3 

2023-12-25 10:56:44,855 [trainer.py] => All params: 171816392
2023-12-25 10:56:44,856 [trainer.py] => Trainable params: 122980
2023-12-25 10:56:44,857 [l2p_self_training.py] => Learning on 10-20
250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.393, Train_accy 84.80, Test_accy 84.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.10s/it]
2023-12-25 10:57:10,414 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.393, Train_accy 84.80, Test_accy 84.40
              precision    recall  f1-score   support

           0       0.82      0.98      0.89       100
           1       0.94      0.96      0.95       100
           2       1.00      0.82      0.90       100
           3       0.83      0.99      0.90       100
           4       1.00      0.68      0.81       100
           5       1.00      0.58      0.73       100
           6       0.89      0.85      0.87       100
           7       0.87      0.80      0.83       100
           8       0.90      0.52      0.66       100
           9       0.99      0.70      0.82       100
          10       0.57      0.96      0.71       100
          11       0.99      0.92      0.95       100
          12       0.69      0.92      0.79       100
          13       0.99      0.96      0.97       100
          14       0.74      0.75      0.74       100
          15       1.00      0.95      0.97       100
          16       0.91      0.92      0.92       100
          17       0.57      0.88      0.69       100
          18       0.95      0.91      0.93       100
          19       0.87      0.83      0.85       100

    accuracy                           0.84      2000
   macro avg       0.88      0.84      0.85      2000
weighted avg       0.88      0.84      0.85      2000

2023-12-25 10:57:22,574 [l2p_self_training.py] => pseudo labeling start
2023-12-25 10:57:50,966 [l2p_self_training.py] => 1934 unlabeled samples will be pseudo labeled
2023-12-25 10:57:50,966 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9865563598759048
2184
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 90.89, Test_accy 83.20: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [01:54<00:00, 22.87s/it]
2023-12-25 10:59:45,540 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 90.89, Test_accy 83.20
              precision    recall  f1-score   support

           0       0.65      1.00      0.79       100
           1       0.82      0.95      0.88       100
           2       0.99      0.92      0.95       100
           3       0.65      0.99      0.79       100
           4       1.00      0.53      0.69       100
           5       0.95      0.69      0.80       100
           6       0.69      0.82      0.75       100
           7       0.63      0.90      0.74       100
           8       0.59      0.80      0.68       100
           9       0.99      0.66      0.79       100
          10       0.71      0.89      0.79       100
          11       0.93      0.97      0.95       100
          12       0.94      0.83      0.88       100
          13       1.00      0.96      0.98       100
          14       0.97      0.59      0.73       100
          15       1.00      0.93      0.96       100
          16       0.96      0.91      0.93       100
          17       0.98      0.57      0.72       100
          18       0.99      0.93      0.96       100
          19       0.96      0.80      0.87       100

    accuracy                           0.83      2000
   macro avg       0.87      0.83      0.83      2000
weighted avg       0.87      0.83      0.83      2000

2023-12-25 10:59:57,742 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:00:26,126 [l2p_self_training.py] => 2288 unlabeled samples will be pseudo labeled
2023-12-25 11:00:26,126 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9768356643356644
2538
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss inf, Train_accy 89.91, Test_accy 39.15: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [02:09<00:00, 25.98s/it]
2023-12-25 11:02:36,329 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss inf, Train_accy 89.91, Test_accy 39.15
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.86      0.82      0.84       100
           1       0.48      0.97      0.64       100
           2       0.67      0.95      0.79       100
           3       0.19      1.00      0.32       100
           4       0.96      0.25      0.40       100
           5       0.68      0.50      0.57       100
           6       0.36      0.93      0.52       100
           7       0.31      0.87      0.45       100
           8       0.28      0.84      0.41       100
           9       0.81      0.70      0.75       100
          10       0.00      0.00      0.00       100
          11       0.00      0.00      0.00       100
          12       0.00      0.00      0.00       100
          13       0.00      0.00      0.00       100
          14       0.00      0.00      0.00       100
          15       0.00      0.00      0.00       100
          16       0.00      0.00      0.00       100
          17       0.00      0.00      0.00       100
          18       0.00      0.00      0.00       100
          19       0.00      0.00      0.00       100

    accuracy                           0.39      2000
   macro avg       0.28      0.39      0.29      2000
weighted avg       0.28      0.39      0.29      2000

2023-12-25 11:02:48,588 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:03:17,003 [l2p_self_training.py] => 327 unlabeled samples will be pseudo labeled
2023-12-25 11:03:17,003 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.0
577
Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss inf, Train_accy 24.09, Test_accy 14.55: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:40<00:00,  8.19s/it]
2023-12-25 11:03:58,190 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss inf, Train_accy 24.09, Test_accy 14.55
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           1       0.09      0.99      0.16       100
           2       1.00      0.01      0.02       100
           3       0.14      1.00      0.25       100
           4       0.00      0.00      0.00       100
           5       0.00      0.00      0.00       100
           6       0.12      0.04      0.06       100
           7       0.92      0.44      0.59       100
           8       0.41      0.43      0.42       100
           9       0.00      0.00      0.00       100
          10       0.00      0.00      0.00       100
          11       0.00      0.00      0.00       100
          12       0.00      0.00      0.00       100
          13       0.00      0.00      0.00       100
          14       0.00      0.00      0.00       100
          15       0.00      0.00      0.00       100
          16       0.00      0.00      0.00       100
          17       0.00      0.00      0.00       100
          18       0.00      0.00      0.00       100
          19       0.00      0.00      0.00       100

    accuracy                           0.15      2000
   macro avg       0.13      0.15      0.08      2000
weighted avg       0.13      0.15      0.08      2000

2023-12-25 11:04:10,525 [l2p_self_training.py] => pseudo labeling start
2023-12-25 11:04:38,903 [l2p_self_training.py] => 0 unlabeled samples will be pseudo labeled
2023-12-25 11:04:38,903 [l2p_self_training.py] => pseudo labeling finish
/home/lhz/code/LAMDA-PILOT/utils/toolkit.py:82: RuntimeWarning: invalid value encountered in scalar divide
  pseudo_accuracy = np.sum(pseudo_labels == ground_truth) / len(ground_truth)
Pseudo Accuracy:  nan
Traceback (most recent call last):
  File "/home/lhz/code/LAMDA-PILOT/main.py", line 25, in <module>
    main()
  File "/home/lhz/code/LAMDA-PILOT/main.py", line 11, in main
    train(args)
  File "/home/lhz/code/LAMDA-PILOT/trainer.py", line 18, in train
    _train(args)
  File "/home/lhz/code/LAMDA-PILOT/trainer.py", line 74, in _train
    model.incremental_train(data_manager)
  File "/home/lhz/code/LAMDA-PILOT/models/l2p_self_training.py", line 76, in incremental_train
    self._train(self.train_labeled_dataset, self.train_unlabeled_dataset, self.test_loader)
  File "/home/lhz/code/LAMDA-PILOT/models/l2p_self_training.py", line 92, in _train
    self._init_train(train_labeled_dataset, train_unlabeled_dataset, test_loader, optimizer, scheduler)
  File "/home/lhz/code/LAMDA-PILOT/models/l2p_self_training.py", line 252, in _init_train
    train_unlabeled_dataset = self._pseudo_label(unlabeled_dataset)
  File "/home/lhz/code/LAMDA-PILOT/models/l2p_self_training.py", line 335, in _pseudo_label
    confusion_matrix_on_pseudo_labels(pseudo_label_dataset.labels[selected_unlabeled_data_idx], pseudo_label)
  File "/home/lhz/code/LAMDA-PILOT/utils/toolkit.py", line 86, in confusion_matrix_on_pseudo_labels
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
  File "/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/seaborn/matrix.py", line 446, in heatmap
    plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,
  File "/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/seaborn/matrix.py", line 163, in __init__
    self._determine_cmap_params(plot_data, vmin, vmax,
  File "/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/seaborn/matrix.py", line 202, in _determine_cmap_params
    vmin = np.nanmin(calc_data)
  File "<__array_function__ internals>", line 200, in nanmin
  File "/home/lhz/.conda/envs/lhz-torch-2.0/lib/python3.9/site-packages/numpy/lib/nanfunctions.py", line 343, in nanmin
    res = np.fmin.reduce(a, axis=axis, out=out, **kwargs)
ValueError: zero-size array to reduction operation fmin which has no identity

======================================================================================
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
======================================================================================

将方法2-1和方法2-2结合，即task 1不做self-training，task 2开始做Self-Training并剔除标签打错的数据，只把打对伪标签的数据进行self-training
(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2023-12-25 22:31:22,507 [trainer.py] => config: ./exps/l2p_self_training.json
2023-12-25 22:31:22,507 [trainer.py] => prefix:  
2023-12-25 22:31:22,507 [trainer.py] => dataset: cifar224
2023-12-25 22:31:22,507 [trainer.py] => memory_size: 0
2023-12-25 22:31:22,507 [trainer.py] => memory_per_class: 0
2023-12-25 22:31:22,507 [trainer.py] => fixed_memory: False
2023-12-25 22:31:22,507 [trainer.py] => shuffle: True
2023-12-25 22:31:22,507 [trainer.py] => init_cls: 10
2023-12-25 22:31:22,507 [trainer.py] => increment: 10
2023-12-25 22:31:22,507 [trainer.py] => model_name: l2p_self_training
2023-12-25 22:31:22,507 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2023-12-25 22:31:22,507 [trainer.py] => get_original_backbone: True
2023-12-25 22:31:22,507 [trainer.py] => device: [device(type='cuda', index=5)]
2023-12-25 22:31:22,507 [trainer.py] => seed: 1993
2023-12-25 22:31:22,507 [trainer.py] => tuned_epoch: 5
2023-12-25 22:31:22,507 [trainer.py] => init_lr: 0.001875
2023-12-25 22:31:22,507 [trainer.py] => batch_size: 16
2023-12-25 22:31:22,507 [trainer.py] => weight_decay: 0
2023-12-25 22:31:22,508 [trainer.py] => min_lr: 1e-05
2023-12-25 22:31:22,508 [trainer.py] => optimizer: adam
2023-12-25 22:31:22,508 [trainer.py] => scheduler: constant
2023-12-25 22:31:22,508 [trainer.py] => reinit_optimizer: True
2023-12-25 22:31:22,508 [trainer.py] => global_pool: token
2023-12-25 22:31:22,508 [trainer.py] => head_type: prompt
2023-12-25 22:31:22,508 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2023-12-25 22:31:22,508 [trainer.py] => pretrained: True
2023-12-25 22:31:22,508 [trainer.py] => drop: 0.0
2023-12-25 22:31:22,508 [trainer.py] => drop_path: 0.0
2023-12-25 22:31:22,508 [trainer.py] => prompt_pool: True
2023-12-25 22:31:22,508 [trainer.py] => pool_size: 10
2023-12-25 22:31:22,508 [trainer.py] => length: 5
2023-12-25 22:31:22,508 [trainer.py] => top_k: 5
2023-12-25 22:31:22,508 [trainer.py] => initializer: uniform
2023-12-25 22:31:22,508 [trainer.py] => prompt_key: True
2023-12-25 22:31:22,508 [trainer.py] => prompt_key_init: uniform
2023-12-25 22:31:22,508 [trainer.py] => use_prompt_mask: False
2023-12-25 22:31:22,508 [trainer.py] => shared_prompt_pool: False
2023-12-25 22:31:22,508 [trainer.py] => shared_prompt_key: False
2023-12-25 22:31:22,508 [trainer.py] => batchwise_prompt: True
2023-12-25 22:31:22,508 [trainer.py] => embedding_key: cls
2023-12-25 22:31:22,508 [trainer.py] => predefined_key: 
2023-12-25 22:31:22,508 [trainer.py] => pull_constraint: True
2023-12-25 22:31:22,508 [trainer.py] => pull_constraint_coeff: 0.1
2023-12-25 22:31:22,508 [trainer.py] => semi_supervised_mode: True
2023-12-25 22:31:22,508 [trainer.py] => labeled_ratio: 0.05
2023-12-25 22:31:22,508 [trainer.py] => unlabeled_data_distribution_mode: no_ood
2023-12-25 22:31:22,508 [trainer.py] => confidence_threshold: 0.9
2023-12-25 22:31:22,508 [trainer.py] => max_self_training_iteration: 5
Files already downloaded and verified
Files already downloaded and verified
2023-12-25 22:31:24,553 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-25 22:31:26,889 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2023-12-25 22:31:26,890 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2023-12-25 22:31:30,094 [l2p_self_training.py] => 85,940,836 model total parameters.
2023-12-25 22:31:30,094 [l2p_self_training.py] => 122,980 model training parameters.
2023-12-25 22:31:30,094 [l2p_self_training.py] => prompt.prompt: 38400
2023-12-25 22:31:30,094 [l2p_self_training.py] => prompt.prompt_key: 7680
2023-12-25 22:31:30,095 [l2p_self_training.py] => head.weight: 76800
2023-12-25 22:31:30,095 [l2p_self_training.py] => head.bias: 100
2023-12-25 22:31:30,096 [trainer.py] => All params: 171816392
2023-12-25 22:31:30,097 [trainer.py] => Trainable params: 122980
2023-12-25 22:31:30,097 [l2p_self_training.py] => Learning on 0-10
250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40: 100%|█████████████| 5/5 [00:20<00:00,  4.03s/it]
2023-12-25 22:31:51,062 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40
              precision    recall  f1-score   support

           0       0.78      0.99      0.87       100
           1       0.84      1.00      0.91       100
           2       1.00      0.87      0.93       100
           3       0.85      1.00      0.92       100
           4       1.00      0.87      0.93       100
           5       1.00      0.83      0.91       100
           6       0.85      0.88      0.87       100
           7       0.79      0.97      0.87       100
           8       0.94      0.68      0.79       100
           9       1.00      0.81      0.90       100

    accuracy                           0.89      1000
   macro avg       0.91      0.89      0.89      1000
weighted avg       0.91      0.89      0.89      1000

2023-12-25 22:32:03,550 [trainer.py] => No NME accuracy.
2023-12-25 22:32:03,550 [trainer.py] => CNN: {'total': 89.3, '00-09': 89.3, 'old': 0, 'new': 89.3}
2023-12-25 22:32:03,550 [trainer.py] => CNN top1 curve: [89.3]
2023-12-25 22:32:03,550 [trainer.py] => CNN top5 curve: [98.5]

Average Accuracy (CNN): 89.3
2023-12-25 22:32:03,551 [trainer.py] => Average Accuracy (CNN): 89.3 

2023-12-25 22:32:03,552 [trainer.py] => All params: 171816392
2023-12-25 22:32:03,553 [trainer.py] => Trainable params: 122980
2023-12-25 22:32:03,553 [l2p_self_training.py] => Learning on 10-20
250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.393, Train_accy 84.80, Test_accy 84.40: 100%|█████████████| 5/5 [00:25<00:00,  5.12s/it]
2023-12-25 22:32:29,197 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.393, Train_accy 84.80, Test_accy 84.40
              precision    recall  f1-score   support

           0       0.82      0.98      0.89       100
           1       0.94      0.96      0.95       100
           2       1.00      0.82      0.90       100
           3       0.83      0.99      0.90       100
           4       1.00      0.68      0.81       100
           5       1.00      0.58      0.73       100
           6       0.89      0.85      0.87       100
           7       0.87      0.80      0.83       100
           8       0.90      0.52      0.66       100
           9       0.99      0.70      0.82       100
          10       0.57      0.96      0.71       100
          11       0.99      0.92      0.95       100
          12       0.69      0.92      0.79       100
          13       0.99      0.96      0.97       100
          14       0.74      0.75      0.74       100
          15       1.00      0.95      0.97       100
          16       0.91      0.92      0.92       100
          17       0.57      0.88      0.69       100
          18       0.95      0.91      0.93       100
          19       0.87      0.83      0.85       100

    accuracy                           0.84      2000
   macro avg       0.88      0.84      0.85      2000
weighted avg       0.88      0.84      0.85      2000

2023-12-25 22:32:41,334 [l2p_self_training.py] => pseudo labeling start
0.9865563598759048
[15 14 13 ... 13 19 17]
2023-12-25 22:33:09,543 [l2p_self_training.py] => 1934 unlabeled samples will be pseudo labeled
2023-12-25 22:33:09,543 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9865563598759048
2158
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.007, Train_accy 91.52, Test_accy 79.25: 100%|████████████| 5/5 [01:52<00:00, 22.57s/it]
2023-12-25 22:35:02,632 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.007, Train_accy 91.52, Test_accy 79.25
              precision    recall  f1-score   support

           0       0.91      0.83      0.87       100
           1       0.98      0.92      0.95       100
           2       1.00      0.82      0.90       100
           3       0.91      0.98      0.94       100
           4       1.00      0.28      0.44       100
           5       1.00      0.49      0.66       100
           6       0.90      0.79      0.84       100
           7       0.98      0.61      0.75       100
           8       1.00      0.34      0.51       100
           9       1.00      0.45      0.62       100
          10       0.57      0.93      0.71       100
          11       0.95      0.97      0.96       100
          12       0.64      0.91      0.75       100
          13       0.88      0.99      0.93       100
          14       0.40      0.90      0.55       100
          15       0.87      0.97      0.92       100
          16       0.95      0.92      0.93       100
          17       0.67      0.86      0.75       100
          18       0.97      0.95      0.96       100
          19       0.67      0.94      0.78       100

    accuracy                           0.79      2000
   macro avg       0.86      0.79      0.79      2000
weighted avg       0.86      0.79      0.79      2000

2023-12-25 22:35:14,789 [l2p_self_training.py] => pseudo labeling start
0.9670122525918945
[19 13 15 ... 19 13 13]
2023-12-25 22:35:42,953 [l2p_self_training.py] => 3183 unlabeled samples will be pseudo labeled
2023-12-25 22:35:42,953 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9670122525918945
3328
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.082, Train_accy 92.01, Test_accy 73.50: 100%|████████████| 5/5 [02:45<00:00, 33.10s/it]
2023-12-25 22:38:28,832 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.082, Train_accy 92.01, Test_accy 73.50
              precision    recall  f1-score   support

           0       0.99      0.84      0.91       100
           1       0.98      0.85      0.91       100
           2       1.00      0.76      0.86       100
           3       0.97      0.93      0.95       100
           4       1.00      0.11      0.20       100
           5       1.00      0.22      0.36       100
           6       0.93      0.41      0.57       100
           7       0.95      0.52      0.67       100
           8       1.00      0.22      0.36       100
           9       1.00      0.35      0.52       100
          10       0.49      0.94      0.64       100
          11       0.57      0.99      0.72       100
          12       0.58      0.96      0.72       100
          13       0.83      0.99      0.90       100
          14       0.67      0.91      0.77       100
          15       0.63      0.99      0.77       100
          16       0.78      0.98      0.87       100
          17       0.82      0.80      0.81       100
          18       1.00      0.95      0.97       100
          19       0.48      0.98      0.64       100

    accuracy                           0.73      2000
   macro avg       0.83      0.74      0.71      2000
weighted avg       0.83      0.73      0.71      2000

2023-12-25 22:38:41,197 [l2p_self_training.py] => pseudo labeling start
0.9661590524534687
[19 11 11 ... 17 17 13]
2023-12-25 22:39:09,637 [l2p_self_training.py] => 3546 unlabeled samples will be pseudo labeled
2023-12-25 22:39:09,638 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9661590524534687
3676
Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.054, Train_accy 91.92, Test_accy 70.65: 100%|████████████| 5/5 [03:01<00:00, 36.30s/it]
2023-12-25 22:42:11,283 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.054, Train_accy 91.92, Test_accy 70.65
              precision    recall  f1-score   support

           0       0.92      0.81      0.86       100
           1       0.99      0.86      0.92       100
           2       1.00      0.63      0.77       100
           3       0.99      0.90      0.94       100
           4       1.00      0.19      0.32       100
           5       1.00      0.12      0.21       100
           6       0.90      0.18      0.30       100
           7       0.98      0.45      0.62       100
           8       1.00      0.16      0.28       100
           9       1.00      0.26      0.41       100
          10       0.38      0.98      0.54       100
          11       0.44      1.00      0.62       100
          12       0.60      0.99      0.75       100
          13       0.90      0.99      0.94       100
          14       0.62      0.93      0.75       100
          15       0.77      0.97      0.86       100
          16       0.67      0.98      0.79       100
          17       0.77      0.84      0.80       100
          18       1.00      0.94      0.97       100
          19       0.65      0.95      0.77       100

    accuracy                           0.71      2000
   macro avg       0.83      0.71      0.67      2000
weighted avg       0.83      0.71      0.67      2000

2023-12-25 22:42:23,524 [l2p_self_training.py] => pseudo labeling start
0.9698123470220288
[16 13 12 ... 17 15 19]
2023-12-25 22:42:52,029 [l2p_self_training.py] => 3677 unlabeled samples will be pseudo labeled
2023-12-25 22:42:52,030 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9698123470220288
3816

======================================================================================
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
======================================================================================

在上述方法的基础上，故意在task 2的self-training时加入几十个个噪声标签，标签为新task的标签
(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2023-12-26 10:41:58,654 [trainer.py] => config: ./exps/l2p_self_training.json
2023-12-26 10:41:58,654 [trainer.py] => prefix:  
2023-12-26 10:41:58,654 [trainer.py] => dataset: cifar224
2023-12-26 10:41:58,654 [trainer.py] => memory_size: 0
2023-12-26 10:41:58,654 [trainer.py] => memory_per_class: 0
2023-12-26 10:41:58,654 [trainer.py] => fixed_memory: False
2023-12-26 10:41:58,654 [trainer.py] => shuffle: True
2023-12-26 10:41:58,654 [trainer.py] => init_cls: 10
2023-12-26 10:41:58,654 [trainer.py] => increment: 10
2023-12-26 10:41:58,654 [trainer.py] => model_name: l2p_self_training
2023-12-26 10:41:58,654 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2023-12-26 10:41:58,654 [trainer.py] => get_original_backbone: True
2023-12-26 10:41:58,654 [trainer.py] => device: [device(type='cuda', index=5)]
2023-12-26 10:41:58,654 [trainer.py] => seed: 1993
2023-12-26 10:41:58,654 [trainer.py] => tuned_epoch: 5
2023-12-26 10:41:58,655 [trainer.py] => init_lr: 0.001875
2023-12-26 10:41:58,655 [trainer.py] => batch_size: 16
2023-12-26 10:41:58,655 [trainer.py] => weight_decay: 0
2023-12-26 10:41:58,655 [trainer.py] => min_lr: 1e-05
2023-12-26 10:41:58,655 [trainer.py] => optimizer: adam
2023-12-26 10:41:58,655 [trainer.py] => scheduler: constant
2023-12-26 10:41:58,655 [trainer.py] => reinit_optimizer: True
2023-12-26 10:41:58,655 [trainer.py] => global_pool: token
2023-12-26 10:41:58,655 [trainer.py] => head_type: prompt
2023-12-26 10:41:58,655 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2023-12-26 10:41:58,655 [trainer.py] => pretrained: True
2023-12-26 10:41:58,655 [trainer.py] => drop: 0.0
2023-12-26 10:41:58,655 [trainer.py] => drop_path: 0.0
2023-12-26 10:41:58,655 [trainer.py] => prompt_pool: True
2023-12-26 10:41:58,655 [trainer.py] => pool_size: 10
2023-12-26 10:41:58,655 [trainer.py] => length: 5
2023-12-26 10:41:58,655 [trainer.py] => top_k: 5
2023-12-26 10:41:58,655 [trainer.py] => initializer: uniform
2023-12-26 10:41:58,655 [trainer.py] => prompt_key: True
2023-12-26 10:41:58,655 [trainer.py] => prompt_key_init: uniform
2023-12-26 10:41:58,655 [trainer.py] => use_prompt_mask: False
2023-12-26 10:41:58,656 [trainer.py] => shared_prompt_pool: False
2023-12-26 10:41:58,656 [trainer.py] => shared_prompt_key: False
2023-12-26 10:41:58,656 [trainer.py] => batchwise_prompt: True
2023-12-26 10:41:58,656 [trainer.py] => embedding_key: cls
2023-12-26 10:41:58,656 [trainer.py] => predefined_key: 
2023-12-26 10:41:58,656 [trainer.py] => pull_constraint: True
2023-12-26 10:41:58,656 [trainer.py] => pull_constraint_coeff: 0.1
2023-12-26 10:41:58,656 [trainer.py] => semi_supervised_mode: True
2023-12-26 10:41:58,656 [trainer.py] => labeled_ratio: 0.05
2023-12-26 10:41:58,656 [trainer.py] => unlabeled_data_distribution_mode: no_ood
2023-12-26 10:41:58,656 [trainer.py] => confidence_threshold: 0.9
2023-12-26 10:41:58,656 [trainer.py] => max_self_training_iteration: 5
Files already downloaded and verified
Files already downloaded and verified
2023-12-26 10:42:00,586 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-26 10:42:02,395 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2023-12-26 10:42:02,396 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2023-12-26 10:42:05,468 [l2p_self_training.py] => 85,940,836 model total parameters.
2023-12-26 10:42:05,469 [l2p_self_training.py] => 122,980 model training parameters.
2023-12-26 10:42:05,469 [l2p_self_training.py] => prompt.prompt: 38400
2023-12-26 10:42:05,469 [l2p_self_training.py] => prompt.prompt_key: 7680
2023-12-26 10:42:05,470 [l2p_self_training.py] => head.weight: 76800
2023-12-26 10:42:05,470 [l2p_self_training.py] => head.bias: 100
2023-12-26 10:42:05,471 [trainer.py] => All params: 171816392
2023-12-26 10:42:05,473 [trainer.py] => Trainable params: 122980
2023-12-26 10:42:05,473 [l2p_self_training.py] => Learning on 0-10
250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.04s/it]
2023-12-26 10:42:26,395 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40
              precision    recall  f1-score   support

           0       0.78      0.99      0.87       100
           1       0.84      1.00      0.91       100
           2       1.00      0.87      0.93       100
           3       0.85      1.00      0.92       100
           4       1.00      0.87      0.93       100
           5       1.00      0.83      0.91       100
           6       0.85      0.88      0.87       100
           7       0.79      0.97      0.87       100
           8       0.94      0.68      0.79       100
           9       1.00      0.81      0.90       100

    accuracy                           0.89      1000
   macro avg       0.91      0.89      0.89      1000
weighted avg       0.91      0.89      0.89      1000

2023-12-26 10:42:38,981 [trainer.py] => No NME accuracy.
2023-12-26 10:42:38,981 [trainer.py] => CNN: {'total': 89.3, '00-09': 89.3, 'old': 0, 'new': 89.3}
2023-12-26 10:42:38,981 [trainer.py] => CNN top1 curve: [89.3]
2023-12-26 10:42:38,982 [trainer.py] => CNN top5 curve: [98.5]

Average Accuracy (CNN): 89.3
2023-12-26 10:42:38,982 [trainer.py] => Average Accuracy (CNN): 89.3 

2023-12-26 10:42:38,983 [trainer.py] => All params: 171816392
2023-12-26 10:42:38,985 [trainer.py] => Trainable params: 122980
2023-12-26 10:42:38,985 [l2p_self_training.py] => Learning on 10-20
250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.393, Train_accy 84.80, Test_accy 84.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.15s/it]
2023-12-26 10:43:04,791 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.393, Train_accy 84.80, Test_accy 84.40
              precision    recall  f1-score   support

           0       0.82      0.98      0.89       100
           1       0.94      0.96      0.95       100
           2       1.00      0.82      0.90       100
           3       0.83      0.99      0.90       100
           4       1.00      0.68      0.81       100
           5       1.00      0.58      0.73       100
           6       0.89      0.85      0.87       100
           7       0.87      0.80      0.83       100
           8       0.90      0.52      0.66       100
           9       0.99      0.70      0.82       100
          10       0.57      0.96      0.71       100
          11       0.99      0.92      0.95       100
          12       0.69      0.92      0.79       100
          13       0.99      0.96      0.97       100
          14       0.74      0.75      0.74       100
          15       1.00      0.95      0.97       100
          16       0.91      0.92      0.92       100
          17       0.57      0.88      0.69       100
          18       0.95      0.91      0.93       100
          19       0.87      0.83      0.85       100

    accuracy                           0.84      2000
   macro avg       0.88      0.84      0.85      2000
weighted avg       0.88      0.84      0.85      2000

2023-12-26 10:43:16,918 [l2p_self_training.py] => pseudo labeling start
[15 14 13 ... 13 19 17]
2023-12-26 10:43:45,372 [l2p_self_training.py] => 1934 unlabeled samples will be pseudo labeled
2023-12-26 10:43:45,372 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9865563598759048
2023-12-26 10:43:45,668 [l2p_self_training.py] => 26 noisy labeled samples will be added to train
2184
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.037, Train_accy 90.66, Test_accy 78.90: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [01:55<00:00, 23.15s/it]
2023-12-26 10:45:41,443 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.037, Train_accy 90.66, Test_accy 78.90
              precision    recall  f1-score   support

           0       0.83      0.94      0.88       100
           1       0.98      0.92      0.95       100
           2       1.00      0.76      0.86       100
           3       0.87      0.97      0.92       100
           4       1.00      0.44      0.61       100
           5       1.00      0.34      0.51       100
           6       0.89      0.66      0.76       100
           7       0.96      0.65      0.77       100
           8       0.96      0.46      0.62       100
           9       1.00      0.48      0.65       100
          10       0.44      0.99      0.61       100
          11       0.84      0.99      0.91       100
          12       0.81      0.79      0.80       100
          13       1.00      0.95      0.97       100
          14       0.82      0.76      0.79       100
          15       0.83      0.96      0.89       100
          16       0.85      0.93      0.89       100
          17       0.46      0.92      0.61       100
          18       0.79      0.96      0.86       100
          19       0.81      0.91      0.85       100

    accuracy                           0.79      2000
   macro avg       0.86      0.79      0.79      2000
weighted avg       0.86      0.79      0.79      2000

2023-12-26 10:45:53,821 [l2p_self_training.py] => pseudo labeling start
[19 13 15 ... 18 19 13]
2023-12-26 10:46:22,269 [l2p_self_training.py] => 2898 unlabeled samples will be pseudo labeled
2023-12-26 10:46:22,269 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9682539682539683
2023-12-26 10:46:22,605 [l2p_self_training.py] => 92 noisy labeled samples will be added to train
3148
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.023, Train_accy 89.71, Test_accy 69.75: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [02:37<00:00, 31.52s/it]
2023-12-26 10:49:00,229 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.023, Train_accy 89.71, Test_accy 69.75
              precision    recall  f1-score   support

           0       0.91      0.60      0.72       100
           1       0.99      0.81      0.89       100
           2       1.00      0.67      0.80       100
           3       0.96      0.89      0.92       100
           4       1.00      0.14      0.25       100
           5       1.00      0.16      0.28       100
           6       0.94      0.29      0.44       100
           7       0.95      0.57      0.71       100
           8       0.92      0.48      0.63       100
           9       1.00      0.10      0.18       100
          10       0.35      0.96      0.51       100
          11       0.59      0.99      0.74       100
          12       0.80      0.94      0.86       100
          13       0.93      0.98      0.96       100
          14       0.72      0.84      0.78       100
          15       0.34      0.98      0.50       100
          16       0.91      0.96      0.94       100
          17       0.87      0.77      0.81       100
          18       0.93      0.92      0.92       100
          19       0.64      0.90      0.75       100

    accuracy                           0.70      2000
   macro avg       0.84      0.70      0.68      2000
weighted avg       0.84      0.70      0.68      2000

2023-12-26 10:49:12,454 [l2p_self_training.py] => pseudo labeling start
[19 11 11 ... 15 17 13]
2023-12-26 10:49:40,804 [l2p_self_training.py] => 2979 unlabeled samples will be pseudo labeled
2023-12-26 10:49:40,804 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9815374286673381
2023-12-26 10:49:40,984 [l2p_self_training.py] => 55 noisy labeled samples will be added to train
3229
Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.046, Train_accy 91.76, Test_accy 63.70: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [02:41<00:00, 32.40s/it]
2023-12-26 10:52:23,001 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 3 => Loss -0.046, Train_accy 91.76, Test_accy 63.70
              precision    recall  f1-score   support

           0       0.98      0.65      0.78       100
           1       1.00      0.50      0.67       100
           2       1.00      0.58      0.73       100
           3       1.00      0.73      0.84       100
           4       1.00      0.00      0.00       100
           5       1.00      0.04      0.08       100
           6       0.91      0.10      0.18       100
           7       0.95      0.61      0.74       100
           8       0.92      0.45      0.60       100
           9       1.00      0.04      0.08       100
          10       0.20      1.00      0.33       100
          11       0.51      0.99      0.67       100
          12       0.48      0.99      0.64       100
          13       0.89      0.97      0.93       100
          14       0.98      0.65      0.78       100
          15       0.98      0.88      0.93       100
          16       0.68      0.99      0.80       100
          17       0.91      0.70      0.79       100
          18       0.94      0.95      0.95       100
          19       0.76      0.92      0.83       100

    accuracy                           0.64      2000
   macro avg       0.85      0.64      0.62      2000
weighted avg       0.85      0.64      0.62      2000

2023-12-26 10:52:35,903 [l2p_self_training.py] => pseudo labeling start
[16 13 12 ... 17 15 19]
2023-12-26 10:53:04,536 [l2p_self_training.py] => 3149 unlabeled samples will be pseudo labeled
2023-12-26 10:53:04,537 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9711019371228962
2023-12-26 10:53:04,718 [l2p_self_training.py] => 91 noisy labeled samples will be added to train
3399


======================================================================================
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
======================================================================================

在上述方法的基础上，故意在task 2的self-training时加入几十个个噪声标签，标签为新和旧task的标签
(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2023-12-26 11:00:19,282 [trainer.py] => config: ./exps/l2p_self_training.json
2023-12-26 11:00:19,282 [trainer.py] => prefix:  
2023-12-26 11:00:19,282 [trainer.py] => dataset: cifar224
2023-12-26 11:00:19,282 [trainer.py] => memory_size: 0
2023-12-26 11:00:19,282 [trainer.py] => memory_per_class: 0
2023-12-26 11:00:19,282 [trainer.py] => fixed_memory: False
2023-12-26 11:00:19,282 [trainer.py] => shuffle: True
2023-12-26 11:00:19,282 [trainer.py] => init_cls: 10
2023-12-26 11:00:19,282 [trainer.py] => increment: 10
2023-12-26 11:00:19,282 [trainer.py] => model_name: l2p_self_training
2023-12-26 11:00:19,282 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2023-12-26 11:00:19,282 [trainer.py] => get_original_backbone: True
2023-12-26 11:00:19,282 [trainer.py] => device: [device(type='cuda', index=5)]
2023-12-26 11:00:19,283 [trainer.py] => seed: 1993
2023-12-26 11:00:19,283 [trainer.py] => tuned_epoch: 5
2023-12-26 11:00:19,283 [trainer.py] => init_lr: 0.001875
2023-12-26 11:00:19,283 [trainer.py] => batch_size: 16
2023-12-26 11:00:19,283 [trainer.py] => weight_decay: 0
2023-12-26 11:00:19,283 [trainer.py] => min_lr: 1e-05
2023-12-26 11:00:19,283 [trainer.py] => optimizer: adam
2023-12-26 11:00:19,283 [trainer.py] => scheduler: constant
2023-12-26 11:00:19,283 [trainer.py] => reinit_optimizer: True
2023-12-26 11:00:19,283 [trainer.py] => global_pool: token
2023-12-26 11:00:19,283 [trainer.py] => head_type: prompt
2023-12-26 11:00:19,283 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2023-12-26 11:00:19,283 [trainer.py] => pretrained: True
2023-12-26 11:00:19,283 [trainer.py] => drop: 0.0
2023-12-26 11:00:19,283 [trainer.py] => drop_path: 0.0
2023-12-26 11:00:19,283 [trainer.py] => prompt_pool: True
2023-12-26 11:00:19,283 [trainer.py] => pool_size: 10
2023-12-26 11:00:19,283 [trainer.py] => length: 5
2023-12-26 11:00:19,283 [trainer.py] => top_k: 5
2023-12-26 11:00:19,283 [trainer.py] => initializer: uniform
2023-12-26 11:00:19,283 [trainer.py] => prompt_key: True
2023-12-26 11:00:19,283 [trainer.py] => prompt_key_init: uniform
2023-12-26 11:00:19,283 [trainer.py] => use_prompt_mask: False
2023-12-26 11:00:19,283 [trainer.py] => shared_prompt_pool: False
2023-12-26 11:00:19,283 [trainer.py] => shared_prompt_key: False
2023-12-26 11:00:19,283 [trainer.py] => batchwise_prompt: True
2023-12-26 11:00:19,283 [trainer.py] => embedding_key: cls
2023-12-26 11:00:19,283 [trainer.py] => predefined_key: 
2023-12-26 11:00:19,283 [trainer.py] => pull_constraint: True
2023-12-26 11:00:19,283 [trainer.py] => pull_constraint_coeff: 0.1
2023-12-26 11:00:19,284 [trainer.py] => semi_supervised_mode: True
2023-12-26 11:00:19,284 [trainer.py] => labeled_ratio: 0.05
2023-12-26 11:00:19,284 [trainer.py] => unlabeled_data_distribution_mode: no_ood
2023-12-26 11:00:19,284 [trainer.py] => confidence_threshold: 0.9
2023-12-26 11:00:19,284 [trainer.py] => max_self_training_iteration: 5
Files already downloaded and verified
Files already downloaded and verified
2023-12-26 11:00:21,311 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-26 11:00:23,537 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2023-12-26 11:00:23,537 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2023-12-26 11:00:27,047 [l2p_self_training.py] => 85,940,836 model total parameters.
2023-12-26 11:00:27,048 [l2p_self_training.py] => 122,980 model training parameters.
2023-12-26 11:00:27,048 [l2p_self_training.py] => prompt.prompt: 38400
2023-12-26 11:00:27,048 [l2p_self_training.py] => prompt.prompt_key: 7680
2023-12-26 11:00:27,049 [l2p_self_training.py] => head.weight: 76800
2023-12-26 11:00:27,049 [l2p_self_training.py] => head.bias: 100
2023-12-26 11:00:27,050 [trainer.py] => All params: 171816392
2023-12-26 11:00:27,051 [trainer.py] => Trainable params: 122980
2023-12-26 11:00:27,051 [l2p_self_training.py] => Learning on 0-10
250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.01s/it]
2023-12-26 11:00:47,814 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.500, Train_accy 82.40, Test_accy 89.40
              precision    recall  f1-score   support

           0       0.78      0.99      0.87       100
           1       0.84      1.00      0.91       100
           2       1.00      0.87      0.93       100
           3       0.85      1.00      0.92       100
           4       1.00      0.87      0.93       100
           5       1.00      0.83      0.91       100
           6       0.85      0.88      0.87       100
           7       0.79      0.97      0.87       100
           8       0.94      0.68      0.79       100
           9       1.00      0.81      0.90       100

    accuracy                           0.89      1000
   macro avg       0.91      0.89      0.89      1000
weighted avg       0.91      0.89      0.89      1000

2023-12-26 11:01:00,354 [trainer.py] => No NME accuracy.
2023-12-26 11:01:00,355 [trainer.py] => CNN: {'total': 89.3, '00-09': 89.3, 'old': 0, 'new': 89.3}
2023-12-26 11:01:00,355 [trainer.py] => CNN top1 curve: [89.3]
2023-12-26 11:01:00,355 [trainer.py] => CNN top5 curve: [98.5]

Average Accuracy (CNN): 89.3
2023-12-26 11:01:00,355 [trainer.py] => Average Accuracy (CNN): 89.3 

2023-12-26 11:01:00,358 [trainer.py] => All params: 171816392
2023-12-26 11:01:00,359 [trainer.py] => Trainable params: 122980
2023-12-26 11:01:00,359 [l2p_self_training.py] => Learning on 10-20
250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.393, Train_accy 84.80, Test_accy 84.40: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:26<00:00,  5.21s/it]
2023-12-26 11:01:26,454 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.393, Train_accy 84.80, Test_accy 84.40
              precision    recall  f1-score   support

           0       0.82      0.98      0.89       100
           1       0.94      0.96      0.95       100
           2       1.00      0.82      0.90       100
           3       0.83      0.99      0.90       100
           4       1.00      0.68      0.81       100
           5       1.00      0.58      0.73       100
           6       0.89      0.85      0.87       100
           7       0.87      0.80      0.83       100
           8       0.90      0.52      0.66       100
           9       0.99      0.70      0.82       100
          10       0.57      0.96      0.71       100
          11       0.99      0.92      0.95       100
          12       0.69      0.92      0.79       100
          13       0.99      0.96      0.97       100
          14       0.74      0.75      0.74       100
          15       1.00      0.95      0.97       100
          16       0.91      0.92      0.92       100
          17       0.57      0.88      0.69       100
          18       0.95      0.91      0.93       100
          19       0.87      0.83      0.85       100

    accuracy                           0.84      2000
   macro avg       0.88      0.84      0.85      2000
weighted avg       0.88      0.84      0.85      2000

2023-12-26 11:01:38,549 [l2p_self_training.py] => pseudo labeling start
[18 17 17 18 18  3 19 12 18  1  3 15 12 10 16 10  3 12 10 10 15  3 15 10
 12 10]
2023-12-26 11:02:06,780 [l2p_self_training.py] => 1934 unlabeled samples will be pseudo labeled
2023-12-26 11:02:06,781 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.9865563598759048
2023-12-26 11:02:06,786 [l2p_self_training.py] => 26 noisy labeled samples will be added to train
2184
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 90.48, Test_accy 62.45: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [01:54<00:00, 22.84s/it]
2023-12-26 11:04:00,973 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss inf, Train_accy 90.48, Test_accy 62.45
              precision    recall  f1-score   support

           0       0.62      1.00      0.77       100
           1       0.67      0.97      0.80       100
           2       0.97      0.94      0.95       100
           3       0.43      1.00      0.60       100
           4       0.98      0.59      0.74       100
           5       0.64      0.83      0.72       100
           6       0.42      0.89      0.57       100
           7       0.33      0.92      0.49       100
           8       0.38      0.81      0.52       100
           9       0.78      0.73      0.75       100
          10       1.00      0.12      0.21       100
          11       1.00      0.48      0.65       100
          12       1.00      0.31      0.47       100
          13       1.00      0.36      0.53       100
          14       1.00      0.06      0.11       100
          15       1.00      0.25      0.40       100
          16       1.00      0.59      0.74       100
          17       1.00      0.38      0.55       100
          18       1.00      0.77      0.87       100
          19       1.00      0.49      0.66       100

    accuracy                           0.62      2000
   macro avg       0.81      0.62      0.61      2000
weighted avg       0.81      0.62      0.61      2000

2023-12-26 11:04:13,235 [l2p_self_training.py] => pseudo labeling start
[7 5 3 3 8 7 1 6 6 8 7 5 7 1 7 3 7 8 8 8 7 6 1 1 8 8 6 6 6 7 7 3 3 8 8 7 7
 8 3 8 3 8 3 7 8 8 6 6 6 8 3 8 3 8 3 6 6 3 8 8 5 8 7 6 6 3 8 8 8 8 3 8 8 8
 6 8 8 6 6 6 7 8 8 6 3 8 7 3 9 8 9 6 3 3 6 8 3 8 6]
2023-12-26 11:04:41,730 [l2p_self_training.py] => 590 unlabeled samples will be pseudo labeled
2023-12-26 11:04:41,730 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.8322033898305085
2023-12-26 11:04:41,733 [l2p_self_training.py] => 99 noisy labeled samples will be added to train
840
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss inf, Train_accy 79.76, Test_accy 31.70: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:53<00:00, 10.65s/it]
2023-12-26 11:05:35,010 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss inf, Train_accy 79.76, Test_accy 31.70
              precision    recall  f1-score   support

           0       1.00      0.07      0.13       100
           1       0.24      1.00      0.38       100
           2       0.99      0.91      0.95       100
           3       0.31      0.99      0.47       100
           4       1.00      0.00      0.00       100
           5       0.90      0.44      0.59       100
           6       0.16      0.97      0.27       100
           7       0.28      0.91      0.43       100
           8       0.51      0.61      0.56       100
           9       0.90      0.44      0.59       100
          10       1.00      0.00      0.00       100
          11       1.00      0.00      0.00       100
          12       1.00      0.00      0.00       100
          13       1.00      0.00      0.00       100
          14       1.00      0.00      0.00       100
          15       1.00      0.00      0.00       100
          16       1.00      0.00      0.00       100
          17       1.00      0.00      0.00       100
          18       1.00      0.00      0.00       100
          19       1.00      0.00      0.00       100

    accuracy                           0.32      2000
   macro avg       0.81      0.32      0.22      2000
weighted avg       0.81      0.32      0.22      2000

2023-12-26 11:05:47,746 [l2p_self_training.py] => pseudo labeling start
[6 6 1 6 6 6 6 6 6 6 6 7 6 6 7 6 3 6 1 6 6 7 6 7 6 6 6 6 6 6 3 1 7 6 6 6 6
 6 3 6 6 6 6 6 6 6 1 6 6 3 1 6 6 6 6 6 1 6 6 3 6 6 6 6 6 6 7 7 6 7 6 6 6 1
 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 1 3 6 6 1 6 6 7 6 6 6 6 6 6 6 6 6 1 3 8 6 6
 7 6 6 7 6 6 6 6 6 6 6 6 6 1 7 6 7 6 7 6 6 6 6 6 6 6 6 1 6 6 6 6 7 6 6 7 6
 6 6 7 6 6 6 6 6 6 3 1 6 1 6 6 7 7 3 6 7 7 6 6 6 6 6 6 6 6 6 7 6 1 6 6 6 6
 6 1 1 6 6 6 6 6 6 6 6 6 6 3 6 6 6 6 6 6 6 6 6 6 6 6 7 6 6 6 6 6 6 6 7 6 6
 6 6 6 6 6 6 1 1 1 6 6 6 1 6 6 6 6 6 6 3 6 6 6 6 7 6 6 6 1 6 6 6 1 6 6 6 6
 6 6 6 6 7 6]
2023-12-26 11:06:16,227 [l2p_self_training.py] => 265 unlabeled samples will be pseudo labeled
2023-12-26 11:06:16,227 [l2p_self_training.py] => pseudo labeling finish
Pseudo Accuracy:  0.0
2023-12-26 11:06:16,233 [l2p_self_training.py] => 265 noisy labeled samples will be added to train
515
