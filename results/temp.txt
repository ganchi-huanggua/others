(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2024-02-28 10:24:32,628 [trainer.py] => config: ./exps/l2p_self_training.json
2024-02-28 10:24:32,629 [trainer.py] => prefix:  
2024-02-28 10:24:32,629 [trainer.py] => dataset: cifar224
2024-02-28 10:24:32,629 [trainer.py] => memory_size: 0
2024-02-28 10:24:32,629 [trainer.py] => memory_per_class: 0
2024-02-28 10:24:32,629 [trainer.py] => fixed_memory: False
2024-02-28 10:24:32,629 [trainer.py] => shuffle: True
2024-02-28 10:24:32,629 [trainer.py] => init_cls: 10
2024-02-28 10:24:32,629 [trainer.py] => increment: 10
2024-02-28 10:24:32,629 [trainer.py] => model_name: l2p_self_training
2024-02-28 10:24:32,629 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2024-02-28 10:24:32,629 [trainer.py] => get_original_backbone: True
2024-02-28 10:24:32,629 [trainer.py] => device: [device(type='cuda', index=5)]
2024-02-28 10:24:32,629 [trainer.py] => seed: 1993
2024-02-28 10:24:32,629 [trainer.py] => tuned_epoch: 5
2024-02-28 10:24:32,629 [trainer.py] => init_lr: 0.001875
2024-02-28 10:24:32,629 [trainer.py] => batch_size: 16
2024-02-28 10:24:32,629 [trainer.py] => weight_decay: 0
2024-02-28 10:24:32,629 [trainer.py] => min_lr: 1e-05
2024-02-28 10:24:32,629 [trainer.py] => optimizer: adam
2024-02-28 10:24:32,629 [trainer.py] => scheduler: constant
2024-02-28 10:24:32,629 [trainer.py] => reinit_optimizer: True
2024-02-28 10:24:32,629 [trainer.py] => global_pool: token
2024-02-28 10:24:32,629 [trainer.py] => head_type: prompt
2024-02-28 10:24:32,629 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2024-02-28 10:24:32,629 [trainer.py] => pretrained: True
2024-02-28 10:24:32,629 [trainer.py] => drop: 0.0
2024-02-28 10:24:32,629 [trainer.py] => drop_path: 0.0
2024-02-28 10:24:32,629 [trainer.py] => prompt_pool: True
2024-02-28 10:24:32,630 [trainer.py] => pool_size: 10
2024-02-28 10:24:32,630 [trainer.py] => length: 5
2024-02-28 10:24:32,630 [trainer.py] => top_k: 5
2024-02-28 10:24:32,630 [trainer.py] => initializer: uniform
2024-02-28 10:24:32,630 [trainer.py] => prompt_key: True
2024-02-28 10:24:32,630 [trainer.py] => prompt_key_init: uniform
2024-02-28 10:24:32,630 [trainer.py] => use_prompt_mask: False
2024-02-28 10:24:32,630 [trainer.py] => shared_prompt_pool: False
2024-02-28 10:24:32,630 [trainer.py] => shared_prompt_key: False
2024-02-28 10:24:32,630 [trainer.py] => batchwise_prompt: True
2024-02-28 10:24:32,630 [trainer.py] => embedding_key: cls
2024-02-28 10:24:32,630 [trainer.py] => predefined_key: 
2024-02-28 10:24:32,630 [trainer.py] => pull_constraint: True
2024-02-28 10:24:32,630 [trainer.py] => pull_constraint_coeff: 0.1
2024-02-28 10:24:32,630 [trainer.py] => semi_supervised_mode: True
2024-02-28 10:24:32,630 [trainer.py] => labeled_ratio: 0.05
2024-02-28 10:24:32,630 [trainer.py] => unlabeled_data_distribution_mode: future_oot
2024-02-28 10:24:32,630 [trainer.py] => confidence_threshold: 0.9
2024-02-28 10:24:32,630 [trainer.py] => max_self_training_iteration: 3
Files already downloaded and verified
Files already downloaded and verified
2024-02-28 10:24:34,480 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-02-28 10:24:36,295 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2024-02-28 10:24:36,295 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2024-02-28 10:24:38,550 [l2p_self_training.py] => 85,940,836 model total parameters.
2024-02-28 10:24:38,550 [l2p_self_training.py] => 122,980 model training parameters.
2024-02-28 10:24:38,551 [l2p_self_training.py] => prompt.prompt: 38400
2024-02-28 10:24:38,551 [l2p_self_training.py] => prompt.prompt_key: 7680
2024-02-28 10:24:38,551 [l2p_self_training.py] => head.weight: 76800
2024-02-28 10:24:38,551 [l2p_self_training.py] => head.bias: 100
2024-02-28 10:24:38,552 [trainer.py] => All params: 171816392
2024-02-28 10:24:38,553 [trainer.py] => Trainable params: 122980
2024-02-28 10:24:38,553 [l2p_self_training.py] => Learning on 0-10
4750
2024-02-28 10:24:39,225 [l2p_self_training.py] => train dataset length: 250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.515, Train_accy 84.00, Test_accy 89.50: 100%|███████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.88s/it]
2024-02-28 10:24:58,640 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.515, Train_accy 84.00, Test_accy 89.50
2024-02-28 10:24:58,642 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:25:26,412 [l2p_self_training.py] => wrong labeled samples count: 249
2024-02-28 10:25:26,414 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 10:25:26,414 [l2p_self_training.py] => it label on future sample count: 233
2024-02-28 10:25:26,414 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-28 10:25:26,414 [l2p_self_training.py] => 1274 unlabeled samples will be pseudo labeled
2024-02-28 10:25:26,414 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:25:26,415 [toolkit.py] => Pseudo Accuracy: 0.804552590266876
[0 1 2 3 4 5 6 7 8 9]
2024-02-28 10:25:26,418 [l2p_self_training.py] => train dataset length: 1524
Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.150, Train_accy 88.39, Test_accy 93.00: 100%|███████████████████████████████████████████████████| 5/5 [01:15<00:00, 15.14s/it]
2024-02-28 10:26:42,107 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.150, Train_accy 88.39, Test_accy 93.00
2024-02-28 10:26:42,110 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:27:09,913 [l2p_self_training.py] => wrong labeled samples count: 792
2024-02-28 10:27:09,915 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 10:27:09,915 [l2p_self_training.py] => it label on future sample count: 731
2024-02-28 10:27:09,915 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-28 10:27:09,915 [l2p_self_training.py] => 2460 unlabeled samples will be pseudo labeled
2024-02-28 10:27:09,915 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:27:09,915 [toolkit.py] => Pseudo Accuracy: 0.6780487804878049
[0 1 2 3 4 5 6 7 8 9]
2024-02-28 10:27:09,921 [l2p_self_training.py] => train dataset length: 2710
Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.076, Train_accy 87.53, Test_accy 94.20: 100%|███████████████████████████████████████████████████| 5/5 [02:09<00:00, 25.90s/it]
2024-02-28 10:29:19,436 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.076, Train_accy 87.53, Test_accy 94.20
2024-02-28 10:29:19,439 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:29:47,403 [l2p_self_training.py] => wrong labeled samples count: 1127
2024-02-28 10:29:47,405 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 10:29:47,405 [l2p_self_training.py] => it label on future sample count: 1058
2024-02-28 10:29:47,405 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-28 10:29:47,405 [l2p_self_training.py] => 2925 unlabeled samples will be pseudo labeled
2024-02-28 10:29:47,405 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:29:47,405 [toolkit.py] => Pseudo Accuracy: 0.6147008547008547
[0 1 2 3 4 5 6 7 8 9]
2024-02-28 10:29:53,517 [trainer.py] => No NME accuracy.
2024-02-28 10:29:53,517 [trainer.py] => CNN: {'total': 94.2, '00-09': 94.2, 'old': 0, 'new': 94.2}
2024-02-28 10:29:53,517 [trainer.py] => CNN top1 curve: [94.2]
2024-02-28 10:29:53,518 [trainer.py] => CNN top5 curve: [99.7]

Average Accuracy (CNN): 94.2
2024-02-28 10:29:53,518 [trainer.py] => Average Accuracy (CNN): 94.2 

2024-02-28 10:29:53,520 [trainer.py] => All params: 171816392
2024-02-28 10:29:53,521 [trainer.py] => Trainable params: 122980
2024-02-28 10:29:53,522 [l2p_self_training.py] => Learning on 10-20
4500
2024-02-28 10:29:53,597 [l2p_self_training.py] => train dataset length: 250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.027, Train_accy 91.20, Test_accy 74.30: 100%|██████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.03s/it]
2024-02-28 10:30:18,736 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.027, Train_accy 91.20, Test_accy 74.30
2024-02-28 10:30:18,740 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:30:45,089 [l2p_self_training.py] => wrong labeled samples count: 1265
2024-02-28 10:30:45,094 [l2p_self_training.py] => previous label on future samples count: 743
2024-02-28 10:30:45,094 [l2p_self_training.py] => it label on future sample count: 48
2024-02-28 10:30:45,094 [l2p_self_training.py] => previous label on it sample count: 471
2024-02-28 10:30:45,094 [l2p_self_training.py] => 1763 unlabeled samples will be pseudo labeled
2024-02-28 10:30:45,094 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:30:45,095 [toolkit.py] => Pseudo Accuracy: 0.2824730572887124
[10 11 12 13 14 15 16 17 18 19]
2024-02-28 10:30:45,096 [l2p_self_training.py] => train dataset length: 799
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.073, Train_accy 91.99, Test_accy 79.85: 100%|██████████████████████████████████████████████████| 5/5 [00:49<00:00,  9.99s/it]
2024-02-28 10:31:35,040 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.073, Train_accy 91.99, Test_accy 79.85
2024-02-28 10:31:35,042 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:32:01,439 [l2p_self_training.py] => wrong labeled samples count: 1187
2024-02-28 10:32:01,441 [l2p_self_training.py] => previous label on future samples count: 682
2024-02-28 10:32:01,442 [l2p_self_training.py] => it label on future sample count: 145
2024-02-28 10:32:01,442 [l2p_self_training.py] => previous label on it sample count: 343
2024-02-28 10:32:01,442 [l2p_self_training.py] => 2022 unlabeled samples will be pseudo labeled
2024-02-28 10:32:01,442 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:32:01,442 [toolkit.py] => Pseudo Accuracy: 0.41295746785361026
[10 11 12 13 14 15 16 17 18 19]
2024-02-28 10:32:01,446 [l2p_self_training.py] => train dataset length: 1247
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.048, Train_accy 92.14, Test_accy 82.75: 100%|██████████████████████████████████████████████████| 5/5 [01:09<00:00, 13.88s/it]
2024-02-28 10:33:10,851 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.048, Train_accy 92.14, Test_accy 82.75
2024-02-28 10:33:10,853 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:33:37,220 [l2p_self_training.py] => wrong labeled samples count: 921
2024-02-28 10:33:37,222 [l2p_self_training.py] => previous label on future samples count: 456
2024-02-28 10:33:37,222 [l2p_self_training.py] => it label on future sample count: 266
2024-02-28 10:33:37,222 [l2p_self_training.py] => previous label on it sample count: 151
2024-02-28 10:33:37,223 [l2p_self_training.py] => 2120 unlabeled samples will be pseudo labeled
2024-02-28 10:33:37,223 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:33:37,224 [toolkit.py] => Pseudo Accuracy: 0.565566037735849
[10 11 12 13 14 15 16 17 18 19]
2024-02-28 10:33:49,036 [trainer.py] => No NME accuracy.
2024-02-28 10:33:49,037 [trainer.py] => CNN: {'total': 82.75, '00-09': 90.0, '10-19': 75.5, 'old': 90.0, 'new': 75.5}
2024-02-28 10:33:49,037 [trainer.py] => CNN top1 curve: [94.2, 82.75]
2024-02-28 10:33:49,037 [trainer.py] => CNN top5 curve: [99.7, 97.15]

Average Accuracy (CNN): 88.475
2024-02-28 10:33:49,037 [trainer.py] => Average Accuracy (CNN): 88.475 

2024-02-28 10:33:49,040 [trainer.py] => All params: 171816392
2024-02-28 10:33:49,041 [trainer.py] => Trainable params: 122980
2024-02-28 10:33:49,041 [l2p_self_training.py] => Learning on 20-30
4250
2024-02-28 10:33:49,199 [l2p_self_training.py] => train dataset length: 250
Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.112, Train_accy 92.00, Test_accy 73.23: 100%|██████████████████████████████████████████████████| 5/5 [00:30<00:00,  6.07s/it]
2024-02-28 10:34:19,547 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.112, Train_accy 92.00, Test_accy 73.23
2024-02-28 10:34:19,550 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:34:44,408 [l2p_self_training.py] => wrong labeled samples count: 1109
2024-02-28 10:34:44,410 [l2p_self_training.py] => previous label on future samples count: 634
2024-02-28 10:34:44,411 [l2p_self_training.py] => it label on future sample count: 1
2024-02-28 10:34:44,411 [l2p_self_training.py] => previous label on it sample count: 474
2024-02-28 10:34:44,411 [l2p_self_training.py] => 1582 unlabeled samples will be pseudo labeled
2024-02-28 10:34:44,411 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:34:44,411 [toolkit.py] => Pseudo Accuracy: 0.29898862199747156
[20 21 22 23 24 25 26 28 29]
2024-02-28 10:34:44,413 [l2p_self_training.py] => train dataset length: 724
Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.161, Train_accy 95.17, Test_accy 78.37: 100%|██████████████████████████████████████████████████| 5/5 [00:51<00:00, 10.33s/it]
2024-02-28 10:35:36,086 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.161, Train_accy 95.17, Test_accy 78.37
2024-02-28 10:35:36,089 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:36:00,820 [l2p_self_training.py] => wrong labeled samples count: 1074
2024-02-28 10:36:00,824 [l2p_self_training.py] => previous label on future samples count: 667
2024-02-28 10:36:00,824 [l2p_self_training.py] => it label on future sample count: 14
2024-02-28 10:36:00,824 [l2p_self_training.py] => previous label on it sample count: 391
2024-02-28 10:36:00,824 [l2p_self_training.py] => 2033 unlabeled samples will be pseudo labeled
2024-02-28 10:36:00,824 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:36:00,825 [toolkit.py] => Pseudo Accuracy: 0.4717166748647319
[20 21 22 23 24 25 26 27 28 29]
2024-02-28 10:36:00,830 [l2p_self_training.py] => train dataset length: 1225
Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.116, Train_accy 93.88, Test_accy 80.77: 100%|██████████████████████████████████████████████████| 5/5 [01:14<00:00, 14.81s/it]
2024-02-28 10:37:14,870 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.116, Train_accy 93.88, Test_accy 80.77
2024-02-28 10:37:14,875 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:37:39,708 [l2p_self_training.py] => wrong labeled samples count: 921
2024-02-28 10:37:39,711 [l2p_self_training.py] => previous label on future samples count: 542
2024-02-28 10:37:39,711 [l2p_self_training.py] => it label on future sample count: 50
2024-02-28 10:37:39,712 [l2p_self_training.py] => previous label on it sample count: 325
2024-02-28 10:37:39,712 [l2p_self_training.py] => 2281 unlabeled samples will be pseudo labeled
2024-02-28 10:37:39,712 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:37:39,712 [toolkit.py] => Pseudo Accuracy: 0.5962297238053486
[20 21 22 23 24 25 26 28 29]
2024-02-28 10:37:57,355 [trainer.py] => No NME accuracy.
2024-02-28 10:37:57,355 [trainer.py] => CNN: {'total': 80.77, '00-09': 90.2, '10-19': 76.2, '20-29': 75.9, 'old': 83.2, 'new': 75.9}
2024-02-28 10:37:57,356 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77]
2024-02-28 10:37:57,356 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07]

Average Accuracy (CNN): 85.90666666666665
2024-02-28 10:37:57,356 [trainer.py] => Average Accuracy (CNN): 85.90666666666665 

2024-02-28 10:37:57,357 [trainer.py] => All params: 171816392
2024-02-28 10:37:57,358 [trainer.py] => Trainable params: 122980
2024-02-28 10:37:57,358 [l2p_self_training.py] => Learning on 30-40
4000
2024-02-28 10:37:57,416 [l2p_self_training.py] => train dataset length: 250
Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.086, Train_accy 93.20, Test_accy 70.78: 100%|██████████████████████████████████████████████████| 5/5 [00:36<00:00,  7.25s/it]
2024-02-28 10:38:33,684 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.086, Train_accy 93.20, Test_accy 70.78
2024-02-28 10:38:33,687 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:38:57,027 [l2p_self_training.py] => wrong labeled samples count: 1264
2024-02-28 10:38:57,029 [l2p_self_training.py] => previous label on future samples count: 477
2024-02-28 10:38:57,029 [l2p_self_training.py] => it label on future sample count: 7
2024-02-28 10:38:57,029 [l2p_self_training.py] => previous label on it sample count: 780
2024-02-28 10:38:57,029 [l2p_self_training.py] => 1682 unlabeled samples will be pseudo labeled
2024-02-28 10:38:57,029 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:38:57,029 [toolkit.py] => Pseudo Accuracy: 0.24851367419738407
[33 34 35 36 38 39]
2024-02-28 10:38:57,031 [l2p_self_training.py] => train dataset length: 675
Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.173, Train_accy 94.37, Test_accy 72.88: 100%|██████████████████████████████████████████████████| 5/5 [00:55<00:00, 11.06s/it]
2024-02-28 10:39:52,339 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.173, Train_accy 94.37, Test_accy 72.88
2024-02-28 10:39:52,341 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:40:15,697 [l2p_self_training.py] => wrong labeled samples count: 1116
2024-02-28 10:40:15,700 [l2p_self_training.py] => previous label on future samples count: 439
2024-02-28 10:40:15,700 [l2p_self_training.py] => it label on future sample count: 29
2024-02-28 10:40:15,700 [l2p_self_training.py] => previous label on it sample count: 645
2024-02-28 10:40:15,700 [l2p_self_training.py] => 1853 unlabeled samples will be pseudo labeled
2024-02-28 10:40:15,700 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:40:15,701 [toolkit.py] => Pseudo Accuracy: 0.397733405288721
[33 34 35 36 37 38 39]
2024-02-28 10:40:15,704 [l2p_self_training.py] => train dataset length: 1019
Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.131, Train_accy 94.50, Test_accy 74.95: 100%|██████████████████████████████████████████████████| 5/5 [01:10<00:00, 14.11s/it]
2024-02-28 10:41:26,246 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.131, Train_accy 94.50, Test_accy 74.95
2024-02-28 10:41:26,250 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:41:49,556 [l2p_self_training.py] => wrong labeled samples count: 925
2024-02-28 10:41:49,558 [l2p_self_training.py] => previous label on future samples count: 384
2024-02-28 10:41:49,558 [l2p_self_training.py] => it label on future sample count: 43
2024-02-28 10:41:49,558 [l2p_self_training.py] => previous label on it sample count: 494
2024-02-28 10:41:49,558 [l2p_self_training.py] => 1796 unlabeled samples will be pseudo labeled
2024-02-28 10:41:49,558 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:41:49,558 [toolkit.py] => Pseudo Accuracy: 0.4849665924276169
[30 31 32 33 34 35 36 37 38 39]
2024-02-28 10:42:12,856 [trainer.py] => No NME accuracy.
2024-02-28 10:42:12,856 [trainer.py] => CNN: {'total': 74.95, '00-09': 89.4, '10-19': 76.3, '20-29': 76.1, '30-39': 58.0, 'old': 80.6, 'new': 58.0}
2024-02-28 10:42:12,857 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95]
2024-02-28 10:42:12,857 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58]

Average Accuracy (CNN): 83.16749999999999
2024-02-28 10:42:12,857 [trainer.py] => Average Accuracy (CNN): 83.16749999999999 

2024-02-28 10:42:12,858 [trainer.py] => All params: 171816392
2024-02-28 10:42:12,859 [trainer.py] => Trainable params: 122980
2024-02-28 10:42:12,859 [l2p_self_training.py] => Learning on 40-50
3750
2024-02-28 10:42:12,916 [l2p_self_training.py] => train dataset length: 250
Task 4, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.136, Train_accy 94.00, Test_accy 69.10: 100%|██████████████████████████████████████████████████| 5/5 [00:42<00:00,  8.47s/it]
2024-02-28 10:42:55,267 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.136, Train_accy 94.00, Test_accy 69.10
2024-02-28 10:42:55,270 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:43:17,138 [l2p_self_training.py] => wrong labeled samples count: 768
2024-02-28 10:43:17,141 [l2p_self_training.py] => previous label on future samples count: 350
2024-02-28 10:43:17,141 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 10:43:17,141 [l2p_self_training.py] => previous label on it sample count: 418
2024-02-28 10:43:17,141 [l2p_self_training.py] => 1011 unlabeled samples will be pseudo labeled
2024-02-28 10:43:17,141 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:43:17,141 [toolkit.py] => Pseudo Accuracy: 0.2403560830860534
[40 41 42 43 44 46 47 48 49]
2024-02-28 10:43:17,143 [l2p_self_training.py] => train dataset length: 493
Task 4, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.203, Train_accy 95.74, Test_accy 71.38: 100%|██████████████████████████████████████████████████| 5/5 [00:52<00:00, 10.54s/it]
2024-02-28 10:44:09,839 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.203, Train_accy 95.74, Test_accy 71.38
2024-02-28 10:44:09,842 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:44:31,708 [l2p_self_training.py] => wrong labeled samples count: 778
2024-02-28 10:44:31,710 [l2p_self_training.py] => previous label on future samples count: 344
2024-02-28 10:44:31,710 [l2p_self_training.py] => it label on future sample count: 9
2024-02-28 10:44:31,710 [l2p_self_training.py] => previous label on it sample count: 423
2024-02-28 10:44:31,710 [l2p_self_training.py] => 1377 unlabeled samples will be pseudo labeled
2024-02-28 10:44:31,711 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:44:31,711 [toolkit.py] => Pseudo Accuracy: 0.43500363108206247
[40 41 42 43 44 46 47 48 49]
2024-02-28 10:44:31,715 [l2p_self_training.py] => train dataset length: 860
Task 4, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.151, Train_accy 95.35, Test_accy 73.46: 100%|██████████████████████████████████████████████████| 5/5 [01:09<00:00, 13.83s/it]
2024-02-28 10:45:40,870 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.151, Train_accy 95.35, Test_accy 73.46
2024-02-28 10:45:40,872 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:46:02,859 [l2p_self_training.py] => wrong labeled samples count: 547
2024-02-28 10:46:02,861 [l2p_self_training.py] => previous label on future samples count: 303
2024-02-28 10:46:02,861 [l2p_self_training.py] => it label on future sample count: 24
2024-02-28 10:46:02,861 [l2p_self_training.py] => previous label on it sample count: 209
2024-02-28 10:46:02,861 [l2p_self_training.py] => 1476 unlabeled samples will be pseudo labeled
2024-02-28 10:46:02,862 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:46:02,862 [toolkit.py] => Pseudo Accuracy: 0.6294037940379403
[40 41 42 43 44 45 46 47 48 49]
2024-02-28 10:46:32,014 [trainer.py] => No NME accuracy.
2024-02-28 10:46:32,014 [trainer.py] => CNN: {'total': 73.46, '00-09': 86.7, '10-19': 77.2, '20-29': 76.7, '30-39': 59.1, '40-49': 67.6, 'old': 74.92, 'new': 67.6}
2024-02-28 10:46:32,015 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46]
2024-02-28 10:46:32,015 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92]

Average Accuracy (CNN): 81.22599999999998
2024-02-28 10:46:32,015 [trainer.py] => Average Accuracy (CNN): 81.22599999999998 

2024-02-28 10:46:32,017 [trainer.py] => All params: 171816392
2024-02-28 10:46:32,019 [trainer.py] => Trainable params: 122980
2024-02-28 10:46:32,020 [l2p_self_training.py] => Learning on 50-60
3500
2024-02-28 10:46:32,096 [l2p_self_training.py] => train dataset length: 250
Task 5, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.057, Train_accy 89.20, Test_accy 68.67: 100%|██████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.66s/it]
2024-02-28 10:47:20,395 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.057, Train_accy 89.20, Test_accy 68.67
2024-02-28 10:47:20,399 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:47:41,107 [l2p_self_training.py] => wrong labeled samples count: 598
2024-02-28 10:47:41,108 [l2p_self_training.py] => previous label on future samples count: 262
2024-02-28 10:47:41,108 [l2p_self_training.py] => it label on future sample count: 9
2024-02-28 10:47:41,108 [l2p_self_training.py] => previous label on it sample count: 327
2024-02-28 10:47:41,108 [l2p_self_training.py] => 933 unlabeled samples will be pseudo labeled
2024-02-28 10:47:41,108 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:47:41,109 [toolkit.py] => Pseudo Accuracy: 0.3590568060021436
[50 51 52 53 55 56 57 58 59]
2024-02-28 10:47:41,115 [l2p_self_training.py] => train dataset length: 594
Task 5, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.234, Train_accy 96.46, Test_accy 71.33: 100%|██████████████████████████████████████████████████| 5/5 [01:04<00:00, 12.83s/it]
2024-02-28 10:48:45,252 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.234, Train_accy 96.46, Test_accy 71.33
2024-02-28 10:48:45,256 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:49:05,711 [l2p_self_training.py] => wrong labeled samples count: 496
2024-02-28 10:49:05,712 [l2p_self_training.py] => previous label on future samples count: 239
2024-02-28 10:49:05,712 [l2p_self_training.py] => it label on future sample count: 16
2024-02-28 10:49:05,712 [l2p_self_training.py] => previous label on it sample count: 238
2024-02-28 10:49:05,712 [l2p_self_training.py] => 1233 unlabeled samples will be pseudo labeled
2024-02-28 10:49:05,712 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:49:05,712 [toolkit.py] => Pseudo Accuracy: 0.5977291159772912
[50 51 52 53 54 55 56 57 58 59]
2024-02-28 10:49:05,718 [l2p_self_training.py] => train dataset length: 1006
Task 5, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.145, Train_accy 94.53, Test_accy 72.08: 100%|██████████████████████████████████████████████████| 5/5 [01:21<00:00, 16.38s/it]
2024-02-28 10:50:27,599 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.145, Train_accy 94.53, Test_accy 72.08
2024-02-28 10:50:27,601 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:50:48,104 [l2p_self_training.py] => wrong labeled samples count: 426
2024-02-28 10:50:48,105 [l2p_self_training.py] => previous label on future samples count: 232
2024-02-28 10:50:48,105 [l2p_self_training.py] => it label on future sample count: 48
2024-02-28 10:50:48,106 [l2p_self_training.py] => previous label on it sample count: 130
2024-02-28 10:50:48,106 [l2p_self_training.py] => 1514 unlabeled samples will be pseudo labeled
2024-02-28 10:50:48,106 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:50:48,106 [toolkit.py] => Pseudo Accuracy: 0.7186261558784677
[50 51 52 53 54 55 56 57 58 59]
2024-02-28 10:51:22,801 [trainer.py] => No NME accuracy.
2024-02-28 10:51:22,801 [trainer.py] => CNN: {'total': 72.08, '00-09': 80.6, '10-19': 77.9, '20-29': 76.8, '30-39': 58.8, '40-49': 63.7, '50-59': 74.7, 'old': 71.56, 'new': 74.7}
2024-02-28 10:51:22,801 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46, 72.08]
2024-02-28 10:51:22,801 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92, 95.17]

Average Accuracy (CNN): 79.70166666666665
2024-02-28 10:51:22,801 [trainer.py] => Average Accuracy (CNN): 79.70166666666665 

2024-02-28 10:51:22,803 [trainer.py] => All params: 171816392
2024-02-28 10:51:22,804 [trainer.py] => Trainable params: 122980
2024-02-28 10:51:22,804 [l2p_self_training.py] => Learning on 60-70
3250
2024-02-28 10:51:22,871 [l2p_self_training.py] => train dataset length: 250
Task 6, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.107, Train_accy 93.60, Test_accy 69.31: 100%|██████████████████████████████████████████████████| 5/5 [00:53<00:00, 10.64s/it]
2024-02-28 10:52:16,082 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.107, Train_accy 93.60, Test_accy 69.31
2024-02-28 10:52:16,084 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:52:35,124 [l2p_self_training.py] => wrong labeled samples count: 554
2024-02-28 10:52:35,127 [l2p_self_training.py] => previous label on future samples count: 208
2024-02-28 10:52:35,127 [l2p_self_training.py] => it label on future sample count: 1
2024-02-28 10:52:35,127 [l2p_self_training.py] => previous label on it sample count: 345
2024-02-28 10:52:35,127 [l2p_self_training.py] => 1033 unlabeled samples will be pseudo labeled
2024-02-28 10:52:35,127 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:52:35,127 [toolkit.py] => Pseudo Accuracy: 0.46369796708615685
[60 61 62 63 64 65 66 67 68]
2024-02-28 10:52:35,129 [l2p_self_training.py] => train dataset length: 730
Task 6, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.152, Train_accy 95.21, Test_accy 70.79: 100%|██████████████████████████████████████████████████| 5/5 [01:15<00:00, 15.04s/it]
2024-02-28 10:53:50,310 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.152, Train_accy 95.21, Test_accy 70.79
2024-02-28 10:53:50,314 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:54:09,560 [l2p_self_training.py] => wrong labeled samples count: 497
2024-02-28 10:54:09,562 [l2p_self_training.py] => previous label on future samples count: 206
2024-02-28 10:54:09,563 [l2p_self_training.py] => it label on future sample count: 6
2024-02-28 10:54:09,563 [l2p_self_training.py] => previous label on it sample count: 283
2024-02-28 10:54:09,563 [l2p_self_training.py] => 1250 unlabeled samples will be pseudo labeled
2024-02-28 10:54:09,563 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:54:09,563 [toolkit.py] => Pseudo Accuracy: 0.6024
[60 61 62 63 64 65 66 67 68]
2024-02-28 10:54:09,566 [l2p_self_training.py] => train dataset length: 1011
Task 6, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.156, Train_accy 95.15, Test_accy 71.17: 100%|██████████████████████████████████████████████████| 5/5 [01:27<00:00, 17.58s/it]
2024-02-28 10:55:37,444 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.156, Train_accy 95.15, Test_accy 71.17
2024-02-28 10:55:37,447 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:55:56,723 [l2p_self_training.py] => wrong labeled samples count: 424
2024-02-28 10:55:56,724 [l2p_self_training.py] => previous label on future samples count: 163
2024-02-28 10:55:56,724 [l2p_self_training.py] => it label on future sample count: 31
2024-02-28 10:55:56,724 [l2p_self_training.py] => previous label on it sample count: 217
2024-02-28 10:55:56,724 [l2p_self_training.py] => 1527 unlabeled samples will be pseudo labeled
2024-02-28 10:55:56,724 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:55:56,724 [toolkit.py] => Pseudo Accuracy: 0.722331368696791
[60 61 62 63 64 65 66 67 68]
2024-02-28 10:56:37,275 [trainer.py] => No NME accuracy.
2024-02-28 10:56:37,275 [trainer.py] => CNN: {'total': 71.17, '00-09': 77.9, '10-19': 77.4, '20-29': 77.0, '30-39': 59.4, '40-49': 64.0, '50-59': 71.6, '60-69': 70.9, 'old': 71.22, 'new': 70.9}
2024-02-28 10:56:37,275 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46, 72.08, 71.17]
2024-02-28 10:56:37,275 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92, 95.17, 94.79]

Average Accuracy (CNN): 78.48285714285713
2024-02-28 10:56:37,275 [trainer.py] => Average Accuracy (CNN): 78.48285714285713 

2024-02-28 10:56:37,277 [trainer.py] => All params: 171816392
2024-02-28 10:56:37,278 [trainer.py] => Trainable params: 122980
2024-02-28 10:56:37,279 [l2p_self_training.py] => Learning on 70-80
3000
2024-02-28 10:56:37,352 [l2p_self_training.py] => train dataset length: 250
Task 7, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.073, Train_accy 93.60, Test_accy 65.92: 100%|██████████████████████████████████████████████████| 5/5 [00:58<00:00, 11.79s/it]
2024-02-28 10:57:36,295 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.073, Train_accy 93.60, Test_accy 65.92
2024-02-28 10:57:36,297 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:57:53,853 [l2p_self_training.py] => wrong labeled samples count: 501
2024-02-28 10:57:53,855 [l2p_self_training.py] => previous label on future samples count: 136
2024-02-28 10:57:53,855 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 10:57:53,855 [l2p_self_training.py] => previous label on it sample count: 365
2024-02-28 10:57:53,855 [l2p_self_training.py] => 593 unlabeled samples will be pseudo labeled
2024-02-28 10:57:53,855 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:57:53,855 [toolkit.py] => Pseudo Accuracy: 0.1551433389544688
[70 71 72 73 74 75 77]
2024-02-28 10:57:53,857 [l2p_self_training.py] => train dataset length: 342
Task 7, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.206, Train_accy 94.74, Test_accy 67.44: 100%|██████████████████████████████████████████████████| 5/5 [01:03<00:00, 12.61s/it]
2024-02-28 10:58:56,899 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.206, Train_accy 94.74, Test_accy 67.44
2024-02-28 10:58:56,903 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:59:14,475 [l2p_self_training.py] => wrong labeled samples count: 406
2024-02-28 10:59:14,477 [l2p_self_training.py] => previous label on future samples count: 136
2024-02-28 10:59:14,477 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 10:59:14,477 [l2p_self_training.py] => previous label on it sample count: 268
2024-02-28 10:59:14,477 [l2p_self_training.py] => 699 unlabeled samples will be pseudo labeled
2024-02-28 10:59:14,477 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:59:14,478 [toolkit.py] => Pseudo Accuracy: 0.41917024320457796
[70 71 72 73 74 75 76 77 78 79]
2024-02-28 10:59:14,479 [l2p_self_training.py] => train dataset length: 545
Task 7, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.186, Train_accy 95.23, Test_accy 68.65: 100%|██████████████████████████████████████████████████| 5/5 [01:12<00:00, 14.59s/it]
2024-02-28 11:00:27,407 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.186, Train_accy 95.23, Test_accy 68.65
2024-02-28 11:00:27,410 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:00:44,961 [l2p_self_training.py] => wrong labeled samples count: 364
2024-02-28 11:00:44,963 [l2p_self_training.py] => previous label on future samples count: 124
2024-02-28 11:00:44,963 [l2p_self_training.py] => it label on future sample count: 7
2024-02-28 11:00:44,963 [l2p_self_training.py] => previous label on it sample count: 232
2024-02-28 11:00:44,963 [l2p_self_training.py] => 876 unlabeled samples will be pseudo labeled
2024-02-28 11:00:44,963 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:00:44,963 [toolkit.py] => Pseudo Accuracy: 0.5844748858447488
[70 71 72 73 74 75 77 78 79]
2024-02-28 11:01:31,198 [trainer.py] => No NME accuracy.
2024-02-28 11:01:31,198 [trainer.py] => CNN: {'total': 68.65, '00-09': 79.8, '10-19': 77.7, '20-29': 76.3, '30-39': 60.9, '40-49': 63.8, '50-59': 69.1, '60-69': 67.4, '70-79': 54.2, 'old': 70.71, 'new': 54.2}
2024-02-28 11:01:31,198 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46, 72.08, 71.17, 68.65]
2024-02-28 11:01:31,198 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92, 95.17, 94.79, 94.06]

Average Accuracy (CNN): 77.25374999999998
2024-02-28 11:01:31,198 [trainer.py] => Average Accuracy (CNN): 77.25374999999998 

2024-02-28 11:01:31,200 [trainer.py] => All params: 171816392
2024-02-28 11:01:31,201 [trainer.py] => Trainable params: 122980
2024-02-28 11:01:31,201 [l2p_self_training.py] => Learning on 80-90
2750
2024-02-28 11:01:31,251 [l2p_self_training.py] => train dataset length: 250
Task 8, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.055, Train_accy 90.80, Test_accy 64.39: 100%|██████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.01s/it]
2024-02-28 11:02:36,297 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.055, Train_accy 90.80, Test_accy 64.39
2024-02-28 11:02:36,302 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:02:52,365 [l2p_self_training.py] => wrong labeled samples count: 648
2024-02-28 11:02:52,367 [l2p_self_training.py] => previous label on future samples count: 46
2024-02-28 11:02:52,367 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 11:02:52,367 [l2p_self_training.py] => previous label on it sample count: 602
2024-02-28 11:02:52,367 [l2p_self_training.py] => 809 unlabeled samples will be pseudo labeled
2024-02-28 11:02:52,367 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:02:52,367 [toolkit.py] => Pseudo Accuracy: 0.19901112484548825
[81 82 83 85 86 87 88]
2024-02-28 11:02:52,368 [l2p_self_training.py] => train dataset length: 411
Task 8, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.160, Train_accy 94.16, Test_accy 66.01: 100%|██████████████████████████████████████████████████| 5/5 [01:11<00:00, 14.37s/it]
2024-02-28 11:04:04,197 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.160, Train_accy 94.16, Test_accy 66.01
2024-02-28 11:04:04,199 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:04:20,202 [l2p_self_training.py] => wrong labeled samples count: 574
2024-02-28 11:04:20,204 [l2p_self_training.py] => previous label on future samples count: 34
2024-02-28 11:04:20,205 [l2p_self_training.py] => it label on future sample count: 1
2024-02-28 11:04:20,205 [l2p_self_training.py] => previous label on it sample count: 538
2024-02-28 11:04:20,205 [l2p_self_training.py] => 930 unlabeled samples will be pseudo labeled
2024-02-28 11:04:20,205 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:04:20,205 [toolkit.py] => Pseudo Accuracy: 0.3827956989247312
[81 82 83 84 85 86 87 88]
2024-02-28 11:04:20,208 [l2p_self_training.py] => train dataset length: 608
Task 8, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.191, Train_accy 95.07, Test_accy 66.37: 100%|██████████████████████████████████████████████████| 5/5 [01:20<00:00, 16.12s/it]
2024-02-28 11:05:40,793 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.191, Train_accy 95.07, Test_accy 66.37
2024-02-28 11:05:40,796 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:05:56,878 [l2p_self_training.py] => wrong labeled samples count: 481
2024-02-28 11:05:56,879 [l2p_self_training.py] => previous label on future samples count: 52
2024-02-28 11:05:56,879 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 11:05:56,879 [l2p_self_training.py] => previous label on it sample count: 429
2024-02-28 11:05:56,880 [l2p_self_training.py] => 1035 unlabeled samples will be pseudo labeled
2024-02-28 11:05:56,880 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:05:56,880 [toolkit.py] => Pseudo Accuracy: 0.5352657004830917
[80 81 82 83 84 85 86 87 88 89]
2024-02-28 11:06:48,728 [trainer.py] => No NME accuracy.
2024-02-28 11:06:48,728 [trainer.py] => CNN: {'total': 66.37, '00-09': 78.2, '10-19': 77.9, '20-29': 76.4, '30-39': 61.1, '40-49': 63.6, '50-59': 70.2, '60-69': 68.9, '70-79': 51.9, '80-89': 49.1, 'old': 68.53, 'new': 49.1}
2024-02-28 11:06:48,728 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46, 72.08, 71.17, 68.65, 66.37]
2024-02-28 11:06:48,728 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92, 95.17, 94.79, 94.06, 93.31]

Average Accuracy (CNN): 76.04444444444442
2024-02-28 11:06:48,728 [trainer.py] => Average Accuracy (CNN): 76.04444444444442 

2024-02-28 11:06:48,730 [trainer.py] => All params: 171816392
2024-02-28 11:06:48,731 [trainer.py] => Trainable params: 122980
2024-02-28 11:06:48,731 [l2p_self_training.py] => Learning on 90-100
2500
2024-02-28 11:06:48,779 [l2p_self_training.py] => train dataset length: 250
Task 9, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.044, Train_accy 88.80, Test_accy 64.34: 100%|██████████████████████████████████████████████████| 5/5 [01:11<00:00, 14.24s/it]
2024-02-28 11:07:59,996 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.044, Train_accy 88.80, Test_accy 64.34
2024-02-28 11:08:00,000 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:08:14,657 [l2p_self_training.py] => wrong labeled samples count: 282
2024-02-28 11:08:14,659 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 11:08:14,659 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 11:08:14,659 [l2p_self_training.py] => previous label on it sample count: 282
2024-02-28 11:08:14,660 [l2p_self_training.py] => 604 unlabeled samples will be pseudo labeled
2024-02-28 11:08:14,660 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:08:14,660 [toolkit.py] => Pseudo Accuracy: 0.5331125827814569
[90 91 92 93 94 95 96 97 98]
2024-02-28 11:08:14,663 [l2p_self_training.py] => train dataset length: 572
Task 9, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.209, Train_accy 96.33, Test_accy 65.01: 100%|██████████████████████████████████████████████████| 5/5 [01:25<00:00, 17.01s/it]
2024-02-28 11:09:39,736 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.209, Train_accy 96.33, Test_accy 65.01
2024-02-28 11:09:39,739 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:09:54,333 [l2p_self_training.py] => wrong labeled samples count: 250
2024-02-28 11:09:54,334 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 11:09:54,334 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 11:09:54,334 [l2p_self_training.py] => previous label on it sample count: 248
2024-02-28 11:09:54,334 [l2p_self_training.py] => 867 unlabeled samples will be pseudo labeled
2024-02-28 11:09:54,334 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:09:54,335 [toolkit.py] => Pseudo Accuracy: 0.7116493656286044
[90 91 93 94 95 96 97 98 99]
2024-02-28 11:09:54,338 [l2p_self_training.py] => train dataset length: 869
Task 9, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.181, Train_accy 95.28, Test_accy 65.68: 100%|██████████████████████████████████████████████████| 5/5 [01:37<00:00, 19.58s/it]
2024-02-28 11:11:32,220 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.181, Train_accy 95.28, Test_accy 65.68
2024-02-28 11:11:32,222 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:11:46,840 [l2p_self_training.py] => wrong labeled samples count: 176
2024-02-28 11:11:46,841 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 11:11:46,841 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 11:11:46,841 [l2p_self_training.py] => previous label on it sample count: 169
2024-02-28 11:11:46,841 [l2p_self_training.py] => 1132 unlabeled samples will be pseudo labeled
2024-02-28 11:11:46,842 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:11:46,842 [toolkit.py] => Pseudo Accuracy: 0.8445229681978799
[90 91 92 93 94 95 96 97 98 99]
2024-02-28 11:12:44,129 [trainer.py] => No NME accuracy.
2024-02-28 11:12:44,129 [trainer.py] => CNN: {'total': 65.68, '00-09': 72.8, '10-19': 78.4, '20-29': 76.8, '30-39': 61.5, '40-49': 63.1, '50-59': 69.0, '60-69': 67.6, '70-79': 52.0, '80-89': 48.8, '90-99': 66.8, 'old': 65.56, 'new': 66.8}
2024-02-28 11:12:44,130 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46, 72.08, 71.17, 68.65, 66.37, 65.68]
2024-02-28 11:12:44,130 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92, 95.17, 94.79, 94.06, 93.31, 92.71]

Average Accuracy (CNN): 75.008
2024-02-28 11:12:44,130 [trainer.py] => Average Accuracy (CNN): 75.008 

Accuracy Matrix (CNN):
[[94.2 90.  90.2 89.4 86.7 80.6 77.9 79.8 78.2 72.8]
 [ 0.  75.5 76.2 76.3 77.2 77.9 77.4 77.7 77.9 78.4]
 [ 0.   0.  75.9 76.1 76.7 76.8 77.  76.3 76.4 76.8]
 [ 0.   0.   0.  58.  59.1 58.8 59.4 60.9 61.1 61.5]
 [ 0.   0.   0.   0.  67.6 63.7 64.  63.8 63.6 63.1]
 [ 0.   0.   0.   0.   0.  74.7 71.6 69.1 70.2 69. ]
 [ 0.   0.   0.   0.   0.   0.  70.9 67.4 68.9 67.6]
 [ 0.   0.   0.   0.   0.   0.   0.  54.2 51.9 52. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.  49.1 48.8]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.  66.8]]
2024-02-28 11:12:44,131 [trainer.py] => Forgetting (CNN): 4.177777777777781
