原l2p+Self-Training方法在有future oot数据时伪标签打的对错（分类头为 0:total classes）
(base) root@autodl-container-56644e8033-177721e5:~/autodl-tmp/LAMDA-PILOT# python main.py --config=exps/l2p_self_training.json
2024-02-02 21:45:13,597 [trainer.py] => config: exps/l2p_self_training.json
2024-02-02 21:45:13,597 [trainer.py] => prefix:  
2024-02-02 21:45:13,598 [trainer.py] => dataset: cifar224
2024-02-02 21:45:13,598 [trainer.py] => memory_size: 0
2024-02-02 21:45:13,598 [trainer.py] => memory_per_class: 0
2024-02-02 21:45:13,598 [trainer.py] => fixed_memory: False
2024-02-02 21:45:13,598 [trainer.py] => shuffle: True
2024-02-02 21:45:13,598 [trainer.py] => init_cls: 10
2024-02-02 21:45:13,598 [trainer.py] => increment: 10
2024-02-02 21:45:13,598 [trainer.py] => model_name: l2p_self_training
2024-02-02 21:45:13,598 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2024-02-02 21:45:13,598 [trainer.py] => get_original_backbone: True
2024-02-02 21:45:13,598 [trainer.py] => device: [device(type='cuda', index=0)]
2024-02-02 21:45:13,598 [trainer.py] => seed: 1993
2024-02-02 21:45:13,598 [trainer.py] => tuned_epoch: 5
2024-02-02 21:45:13,598 [trainer.py] => init_lr: 0.001875
2024-02-02 21:45:13,598 [trainer.py] => batch_size: 16
2024-02-02 21:45:13,598 [trainer.py] => weight_decay: 0
2024-02-02 21:45:13,598 [trainer.py] => min_lr: 1e-05
2024-02-02 21:45:13,598 [trainer.py] => optimizer: adam
2024-02-02 21:45:13,598 [trainer.py] => scheduler: constant
2024-02-02 21:45:13,598 [trainer.py] => reinit_optimizer: True
2024-02-02 21:45:13,598 [trainer.py] => global_pool: token
2024-02-02 21:45:13,598 [trainer.py] => head_type: prompt
2024-02-02 21:45:13,598 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2024-02-02 21:45:13,598 [trainer.py] => pretrained: True
2024-02-02 21:45:13,598 [trainer.py] => drop: 0.0
2024-02-02 21:45:13,598 [trainer.py] => drop_path: 0.0
2024-02-02 21:45:13,598 [trainer.py] => prompt_pool: True
2024-02-02 21:45:13,598 [trainer.py] => pool_size: 10
2024-02-02 21:45:13,598 [trainer.py] => length: 5
2024-02-02 21:45:13,598 [trainer.py] => top_k: 5
2024-02-02 21:45:13,598 [trainer.py] => initializer: uniform
2024-02-02 21:45:13,599 [trainer.py] => prompt_key: True
2024-02-02 21:45:13,599 [trainer.py] => prompt_key_init: uniform
2024-02-02 21:45:13,599 [trainer.py] => use_prompt_mask: False
2024-02-02 21:45:13,599 [trainer.py] => shared_prompt_pool: False
2024-02-02 21:45:13,599 [trainer.py] => shared_prompt_key: False
2024-02-02 21:45:13,599 [trainer.py] => batchwise_prompt: True
2024-02-02 21:45:13,599 [trainer.py] => embedding_key: cls
2024-02-02 21:45:13,599 [trainer.py] => predefined_key: 
2024-02-02 21:45:13,599 [trainer.py] => pull_constraint: True
2024-02-02 21:45:13,599 [trainer.py] => pull_constraint_coeff: 0.1
2024-02-02 21:45:13,599 [trainer.py] => semi_supervised_mode: True
2024-02-02 21:45:13,599 [trainer.py] => labeled_ratio: 0.05
2024-02-02 21:45:13,599 [trainer.py] => unlabeled_data_distribution_mode: future_oot
2024-02-02 21:45:13,599 [trainer.py] => confidence_threshold: 0.9
2024-02-02 21:45:13,599 [trainer.py] => max_self_training_iteration: 3
Files already downloaded and verified
Files already downloaded and verified
2024-02-02 21:45:15,132 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-02-02 21:45:16,660 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2024-02-02 21:45:16,660 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2024-02-02 21:45:19,029 [l2p_self_training.py] => 85,940,836 model total parameters.
2024-02-02 21:45:19,029 [l2p_self_training.py] => 122,980 model training parameters.
2024-02-02 21:45:19,029 [l2p_self_training.py] => prompt.prompt: 38400
2024-02-02 21:45:19,029 [l2p_self_training.py] => prompt.prompt_key: 7680
2024-02-02 21:45:19,030 [l2p_self_training.py] => head.weight: 76800
2024-02-02 21:45:19,030 [l2p_self_training.py] => head.bias: 100
2024-02-02 21:45:19,031 [trainer.py] => All params: 171816392
2024-02-02 21:45:19,032 [trainer.py] => Trainable params: 122980
2024-02-02 21:45:19,032 [l2p_self_training.py] => Learning on 0-10
4750
2024-02-02 21:45:19,312 [l2p_self_training.py] => train dataset length: 250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.515, Train_accy 84.00, Test_accy 89.50: 100%|███████████████| 5/5 [00:11<00:00,  2.25s/it]
2024-02-02 21:45:30,581 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.515, Train_accy 84.00, Test_accy 89.50
2024-02-02 21:45:30,588 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:45:45,604 [l2p_self_training.py] => wrong labeled samples count: 249
2024-02-02 21:45:45,605 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 21:45:45,605 [l2p_self_training.py] => it label on future sample count: 233
2024-02-02 21:45:45,605 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-02 21:45:45,606 [l2p_self_training.py] => 1274 unlabeled samples will be pseudo labeled
2024-02-02 21:45:45,606 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:45:45,606 [toolkit.py] => Pseudo Accuracy: 0.804552590266876
2024-02-02 21:45:45,613 [l2p_self_training.py] => train dataset length: 1524
Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.150, Train_accy 88.39, Test_accy 93.00: 100%|███████████████| 5/5 [00:40<00:00,  8.16s/it]
2024-02-02 21:46:26,437 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.150, Train_accy 88.39, Test_accy 93.00
2024-02-02 21:46:26,439 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:46:41,271 [l2p_self_training.py] => wrong labeled samples count: 792
2024-02-02 21:46:41,272 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 21:46:41,272 [l2p_self_training.py] => it label on future sample count: 731
2024-02-02 21:46:41,273 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-02 21:46:41,273 [l2p_self_training.py] => 2460 unlabeled samples will be pseudo labeled
2024-02-02 21:46:41,273 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:46:41,273 [toolkit.py] => Pseudo Accuracy: 0.6780487804878049
2024-02-02 21:46:41,278 [l2p_self_training.py] => train dataset length: 2710
Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.077, Train_accy 87.64, Test_accy 94.20: 100%|███████████████| 5/5 [01:08<00:00, 13.73s/it]
2024-02-02 21:47:49,920 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.077, Train_accy 87.64, Test_accy 94.20
2024-02-02 21:47:49,926 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:48:04,839 [l2p_self_training.py] => wrong labeled samples count: 1123
2024-02-02 21:48:04,843 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 21:48:04,843 [l2p_self_training.py] => it label on future sample count: 1055
2024-02-02 21:48:04,843 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-02 21:48:04,843 [l2p_self_training.py] => 2922 unlabeled samples will be pseudo labeled
2024-02-02 21:48:04,843 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:48:04,843 [toolkit.py] => Pseudo Accuracy: 0.6156741957563313
2024-02-02 21:48:08,262 [trainer.py] => No NME accuracy.
2024-02-02 21:48:08,262 [trainer.py] => CNN: {'total': 94.2, '00-09': 94.2, 'old': 0, 'new': 94.2}
2024-02-02 21:48:08,262 [trainer.py] => CNN top1 curve: [94.2]
2024-02-02 21:48:08,262 [trainer.py] => CNN top5 curve: [99.7]

Average Accuracy (CNN): 94.2
2024-02-02 21:48:08,262 [trainer.py] => Average Accuracy (CNN): 94.2 

2024-02-02 21:48:08,263 [trainer.py] => All params: 171816392
2024-02-02 21:48:08,265 [trainer.py] => Trainable params: 122980
2024-02-02 21:48:08,265 [l2p_self_training.py] => Learning on 10-20
4500
2024-02-02 21:48:08,326 [l2p_self_training.py] => train dataset length: 250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.028, Train_accy 91.60, Test_accy 74.35: 100%|██████████████| 5/5 [00:14<00:00,  2.85s/it]
2024-02-02 21:48:22,571 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.028, Train_accy 91.60, Test_accy 74.35
2024-02-02 21:48:22,578 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:48:36,646 [l2p_self_training.py] => wrong labeled samples count: 1273
2024-02-02 21:48:36,651 [l2p_self_training.py] => previous label on future samples count: 752
2024-02-02 21:48:36,651 [l2p_self_training.py] => it label on future sample count: 48
2024-02-02 21:48:36,651 [l2p_self_training.py] => previous label on it sample count: 471
2024-02-02 21:48:36,651 [l2p_self_training.py] => 1768 unlabeled samples will be pseudo labeled
2024-02-02 21:48:36,651 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:48:36,652 [toolkit.py] => Pseudo Accuracy: 0.27997737556561086
2024-02-02 21:48:36,658 [l2p_self_training.py] => train dataset length: 2018
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.330, Train_accy 79.78, Test_accy 70.20: 100%|███████████████| 5/5 [00:55<00:00, 11.11s/it]
2024-02-02 21:49:32,229 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.330, Train_accy 79.78, Test_accy 70.20
2024-02-02 21:49:32,236 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:49:46,261 [l2p_self_training.py] => wrong labeled samples count: 1429
2024-02-02 21:49:46,267 [l2p_self_training.py] => previous label on future samples count: 865
2024-02-02 21:49:46,267 [l2p_self_training.py] => it label on future sample count: 100
2024-02-02 21:49:46,267 [l2p_self_training.py] => previous label on it sample count: 449
2024-02-02 21:49:46,267 [l2p_self_training.py] => 2085 unlabeled samples will be pseudo labeled
2024-02-02 21:49:46,267 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:49:46,267 [toolkit.py] => Pseudo Accuracy: 0.3146282973621103
2024-02-02 21:49:46,279 [l2p_self_training.py] => train dataset length: 2335
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.199, Train_accy 84.33, Test_accy 66.60: 100%|███████████████| 5/5 [01:03<00:00, 12.61s/it]
2024-02-02 21:50:49,340 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.199, Train_accy 84.33, Test_accy 66.60
2024-02-02 21:50:49,346 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:51:03,425 [l2p_self_training.py] => wrong labeled samples count: 1488
2024-02-02 21:51:03,431 [l2p_self_training.py] => previous label on future samples count: 955
2024-02-02 21:51:03,431 [l2p_self_training.py] => it label on future sample count: 98
2024-02-02 21:51:03,431 [l2p_self_training.py] => previous label on it sample count: 421
2024-02-02 21:51:03,431 [l2p_self_training.py] => 2244 unlabeled samples will be pseudo labeled
2024-02-02 21:51:03,431 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:51:03,432 [toolkit.py] => Pseudo Accuracy: 0.33689839572192515
2024-02-02 21:51:09,894 [trainer.py] => No NME accuracy.
2024-02-02 21:51:09,895 [trainer.py] => CNN: {'total': 66.6, '00-09': 72.7, '10-19': 60.5, 'old': 72.7, 'new': 60.5}
2024-02-02 21:51:09,895 [trainer.py] => CNN top1 curve: [94.2, 66.6]
2024-02-02 21:51:09,895 [trainer.py] => CNN top5 curve: [99.7, 95.35]

Average Accuracy (CNN): 80.4
2024-02-02 21:51:09,895 [trainer.py] => Average Accuracy (CNN): 80.4 

2024-02-02 21:51:09,899 [trainer.py] => All params: 171816392
2024-02-02 21:51:09,902 [trainer.py] => Trainable params: 122980
2024-02-02 21:51:09,902 [l2p_self_training.py] => Learning on 20-30
4250
2024-02-02 21:51:09,971 [l2p_self_training.py] => train dataset length: 250
Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.102, Train_accy 92.40, Test_accy 55.50: 100%|██████████████| 5/5 [00:17<00:00,  3.49s/it]
2024-02-02 21:51:27,446 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.102, Train_accy 92.40, Test_accy 55.50
2024-02-02 21:51:27,453 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:51:40,757 [l2p_self_training.py] => wrong labeled samples count: 1553
2024-02-02 21:51:40,762 [l2p_self_training.py] => previous label on future samples count: 897
2024-02-02 21:51:40,763 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 21:51:40,763 [l2p_self_training.py] => previous label on it sample count: 656
2024-02-02 21:51:40,763 [l2p_self_training.py] => 1641 unlabeled samples will be pseudo labeled
2024-02-02 21:51:40,763 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:51:40,763 [toolkit.py] => Pseudo Accuracy: 0.05362583790371724
2024-02-02 21:51:40,767 [l2p_self_training.py] => train dataset length: 1891
Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.271, Train_accy 81.23, Test_accy 46.33: 100%|███████████████| 5/5 [00:55<00:00, 11.18s/it]
2024-02-02 21:52:36,673 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.271, Train_accy 81.23, Test_accy 46.33
2024-02-02 21:52:36,680 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:52:50,036 [l2p_self_training.py] => wrong labeled samples count: 1892
2024-02-02 21:52:50,043 [l2p_self_training.py] => previous label on future samples count: 1017
2024-02-02 21:52:50,043 [l2p_self_training.py] => it label on future sample count: 2
2024-02-02 21:52:50,043 [l2p_self_training.py] => previous label on it sample count: 872
2024-02-02 21:52:50,043 [l2p_self_training.py] => 2126 unlabeled samples will be pseudo labeled
2024-02-02 21:52:50,043 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:52:50,043 [toolkit.py] => Pseudo Accuracy: 0.11006585136406397
2024-02-02 21:52:50,048 [l2p_self_training.py] => train dataset length: 2376
Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.205, Train_accy 84.26, Test_accy 48.83: 100%|███████████████| 5/5 [01:07<00:00, 13.44s/it]
2024-02-02 21:53:57,247 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.205, Train_accy 84.26, Test_accy 48.83
2024-02-02 21:53:57,254 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:54:10,525 [l2p_self_training.py] => wrong labeled samples count: 1596
2024-02-02 21:54:10,530 [l2p_self_training.py] => previous label on future samples count: 985
2024-02-02 21:54:10,531 [l2p_self_training.py] => it label on future sample count: 8
2024-02-02 21:54:10,531 [l2p_self_training.py] => previous label on it sample count: 602
2024-02-02 21:54:10,531 [l2p_self_training.py] => 2025 unlabeled samples will be pseudo labeled
2024-02-02 21:54:10,531 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:54:10,531 [toolkit.py] => Pseudo Accuracy: 0.21185185185185185
2024-02-02 21:54:20,164 [trainer.py] => No NME accuracy.
2024-02-02 21:54:20,165 [trainer.py] => CNN: {'total': 48.83, '00-09': 72.2, '10-19': 32.8, '20-29': 41.5, 'old': 52.5, 'new': 41.5}
2024-02-02 21:54:20,165 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83]
2024-02-02 21:54:20,165 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2]

Average Accuracy (CNN): 69.87666666666667
2024-02-02 21:54:20,166 [trainer.py] => Average Accuracy (CNN): 69.87666666666667 

2024-02-02 21:54:20,170 [trainer.py] => All params: 171816392
2024-02-02 21:54:20,173 [trainer.py] => Trainable params: 122980
2024-02-02 21:54:20,174 [l2p_self_training.py] => Learning on 30-40
4000
2024-02-02 21:54:20,231 [l2p_self_training.py] => train dataset length: 250
Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.079, Train_accy 94.00, Test_accy 44.22: 100%|██████████████| 5/5 [00:20<00:00,  4.10s/it]
2024-02-02 21:54:40,756 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.079, Train_accy 94.00, Test_accy 44.22
2024-02-02 21:54:40,763 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:54:53,305 [l2p_self_training.py] => wrong labeled samples count: 1616
2024-02-02 21:54:53,310 [l2p_self_training.py] => previous label on future samples count: 835
2024-02-02 21:54:53,310 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 21:54:53,310 [l2p_self_training.py] => previous label on it sample count: 781
2024-02-02 21:54:53,310 [l2p_self_training.py] => 1788 unlabeled samples will be pseudo labeled
2024-02-02 21:54:53,310 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:54:53,310 [toolkit.py] => Pseudo Accuracy: 0.09619686800894854
2024-02-02 21:54:53,314 [l2p_self_training.py] => train dataset length: 2038
Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.252, Train_accy 82.78, Test_accy 36.95: 100%|███████████████| 5/5 [01:02<00:00, 12.49s/it]
2024-02-02 21:55:55,753 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.252, Train_accy 82.78, Test_accy 36.95
2024-02-02 21:55:55,759 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:56:08,317 [l2p_self_training.py] => wrong labeled samples count: 1809
2024-02-02 21:56:08,322 [l2p_self_training.py] => previous label on future samples count: 989
2024-02-02 21:56:08,322 [l2p_self_training.py] => it label on future sample count: 1
2024-02-02 21:56:08,323 [l2p_self_training.py] => previous label on it sample count: 816
2024-02-02 21:56:08,323 [l2p_self_training.py] => 2237 unlabeled samples will be pseudo labeled
2024-02-02 21:56:08,323 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:56:08,323 [toolkit.py] => Pseudo Accuracy: 0.19132767098793027
2024-02-02 21:56:08,328 [l2p_self_training.py] => train dataset length: 2487
Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.132, Train_accy 85.97, Test_accy 34.83: 100%|███████████████| 5/5 [01:12<00:00, 14.54s/it]
2024-02-02 21:57:21,012 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.132, Train_accy 85.97, Test_accy 34.83
2024-02-02 21:57:21,019 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:57:33,593 [l2p_self_training.py] => wrong labeled samples count: 1640
2024-02-02 21:57:33,599 [l2p_self_training.py] => previous label on future samples count: 943
2024-02-02 21:57:33,599 [l2p_self_training.py] => it label on future sample count: 2
2024-02-02 21:57:33,599 [l2p_self_training.py] => previous label on it sample count: 688
2024-02-02 21:57:33,599 [l2p_self_training.py] => 2186 unlabeled samples will be pseudo labeled
2024-02-02 21:57:33,600 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:57:33,600 [toolkit.py] => Pseudo Accuracy: 0.24977127172918573
2024-02-02 21:57:46,192 [trainer.py] => No NME accuracy.
2024-02-02 21:57:46,193 [trainer.py] => CNN: {'total': 34.83, '00-09': 60.3, '10-19': 26.7, '20-29': 9.2, '30-39': 43.1, 'old': 32.07, 'new': 43.1}
2024-02-02 21:57:46,193 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83]
2024-02-02 21:57:46,193 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08]

Average Accuracy (CNN): 61.114999999999995
2024-02-02 21:57:46,193 [trainer.py] => Average Accuracy (CNN): 61.114999999999995 

2024-02-02 21:57:46,194 [trainer.py] => All params: 171816392
2024-02-02 21:57:46,196 [trainer.py] => Trainable params: 122980
2024-02-02 21:57:46,196 [l2p_self_training.py] => Learning on 40-50
3750
2024-02-02 21:57:46,251 [l2p_self_training.py] => train dataset length: 250
Task 4, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.107, Train_accy 92.00, Test_accy 31.70: 100%|██████████████| 5/5 [00:23<00:00,  4.74s/it]
2024-02-02 21:58:09,965 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.107, Train_accy 92.00, Test_accy 31.70
2024-02-02 21:58:09,971 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:58:21,791 [l2p_self_training.py] => wrong labeled samples count: 2007
2024-02-02 21:58:21,799 [l2p_self_training.py] => previous label on future samples count: 800
2024-02-02 21:58:21,799 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 21:58:21,799 [l2p_self_training.py] => previous label on it sample count: 1207
2024-02-02 21:58:21,799 [l2p_self_training.py] => 2065 unlabeled samples will be pseudo labeled
2024-02-02 21:58:21,799 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:58:21,800 [toolkit.py] => Pseudo Accuracy: 0.028087167070217918
2024-02-02 21:58:21,805 [l2p_self_training.py] => train dataset length: 2315
Task 4, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.209, Train_accy 83.97, Test_accy 26.08: 100%|███████████████| 5/5 [01:12<00:00, 14.44s/it]
2024-02-02 21:59:34,019 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.209, Train_accy 83.97, Test_accy 26.08
2024-02-02 21:59:34,025 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:59:45,807 [l2p_self_training.py] => wrong labeled samples count: 1969
2024-02-02 21:59:45,814 [l2p_self_training.py] => previous label on future samples count: 790
2024-02-02 21:59:45,814 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 21:59:45,814 [l2p_self_training.py] => previous label on it sample count: 1179
2024-02-02 21:59:45,815 [l2p_self_training.py] => 2098 unlabeled samples will be pseudo labeled
2024-02-02 21:59:45,815 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:59:45,815 [toolkit.py] => Pseudo Accuracy: 0.061487130600571975
2024-02-02 21:59:45,821 [l2p_self_training.py] => train dataset length: 2348
Task 4, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.112, Train_accy 85.60, Test_accy 23.08: 100%|███████████████| 5/5 [01:12<00:00, 14.56s/it]
2024-02-02 22:00:58,598 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.112, Train_accy 85.60, Test_accy 23.08
2024-02-02 22:00:58,600 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:01:10,408 [l2p_self_training.py] => wrong labeled samples count: 2027
2024-02-02 22:01:10,415 [l2p_self_training.py] => previous label on future samples count: 870
2024-02-02 22:01:10,415 [l2p_self_training.py] => it label on future sample count: 2
2024-02-02 22:01:10,415 [l2p_self_training.py] => previous label on it sample count: 1154
2024-02-02 22:01:10,415 [l2p_self_training.py] => 2208 unlabeled samples will be pseudo labeled
2024-02-02 22:01:10,415 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:01:10,416 [toolkit.py] => Pseudo Accuracy: 0.08197463768115942
2024-02-02 22:01:26,044 [trainer.py] => No NME accuracy.
2024-02-02 22:01:26,045 [trainer.py] => CNN: {'total': 23.08, '00-09': 56.7, '10-19': 16.8, '20-29': 5.9, '30-39': 13.2, '40-49': 22.8, 'old': 23.15, 'new': 22.8}
2024-02-02 22:01:26,045 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08]
2024-02-02 22:01:26,045 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2]

Average Accuracy (CNN): 53.507999999999996
2024-02-02 22:01:26,045 [trainer.py] => Average Accuracy (CNN): 53.507999999999996 

2024-02-02 22:01:26,049 [trainer.py] => All params: 171816392
2024-02-02 22:01:26,052 [trainer.py] => Trainable params: 122980
2024-02-02 22:01:26,052 [l2p_self_training.py] => Learning on 50-60
3500
2024-02-02 22:01:26,200 [l2p_self_training.py] => train dataset length: 250
Task 5, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.063, Train_accy 89.20, Test_accy 22.25: 100%|██████████████| 5/5 [00:26<00:00,  5.29s/it]
2024-02-02 22:01:52,634 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.063, Train_accy 89.20, Test_accy 22.25
2024-02-02 22:01:52,636 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:02:03,672 [l2p_self_training.py] => wrong labeled samples count: 2022
2024-02-02 22:02:03,679 [l2p_self_training.py] => previous label on future samples count: 658
2024-02-02 22:02:03,679 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:02:03,680 [l2p_self_training.py] => previous label on it sample count: 1364
2024-02-02 22:02:03,680 [l2p_self_training.py] => 2033 unlabeled samples will be pseudo labeled
2024-02-02 22:02:03,680 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:02:03,680 [toolkit.py] => Pseudo Accuracy: 0.0054107230693556324
2024-02-02 22:02:03,685 [l2p_self_training.py] => train dataset length: 2283
Task 5, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.153, Train_accy 84.23, Test_accy 14.87: 100%|███████████████| 5/5 [01:13<00:00, 14.79s/it]
2024-02-02 22:03:17,649 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.153, Train_accy 84.23, Test_accy 14.87
2024-02-02 22:03:17,654 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:03:28,781 [l2p_self_training.py] => wrong labeled samples count: 1830
2024-02-02 22:03:28,788 [l2p_self_training.py] => previous label on future samples count: 630
2024-02-02 22:03:28,788 [l2p_self_training.py] => it label on future sample count: 3
2024-02-02 22:03:28,788 [l2p_self_training.py] => previous label on it sample count: 1197
2024-02-02 22:03:28,788 [l2p_self_training.py] => 1852 unlabeled samples will be pseudo labeled
2024-02-02 22:03:28,788 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:03:28,789 [toolkit.py] => Pseudo Accuracy: 0.011879049676025918
2024-02-02 22:03:28,793 [l2p_self_training.py] => train dataset length: 2102
Task 5, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.049, Train_accy 88.34, Test_accy 13.82: 100%|███████████████| 5/5 [01:09<00:00, 13.99s/it]
2024-02-02 22:04:38,755 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.049, Train_accy 88.34, Test_accy 13.82
2024-02-02 22:04:38,761 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:04:49,790 [l2p_self_training.py] => wrong labeled samples count: 1966
2024-02-02 22:04:49,796 [l2p_self_training.py] => previous label on future samples count: 675
2024-02-02 22:04:49,797 [l2p_self_training.py] => it label on future sample count: 4
2024-02-02 22:04:49,797 [l2p_self_training.py] => previous label on it sample count: 1285
2024-02-02 22:04:49,797 [l2p_self_training.py] => 2056 unlabeled samples will be pseudo labeled
2024-02-02 22:04:49,797 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:04:49,797 [toolkit.py] => Pseudo Accuracy: 0.04377431906614786
2024-02-02 22:05:08,453 [trainer.py] => No NME accuracy.
2024-02-02 22:05:08,454 [trainer.py] => CNN: {'total': 13.82, '00-09': 49.9, '10-19': 13.3, '20-29': 0.3, '30-39': 0.2, '40-49': 1.5, '50-59': 17.7, 'old': 13.04, 'new': 17.7}
2024-02-02 22:05:08,454 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08, 13.82]
2024-02-02 22:05:08,454 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2, 33.65]

Average Accuracy (CNN): 46.893333333333324
2024-02-02 22:05:08,454 [trainer.py] => Average Accuracy (CNN): 46.893333333333324 

2024-02-02 22:05:08,457 [trainer.py] => All params: 171816392
2024-02-02 22:05:08,460 [trainer.py] => Trainable params: 122980
2024-02-02 22:05:08,460 [l2p_self_training.py] => Learning on 60-70
3250
2024-02-02 22:05:08,517 [l2p_self_training.py] => train dataset length: 250
Task 6, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.108, Train_accy 93.20, Test_accy 15.76: 100%|██████████████| 5/5 [00:29<00:00,  5.95s/it]
2024-02-02 22:05:38,248 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.108, Train_accy 93.20, Test_accy 15.76
2024-02-02 22:05:38,254 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:05:48,485 [l2p_self_training.py] => wrong labeled samples count: 1450
2024-02-02 22:05:48,486 [l2p_self_training.py] => previous label on future samples count: 458
2024-02-02 22:05:48,486 [l2p_self_training.py] => it label on future sample count: 1
2024-02-02 22:05:48,486 [l2p_self_training.py] => previous label on it sample count: 991
2024-02-02 22:05:48,486 [l2p_self_training.py] => 1520 unlabeled samples will be pseudo labeled
2024-02-02 22:05:48,486 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:05:48,486 [toolkit.py] => Pseudo Accuracy: 0.046052631578947366
2024-02-02 22:05:48,488 [l2p_self_training.py] => train dataset length: 1770
Task 6, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.209, Train_accy 83.11, Test_accy 13.61: 100%|███████████████| 5/5 [01:05<00:00, 13.06s/it]
2024-02-02 22:06:53,790 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.209, Train_accy 83.11, Test_accy 13.61
2024-02-02 22:06:53,796 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:07:04,089 [l2p_self_training.py] => wrong labeled samples count: 1326
2024-02-02 22:07:04,094 [l2p_self_training.py] => previous label on future samples count: 427
2024-02-02 22:07:04,094 [l2p_self_training.py] => it label on future sample count: 3
2024-02-02 22:07:04,094 [l2p_self_training.py] => previous label on it sample count: 895
2024-02-02 22:07:04,094 [l2p_self_training.py] => 1532 unlabeled samples will be pseudo labeled
2024-02-02 22:07:04,094 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:07:04,095 [toolkit.py] => Pseudo Accuracy: 0.13446475195822455
2024-02-02 22:07:04,099 [l2p_self_training.py] => train dataset length: 1782
Task 6, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.041, Train_accy 87.60, Test_accy 14.30: 100%|███████████████| 5/5 [01:05<00:00, 13.06s/it]
2024-02-02 22:08:09,423 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.041, Train_accy 87.60, Test_accy 14.30
2024-02-02 22:08:09,429 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:08:19,689 [l2p_self_training.py] => wrong labeled samples count: 1143
2024-02-02 22:08:19,690 [l2p_self_training.py] => previous label on future samples count: 466
2024-02-02 22:08:19,690 [l2p_self_training.py] => it label on future sample count: 8
2024-02-02 22:08:19,690 [l2p_self_training.py] => previous label on it sample count: 666
2024-02-02 22:08:19,690 [l2p_self_training.py] => 1583 unlabeled samples will be pseudo labeled
2024-02-02 22:08:19,690 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:08:19,690 [toolkit.py] => Pseudo Accuracy: 0.2779532533164877
2024-02-02 22:08:41,356 [trainer.py] => No NME accuracy.
2024-02-02 22:08:41,357 [trainer.py] => CNN: {'total': 14.3, '00-09': 40.6, '10-19': 10.3, '20-29': 0.0, '30-39': 0.0, '40-49': 0.0, '50-59': 3.3, '60-69': 45.9, 'old': 9.03, 'new': 45.9}
2024-02-02 22:08:41,357 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08, 13.82, 14.3]
2024-02-02 22:08:41,357 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2, 33.65, 29.63]

Average Accuracy (CNN): 42.23714285714285
2024-02-02 22:08:41,357 [trainer.py] => Average Accuracy (CNN): 42.23714285714285 

2024-02-02 22:08:41,361 [trainer.py] => All params: 171816392
2024-02-02 22:08:41,363 [trainer.py] => Trainable params: 122980
2024-02-02 22:08:41,363 [l2p_self_training.py] => Learning on 70-80
3000
2024-02-02 22:08:41,409 [l2p_self_training.py] => train dataset length: 250
Task 7, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.062, Train_accy 92.40, Test_accy 14.75: 100%|██████████████| 5/5 [00:32<00:00,  6.53s/it]
2024-02-02 22:09:14,059 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.062, Train_accy 92.40, Test_accy 14.75
2024-02-02 22:09:14,066 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:09:23,629 [l2p_self_training.py] => wrong labeled samples count: 1433
2024-02-02 22:09:23,635 [l2p_self_training.py] => previous label on future samples count: 301
2024-02-02 22:09:23,635 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:09:23,635 [l2p_self_training.py] => previous label on it sample count: 1132
2024-02-02 22:09:23,635 [l2p_self_training.py] => 1445 unlabeled samples will be pseudo labeled
2024-02-02 22:09:23,635 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:09:23,636 [toolkit.py] => Pseudo Accuracy: 0.008304498269896194
2024-02-02 22:09:23,640 [l2p_self_training.py] => train dataset length: 1695
Task 7, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.202, Train_accy 82.18, Test_accy 13.50: 100%|███████████████| 5/5 [01:06<00:00, 13.27s/it]
2024-02-02 22:10:29,968 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.202, Train_accy 82.18, Test_accy 13.50
2024-02-02 22:10:29,975 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:10:39,595 [l2p_self_training.py] => wrong labeled samples count: 1435
2024-02-02 22:10:39,596 [l2p_self_training.py] => previous label on future samples count: 314
2024-02-02 22:10:39,596 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:10:39,597 [l2p_self_training.py] => previous label on it sample count: 1121
2024-02-02 22:10:39,597 [l2p_self_training.py] => 1470 unlabeled samples will be pseudo labeled
2024-02-02 22:10:39,597 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:10:39,597 [toolkit.py] => Pseudo Accuracy: 0.023809523809523808
2024-02-02 22:10:39,599 [l2p_self_training.py] => train dataset length: 1720
Task 7, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.092, Train_accy 86.45, Test_accy 12.50: 100%|███████████████| 5/5 [01:07<00:00, 13.41s/it]
2024-02-02 22:11:46,659 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.092, Train_accy 86.45, Test_accy 12.50
2024-02-02 22:11:46,666 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:11:56,263 [l2p_self_training.py] => wrong labeled samples count: 1528
2024-02-02 22:11:56,268 [l2p_self_training.py] => previous label on future samples count: 340
2024-02-02 22:11:56,269 [l2p_self_training.py] => it label on future sample count: 2
2024-02-02 22:11:56,269 [l2p_self_training.py] => previous label on it sample count: 1182
2024-02-02 22:11:56,269 [l2p_self_training.py] => 1613 unlabeled samples will be pseudo labeled
2024-02-02 22:11:56,269 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:11:56,269 [toolkit.py] => Pseudo Accuracy: 0.05269683818970862
2024-02-02 22:12:21,195 [trainer.py] => No NME accuracy.
2024-02-02 22:12:21,196 [trainer.py] => CNN: {'total': 12.5, '00-09': 49.4, '10-19': 17.8, '20-29': 0.0, '30-39': 0.0, '40-49': 0.0, '50-59': 1.2, '60-69': 13.2, '70-79': 18.4, 'old': 11.66, 'new': 18.4}
2024-02-02 22:12:21,196 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08, 13.82, 14.3, 12.5]
2024-02-02 22:12:21,196 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2, 33.65, 29.63, 31.32]

Average Accuracy (CNN): 38.519999999999996
2024-02-02 22:12:21,196 [trainer.py] => Average Accuracy (CNN): 38.519999999999996 

2024-02-02 22:12:21,199 [trainer.py] => All params: 171816392
2024-02-02 22:12:21,201 [trainer.py] => Trainable params: 122980
2024-02-02 22:12:21,201 [l2p_self_training.py] => Learning on 80-90
2750
2024-02-02 22:12:21,247 [l2p_self_training.py] => train dataset length: 250
Task 8, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.075, Train_accy 90.40, Test_accy 12.74: 100%|██████████████| 5/5 [00:35<00:00,  7.14s/it]
2024-02-02 22:12:56,973 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.075, Train_accy 90.40, Test_accy 12.74
2024-02-02 22:12:56,980 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:13:05,761 [l2p_self_training.py] => wrong labeled samples count: 1498
2024-02-02 22:13:05,767 [l2p_self_training.py] => previous label on future samples count: 163
2024-02-02 22:13:05,767 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:13:05,767 [l2p_self_training.py] => previous label on it sample count: 1335
2024-02-02 22:13:05,767 [l2p_self_training.py] => 1515 unlabeled samples will be pseudo labeled
2024-02-02 22:13:05,767 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:13:05,767 [toolkit.py] => Pseudo Accuracy: 0.011221122112211221
2024-02-02 22:13:05,771 [l2p_self_training.py] => train dataset length: 1765
Task 8, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.166, Train_accy 83.46, Test_accy 8.77: 100%|████████████████| 5/5 [01:11<00:00, 14.29s/it]
2024-02-02 22:14:17,210 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.166, Train_accy 83.46, Test_accy 8.77
2024-02-02 22:14:17,217 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:14:25,972 [l2p_self_training.py] => wrong labeled samples count: 1465
2024-02-02 22:14:25,978 [l2p_self_training.py] => previous label on future samples count: 155
2024-02-02 22:14:25,978 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:14:25,978 [l2p_self_training.py] => previous label on it sample count: 1310
2024-02-02 22:14:25,978 [l2p_self_training.py] => 1532 unlabeled samples will be pseudo labeled
2024-02-02 22:14:25,978 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:14:25,979 [toolkit.py] => Pseudo Accuracy: 0.043733681462140996
2024-02-02 22:14:25,985 [l2p_self_training.py] => train dataset length: 1782
Task 8, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.017, Train_accy 88.95, Test_accy 7.89: 100%|████████████████| 5/5 [01:11<00:00, 14.37s/it]
2024-02-02 22:15:37,836 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.017, Train_accy 88.95, Test_accy 7.89
2024-02-02 22:15:37,843 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:15:46,644 [l2p_self_training.py] => wrong labeled samples count: 1418
2024-02-02 22:15:46,649 [l2p_self_training.py] => previous label on future samples count: 173
2024-02-02 22:15:46,649 [l2p_self_training.py] => it label on future sample count: 1
2024-02-02 22:15:46,649 [l2p_self_training.py] => previous label on it sample count: 1243
2024-02-02 22:15:46,650 [l2p_self_training.py] => 1565 unlabeled samples will be pseudo labeled
2024-02-02 22:15:46,650 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:15:46,650 [toolkit.py] => Pseudo Accuracy: 0.0939297124600639
2024-02-02 22:16:14,540 [trainer.py] => No NME accuracy.
2024-02-02 22:16:14,541 [trainer.py] => CNN: {'total': 7.89, '00-09': 42.0, '10-19': 6.1, '20-29': 0.0, '30-39': 0.0, '40-49': 0.0, '50-59': 0.0, '60-69': 0.1, '70-79': 0.8, '80-89': 22.0, 'old': 6.12, 'new': 22.0}
2024-02-02 22:16:14,541 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08, 13.82, 14.3, 12.5, 7.89]
2024-02-02 22:16:14,541 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2, 33.65, 29.63, 31.32, 23.84]

Average Accuracy (CNN): 35.11666666666666
2024-02-02 22:16:14,541 [trainer.py] => Average Accuracy (CNN): 35.11666666666666 

2024-02-02 22:16:14,544 [trainer.py] => All params: 171816392
2024-02-02 22:16:14,547 [trainer.py] => Trainable params: 122980
2024-02-02 22:16:14,547 [l2p_self_training.py] => Learning on 90-100
2500
2024-02-02 22:16:14,613 [l2p_self_training.py] => train dataset length: 250
Task 9, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.063, Train_accy 90.40, Test_accy 8.76: 100%|███████████████| 5/5 [00:38<00:00,  7.75s/it]
2024-02-02 22:16:53,388 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.063, Train_accy 90.40, Test_accy 8.76
2024-02-02 22:16:53,395 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:17:01,483 [l2p_self_training.py] => wrong labeled samples count: 1403
2024-02-02 22:17:01,488 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 22:17:01,488 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:17:01,488 [l2p_self_training.py] => previous label on it sample count: 1403
2024-02-02 22:17:01,489 [l2p_self_training.py] => 1422 unlabeled samples will be pseudo labeled
2024-02-02 22:17:01,489 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:17:01,489 [toolkit.py] => Pseudo Accuracy: 0.013361462728551337
2024-02-02 22:17:01,495 [l2p_self_training.py] => train dataset length: 1672
Task 9, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.202, Train_accy 81.70, Test_accy 6.75: 100%|████████████████| 5/5 [01:11<00:00, 14.36s/it]
2024-02-02 22:18:13,301 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.202, Train_accy 81.70, Test_accy 6.75
2024-02-02 22:18:13,307 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:18:21,266 [l2p_self_training.py] => wrong labeled samples count: 1055
2024-02-02 22:18:21,270 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 22:18:21,270 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:18:21,270 [l2p_self_training.py] => previous label on it sample count: 1052
2024-02-02 22:18:21,271 [l2p_self_training.py] => 1165 unlabeled samples will be pseudo labeled
2024-02-02 22:18:21,271 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:18:21,271 [toolkit.py] => Pseudo Accuracy: 0.0944206008583691
2024-02-02 22:18:21,275 [l2p_self_training.py] => train dataset length: 1415
Task 9, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.066, Train_accy 86.22, Test_accy 5.61: 100%|████████████████| 5/5 [01:05<00:00, 13.19s/it]
2024-02-02 22:19:27,206 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.066, Train_accy 86.22, Test_accy 5.61
2024-02-02 22:19:27,212 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:19:35,191 [l2p_self_training.py] => wrong labeled samples count: 972
2024-02-02 22:19:35,195 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 22:19:35,195 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:19:35,195 [l2p_self_training.py] => previous label on it sample count: 971
2024-02-02 22:19:35,195 [l2p_self_training.py] => 1142 unlabeled samples will be pseudo labeled
2024-02-02 22:19:35,195 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:19:35,196 [toolkit.py] => Pseudo Accuracy: 0.14886164623467601
2024-02-02 22:20:06,150 [trainer.py] => No NME accuracy.
2024-02-02 22:20:06,150 [trainer.py] => CNN: {'total': 5.61, '00-09': 30.5, '10-19': 1.6, '20-29': 0.0, '30-39': 0.0, '40-49': 0.0, '50-59': 0.0, '60-69': 0.0, '70-79': 0.0, '80-89': 0.5, '90-99': 23.5, 'old': 3.62, 'new': 23.5}
2024-02-02 22:20:06,150 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08, 13.82, 14.3, 12.5, 7.89, 5.61]
2024-02-02 22:20:06,151 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2, 33.65, 29.63, 31.32, 23.84, 18.08]

Average Accuracy (CNN): 32.166
2024-02-02 22:20:06,151 [trainer.py] => Average Accuracy (CNN): 32.166 

Accuracy Matrix (CNN):
[[94.2 72.7 72.2 60.3 56.7 49.9 40.6 49.4 42.  30.5]
 [ 0.  60.5 32.8 26.7 16.8 13.3 10.3 17.8  6.1  1.6]
 [ 0.   0.  41.5  9.2  5.9  0.3  0.   0.   0.   0. ]
 [ 0.   0.   0.  43.1 13.2  0.2  0.   0.   0.   0. ]
 [ 0.   0.   0.   0.  22.8  1.5  0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.  17.7  3.3  1.2  0.   0. ]
 [ 0.   0.   0.   0.   0.   0.  45.9 13.2  0.1  0. ]
 [ 0.   0.   0.   0.   0.   0.   0.  18.4  0.8  0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.  22.   0.5]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.  23.5]]
2024-02-02 22:20:06,153 [trainer.py] => Forgetting (CNN): 37.05555555555556