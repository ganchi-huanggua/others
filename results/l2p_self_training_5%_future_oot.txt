######################################################################################
======================================================================================
######################################################################################
原l2p+Self-Training方法在有future oot数据时伪标签打的对错（分类头：known classes:total:classes）

(base) root@autodl-container-56644e8033-177721e5:~/autodl-tmp/LAMDA-PILOT# python main.py --config=./exps/l2p_self_training.json
2024-01-29 22:29:49,179 [trainer.py] => config: ./exps/l2p_self_training.json
2024-01-29 22:29:49,179 [trainer.py] => prefix:  
2024-01-29 22:29:49,179 [trainer.py] => dataset: cifar224
2024-01-29 22:29:49,179 [trainer.py] => memory_size: 0
2024-01-29 22:29:49,179 [trainer.py] => memory_per_class: 0
2024-01-29 22:29:49,179 [trainer.py] => fixed_memory: False
2024-01-29 22:29:49,179 [trainer.py] => shuffle: True
2024-01-29 22:29:49,179 [trainer.py] => init_cls: 10
2024-01-29 22:29:49,179 [trainer.py] => increment: 10
2024-01-29 22:29:49,179 [trainer.py] => model_name: l2p_self_training
2024-01-29 22:29:49,179 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2024-01-29 22:29:49,179 [trainer.py] => get_original_backbone: True
2024-01-29 22:29:49,179 [trainer.py] => device: [device(type='cuda', index=0)]
2024-01-29 22:29:49,179 [trainer.py] => seed: 1993
2024-01-29 22:29:49,179 [trainer.py] => tuned_epoch: 5
2024-01-29 22:29:49,179 [trainer.py] => init_lr: 0.001875
2024-01-29 22:29:49,179 [trainer.py] => batch_size: 16
2024-01-29 22:29:49,180 [trainer.py] => weight_decay: 0
2024-01-29 22:29:49,180 [trainer.py] => min_lr: 1e-05
2024-01-29 22:29:49,180 [trainer.py] => optimizer: adam
2024-01-29 22:29:49,180 [trainer.py] => scheduler: constant
2024-01-29 22:29:49,180 [trainer.py] => reinit_optimizer: True
2024-01-29 22:29:49,180 [trainer.py] => global_pool: token
2024-01-29 22:29:49,180 [trainer.py] => head_type: prompt
2024-01-29 22:29:49,180 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2024-01-29 22:29:49,180 [trainer.py] => pretrained: True
2024-01-29 22:29:49,180 [trainer.py] => drop: 0.0
2024-01-29 22:29:49,180 [trainer.py] => drop_path: 0.0
2024-01-29 22:29:49,180 [trainer.py] => prompt_pool: True
2024-01-29 22:29:49,180 [trainer.py] => pool_size: 10
2024-01-29 22:29:49,180 [trainer.py] => length: 5
2024-01-29 22:29:49,180 [trainer.py] => top_k: 5
2024-01-29 22:29:49,180 [trainer.py] => initializer: uniform
2024-01-29 22:29:49,180 [trainer.py] => prompt_key: True
2024-01-29 22:29:49,180 [trainer.py] => prompt_key_init: uniform
2024-01-29 22:29:49,180 [trainer.py] => use_prompt_mask: False
2024-01-29 22:29:49,180 [trainer.py] => shared_prompt_pool: False
2024-01-29 22:29:49,180 [trainer.py] => shared_prompt_key: False
2024-01-29 22:29:49,180 [trainer.py] => batchwise_prompt: True
2024-01-29 22:29:49,180 [trainer.py] => embedding_key: cls
2024-01-29 22:29:49,180 [trainer.py] => predefined_key: 
2024-01-29 22:29:49,180 [trainer.py] => pull_constraint: True
2024-01-29 22:29:49,180 [trainer.py] => pull_constraint_coeff: 0.1
2024-01-29 22:29:49,180 [trainer.py] => semi_supervised_mode: True
2024-01-29 22:29:49,180 [trainer.py] => labeled_ratio: 0.05
2024-01-29 22:29:49,180 [trainer.py] => unlabeled_data_distribution_mode: future_oot
2024-01-29 22:29:49,180 [trainer.py] => confidence_threshold: 0.9
2024-01-29 22:29:49,180 [trainer.py] => max_self_training_iteration: 3
Files already downloaded and verified
Files already downloaded and verified
2024-01-29 22:29:50,728 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-01-29 22:29:52,255 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2024-01-29 22:29:52,256 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2024-01-29 22:29:54,598 [l2p_self_training.py] => 85,940,836 model total parameters.
2024-01-29 22:29:54,598 [l2p_self_training.py] => 122,980 model training parameters.
2024-01-29 22:29:54,598 [l2p_self_training.py] => prompt.prompt: 38400
2024-01-29 22:29:54,598 [l2p_self_training.py] => prompt.prompt_key: 7680
2024-01-29 22:29:54,599 [l2p_self_training.py] => head.weight: 76800
2024-01-29 22:29:54,599 [l2p_self_training.py] => head.bias: 100
2024-01-29 22:29:54,600 [trainer.py] => All params: 171816392
2024-01-29 22:29:54,601 [trainer.py] => Trainable params: 122980
2024-01-29 22:29:54,601 [l2p_self_training.py] => Learning on 0-10
4750
2024-01-29 22:29:54,891 [l2p_self_training.py] => train dataset length: 250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.515, Train_accy 84.00, Test_accy 89.50: 100%|███████████████| 5/5 [00:11<00:00,  2.26s/it]
2024-01-29 22:30:06,205 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.515, Train_accy 84.00, Test_accy 89.50
2024-01-29 22:30:06,212 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:30:20,698 [l2p_self_training.py] => wrong labeled samples count: 249
2024-01-29 22:30:20,699 [l2p_self_training.py] => it label on oot samples count: 233
2024-01-29 22:30:20,699 [l2p_self_training.py] => 1274 unlabeled samples will be pseudo labeled
2024-01-29 22:30:20,699 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:30:20,699 [toolkit.py] => Pseudo Accuracy: 0.804552590266876
2024-01-29 22:30:20,702 [l2p_self_training.py] => train dataset length: 1524
Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.150, Train_accy 88.39, Test_accy 93.00: 100%|███████████████| 5/5 [00:40<00:00,  8.07s/it]
2024-01-29 22:31:01,065 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.150, Train_accy 88.39, Test_accy 93.00
2024-01-29 22:31:01,072 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:31:15,787 [l2p_self_training.py] => wrong labeled samples count: 792
2024-01-29 22:31:15,788 [l2p_self_training.py] => it label on oot samples count: 731
2024-01-29 22:31:15,788 [l2p_self_training.py] => 2460 unlabeled samples will be pseudo labeled
2024-01-29 22:31:15,788 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:31:15,788 [toolkit.py] => Pseudo Accuracy: 0.6780487804878049
2024-01-29 22:31:15,798 [l2p_self_training.py] => train dataset length: 2710
Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.077, Train_accy 87.64, Test_accy 94.20: 100%|███████████████| 5/5 [01:08<00:00, 13.61s/it]
2024-01-29 22:32:23,841 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.077, Train_accy 87.64, Test_accy 94.20
2024-01-29 22:32:23,847 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:32:38,487 [l2p_self_training.py] => wrong labeled samples count: 1123
2024-01-29 22:32:38,488 [l2p_self_training.py] => it label on oot samples count: 1055
2024-01-29 22:32:38,488 [l2p_self_training.py] => 2922 unlabeled samples will be pseudo labeled
2024-01-29 22:32:38,488 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:32:38,488 [toolkit.py] => Pseudo Accuracy: 0.6156741957563313
2024-01-29 22:32:42,010 [trainer.py] => No NME accuracy.
2024-01-29 22:32:42,010 [trainer.py] => CNN: {'total': 94.2, '00-09': 94.2, 'old': 0, 'new': 94.2}
2024-01-29 22:32:42,011 [trainer.py] => CNN top1 curve: [94.2]
2024-01-29 22:32:42,011 [trainer.py] => CNN top5 curve: [99.7]

Average Accuracy (CNN): 94.2
2024-01-29 22:32:42,011 [trainer.py] => Average Accuracy (CNN): 94.2 

2024-01-29 22:32:42,015 [trainer.py] => All params: 171816392
2024-01-29 22:32:42,018 [trainer.py] => Trainable params: 122980
2024-01-29 22:32:42,018 [l2p_self_training.py] => Learning on 10-20
4500
2024-01-29 22:32:42,070 [l2p_self_training.py] => train dataset length: 250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.028, Train_accy 91.60, Test_accy 74.35: 100%|██████████████| 5/5 [00:15<00:00,  3.01s/it]
2024-01-29 22:32:57,142 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.028, Train_accy 91.60, Test_accy 74.35
2024-01-29 22:32:57,148 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:33:11,152 [l2p_self_training.py] => wrong labeled samples count: 363
2024-01-29 22:33:11,153 [l2p_self_training.py] => it label on oot samples count: 329
2024-01-29 22:33:11,153 [l2p_self_training.py] => 1837 unlabeled samples will be pseudo labeled
2024-01-29 22:33:11,153 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:33:11,153 [toolkit.py] => Pseudo Accuracy: 0.8023952095808383
2024-01-29 22:33:11,160 [l2p_self_training.py] => train dataset length: 2087
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.024, Train_accy 90.42, Test_accy 86.80: 100%|██████████████| 5/5 [00:56<00:00, 11.35s/it]
2024-01-29 22:34:07,901 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.024, Train_accy 90.42, Test_accy 86.80
2024-01-29 22:34:07,907 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:34:21,736 [l2p_self_training.py] => wrong labeled samples count: 807
2024-01-29 22:34:21,737 [l2p_self_training.py] => it label on oot samples count: 746
2024-01-29 22:34:21,738 [l2p_self_training.py] => 2553 unlabeled samples will be pseudo labeled
2024-01-29 22:34:21,738 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:34:21,738 [toolkit.py] => Pseudo Accuracy: 0.6839012925969448
2024-01-29 22:34:21,745 [l2p_self_training.py] => train dataset length: 2803
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.072, Train_accy 87.62, Test_accy 85.05: 100%|███████████████| 5/5 [01:13<00:00, 14.61s/it]
2024-01-29 22:35:34,796 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.072, Train_accy 87.62, Test_accy 85.05
2024-01-29 22:35:34,802 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:35:48,758 [l2p_self_training.py] => wrong labeled samples count: 943
2024-01-29 22:35:48,759 [l2p_self_training.py] => it label on oot samples count: 870
2024-01-29 22:35:48,759 [l2p_self_training.py] => 2665 unlabeled samples will be pseudo labeled
2024-01-29 22:35:48,759 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:35:48,760 [toolkit.py] => Pseudo Accuracy: 0.6461538461538462
2024-01-29 22:35:55,228 [trainer.py] => No NME accuracy.
2024-01-29 22:35:55,228 [trainer.py] => CNN: {'total': 85.05, '00-09': 84.5, '10-19': 85.6, 'old': 84.5, 'new': 85.6}
2024-01-29 22:35:55,229 [trainer.py] => CNN top1 curve: [94.2, 85.05]
2024-01-29 22:35:55,229 [trainer.py] => CNN top5 curve: [99.7, 98.0]

Average Accuracy (CNN): 89.625
2024-01-29 22:35:55,229 [trainer.py] => Average Accuracy (CNN): 89.625 

2024-01-29 22:35:55,232 [trainer.py] => All params: 171816392
2024-01-29 22:35:55,236 [trainer.py] => Trainable params: 122980
2024-01-29 22:35:55,236 [l2p_self_training.py] => Learning on 20-30
4250
2024-01-29 22:35:55,316 [l2p_self_training.py] => train dataset length: 250
Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.117, Train_accy 92.80, Test_accy 73.00: 100%|██████████████| 5/5 [00:17<00:00,  3.49s/it]
2024-01-29 22:36:12,769 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.117, Train_accy 92.80, Test_accy 73.00
2024-01-29 22:36:12,776 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:36:25,851 [l2p_self_training.py] => wrong labeled samples count: 258
2024-01-29 22:36:25,852 [l2p_self_training.py] => it label on oot samples count: 245
2024-01-29 22:36:25,852 [l2p_self_training.py] => 2092 unlabeled samples will be pseudo labeled
2024-01-29 22:36:25,852 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:36:25,853 [toolkit.py] => Pseudo Accuracy: 0.8766730401529637
2024-01-29 22:36:25,859 [l2p_self_training.py] => train dataset length: 2342
Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.104, Train_accy 93.13, Test_accy 83.60: 100%|██████████████| 5/5 [01:05<00:00, 13.06s/it]
2024-01-29 22:37:31,150 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.104, Train_accy 93.13, Test_accy 83.60
2024-01-29 22:37:31,156 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:37:44,248 [l2p_self_training.py] => wrong labeled samples count: 676
2024-01-29 22:37:44,248 [l2p_self_training.py] => it label on oot samples count: 633
2024-01-29 22:37:44,249 [l2p_self_training.py] => 2733 unlabeled samples will be pseudo labeled
2024-01-29 22:37:44,249 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:37:44,249 [toolkit.py] => Pseudo Accuracy: 0.7526527625320161
2024-01-29 22:37:44,253 [l2p_self_training.py] => train dataset length: 2983
Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.025, Train_accy 90.68, Test_accy 80.03: 100%|██████████████| 5/5 [01:20<00:00, 16.02s/it]
2024-01-29 22:39:04,359 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.025, Train_accy 90.68, Test_accy 80.03
2024-01-29 22:39:04,361 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:39:17,426 [l2p_self_training.py] => wrong labeled samples count: 836
2024-01-29 22:39:17,427 [l2p_self_training.py] => it label on oot samples count: 797
2024-01-29 22:39:17,427 [l2p_self_training.py] => 2905 unlabeled samples will be pseudo labeled
2024-01-29 22:39:17,427 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:39:17,428 [toolkit.py] => Pseudo Accuracy: 0.7122203098106713
2024-01-29 22:39:26,863 [trainer.py] => No NME accuracy.
2024-01-29 22:39:26,863 [trainer.py] => CNN: {'total': 80.03, '00-09': 76.5, '10-19': 77.3, '20-29': 86.3, 'old': 76.9, 'new': 86.3}
2024-01-29 22:39:26,863 [trainer.py] => CNN top1 curve: [94.2, 85.05, 80.03]
2024-01-29 22:39:26,864 [trainer.py] => CNN top5 curve: [99.7, 98.0, 97.93]

Average Accuracy (CNN): 86.42666666666666
2024-01-29 22:39:26,864 [trainer.py] => Average Accuracy (CNN): 86.42666666666666 

2024-01-29 22:39:26,867 [trainer.py] => All params: 171816392
2024-01-29 22:39:26,870 [trainer.py] => Trainable params: 122980
2024-01-29 22:39:26,870 [l2p_self_training.py] => Learning on 30-40
4000
2024-01-29 22:39:26,927 [l2p_self_training.py] => train dataset length: 250
Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.099, Train_accy 93.60, Test_accy 69.70: 100%|██████████████| 5/5 [00:20<00:00,  4.12s/it]
2024-01-29 22:39:47,527 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.099, Train_accy 93.60, Test_accy 69.70
2024-01-29 22:39:47,533 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:39:59,858 [l2p_self_training.py] => wrong labeled samples count: 271
2024-01-29 22:39:59,858 [l2p_self_training.py] => it label on oot samples count: 237
2024-01-29 22:39:59,858 [l2p_self_training.py] => 1942 unlabeled samples will be pseudo labeled
2024-01-29 22:39:59,858 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:39:59,858 [toolkit.py] => Pseudo Accuracy: 0.860453141091658
2024-01-29 22:39:59,861 [l2p_self_training.py] => train dataset length: 2192
Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.101, Train_accy 92.66, Test_accy 78.03: 100%|██████████████| 5/5 [01:04<00:00, 12.92s/it]
2024-01-29 22:41:04,443 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.101, Train_accy 92.66, Test_accy 78.03
2024-01-29 22:41:04,449 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:41:16,799 [l2p_self_training.py] => wrong labeled samples count: 650
2024-01-29 22:41:16,800 [l2p_self_training.py] => it label on oot samples count: 562
2024-01-29 22:41:16,800 [l2p_self_training.py] => 2595 unlabeled samples will be pseudo labeled
2024-01-29 22:41:16,800 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:41:16,800 [toolkit.py] => Pseudo Accuracy: 0.7495183044315993
2024-01-29 22:41:16,807 [l2p_self_training.py] => train dataset length: 2845
Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.052, Train_accy 91.63, Test_accy 79.08: 100%|██████████████| 5/5 [01:19<00:00, 15.98s/it]
2024-01-29 22:42:36,730 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.052, Train_accy 91.63, Test_accy 79.08
2024-01-29 22:42:36,733 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:42:49,059 [l2p_self_training.py] => wrong labeled samples count: 780
2024-01-29 22:42:49,060 [l2p_self_training.py] => it label on oot samples count: 711
2024-01-29 22:42:49,060 [l2p_self_training.py] => 2695 unlabeled samples will be pseudo labeled
2024-01-29 22:42:49,060 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:42:49,060 [toolkit.py] => Pseudo Accuracy: 0.7105751391465677
2024-01-29 22:43:01,444 [trainer.py] => No NME accuracy.
2024-01-29 22:43:01,444 [trainer.py] => CNN: {'total': 79.08, '00-09': 73.9, '10-19': 77.8, '20-29': 86.9, '30-39': 77.7, 'old': 79.53, 'new': 77.7}
2024-01-29 22:43:01,444 [trainer.py] => CNN top1 curve: [94.2, 85.05, 80.03, 79.08]
2024-01-29 22:43:01,444 [trainer.py] => CNN top5 curve: [99.7, 98.0, 97.93, 97.4]

Average Accuracy (CNN): 84.58999999999999
2024-01-29 22:43:01,444 [trainer.py] => Average Accuracy (CNN): 84.58999999999999 

2024-01-29 22:43:01,445 [trainer.py] => All params: 171816392
2024-01-29 22:43:01,446 [trainer.py] => Trainable params: 122980
2024-01-29 22:43:01,446 [l2p_self_training.py] => Learning on 40-50
3750
2024-01-29 22:43:01,496 [l2p_self_training.py] => train dataset length: 250
Task 4, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.123, Train_accy 93.20, Test_accy 70.26: 100%|██████████████| 5/5 [00:23<00:00,  4.69s/it]
2024-01-29 22:43:24,963 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.123, Train_accy 93.20, Test_accy 70.26
2024-01-29 22:43:24,969 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:43:36,661 [l2p_self_training.py] => wrong labeled samples count: 321
2024-01-29 22:43:36,662 [l2p_self_training.py] => it label on oot samples count: 305
2024-01-29 22:43:36,662 [l2p_self_training.py] => 2206 unlabeled samples will be pseudo labeled
2024-01-29 22:43:36,662 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:43:36,663 [toolkit.py] => Pseudo Accuracy: 0.8544877606527652
2024-01-29 22:43:36,669 [l2p_self_training.py] => train dataset length: 2456
Task 4, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.126, Train_accy 94.42, Test_accy 77.48: 100%|██████████████| 5/5 [01:14<00:00, 14.83s/it]
2024-01-29 22:44:50,822 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.126, Train_accy 94.42, Test_accy 77.48
2024-01-29 22:44:50,824 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:45:02,407 [l2p_self_training.py] => wrong labeled samples count: 558
2024-01-29 22:45:02,408 [l2p_self_training.py] => it label on oot samples count: 532
2024-01-29 22:45:02,408 [l2p_self_training.py] => 2624 unlabeled samples will be pseudo labeled
2024-01-29 22:45:02,408 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:45:02,408 [toolkit.py] => Pseudo Accuracy: 0.7873475609756098
2024-01-29 22:45:02,415 [l2p_self_training.py] => train dataset length: 2874
Task 4, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.107, Train_accy 93.11, Test_accy 76.00: 100%|██████████████| 5/5 [01:23<00:00, 16.68s/it]
2024-01-29 22:46:25,801 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.107, Train_accy 93.11, Test_accy 76.00
2024-01-29 22:46:25,808 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:46:37,422 [l2p_self_training.py] => wrong labeled samples count: 762
2024-01-29 22:46:37,422 [l2p_self_training.py] => it label on oot samples count: 711
2024-01-29 22:46:37,423 [l2p_self_training.py] => 2879 unlabeled samples will be pseudo labeled
2024-01-29 22:46:37,423 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:46:37,423 [toolkit.py] => Pseudo Accuracy: 0.7353247655435915
2024-01-29 22:46:52,765 [trainer.py] => No NME accuracy.
2024-01-29 22:46:52,766 [trainer.py] => CNN: {'total': 76.0, '00-09': 61.0, '10-19': 71.7, '20-29': 84.4, '30-39': 74.4, '40-49': 88.5, 'old': 72.88, 'new': 88.5}
2024-01-29 22:46:52,766 [trainer.py] => CNN top1 curve: [94.2, 85.05, 80.03, 79.08, 76.0]
2024-01-29 22:46:52,766 [trainer.py] => CNN top5 curve: [99.7, 98.0, 97.93, 97.4, 97.04]

Average Accuracy (CNN): 82.87199999999999
2024-01-29 22:46:52,766 [trainer.py] => Average Accuracy (CNN): 82.87199999999999 

2024-01-29 22:46:52,767 [trainer.py] => All params: 171816392
2024-01-29 22:46:52,769 [trainer.py] => Trainable params: 122980
2024-01-29 22:46:52,769 [l2p_self_training.py] => Learning on 50-60
3500
2024-01-29 22:46:52,823 [l2p_self_training.py] => train dataset length: 250
Task 5, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.088, Train_accy 90.40, Test_accy 68.47: 100%|██████████████| 5/5 [00:26<00:00,  5.30s/it]
2024-01-29 22:47:19,309 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.088, Train_accy 90.40, Test_accy 68.47
2024-01-29 22:47:19,311 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:47:30,153 [l2p_self_training.py] => wrong labeled samples count: 242
2024-01-29 22:47:30,153 [l2p_self_training.py] => it label on oot samples count: 218
2024-01-29 22:47:30,153 [l2p_self_training.py] => 1999 unlabeled samples will be pseudo labeled
2024-01-29 22:47:30,154 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:47:30,154 [toolkit.py] => Pseudo Accuracy: 0.8789394697348675
2024-01-29 22:47:30,160 [l2p_self_training.py] => train dataset length: 2249
Task 5, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.139, Train_accy 93.73, Test_accy 74.78: 100%|██████████████| 5/5 [01:11<00:00, 14.39s/it]
2024-01-29 22:48:42,135 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.139, Train_accy 93.73, Test_accy 74.78
2024-01-29 22:48:42,141 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:48:52,971 [l2p_self_training.py] => wrong labeled samples count: 454
2024-01-29 22:48:52,972 [l2p_self_training.py] => it label on oot samples count: 389
2024-01-29 22:48:52,972 [l2p_self_training.py] => 2438 unlabeled samples will be pseudo labeled
2024-01-29 22:48:52,972 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:48:52,973 [toolkit.py] => Pseudo Accuracy: 0.8137817883511075
2024-01-29 22:48:52,979 [l2p_self_training.py] => train dataset length: 2688
Task 5, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.067, Train_accy 92.08, Test_accy 75.52: 100%|██████████████| 5/5 [01:22<00:00, 16.43s/it]
2024-01-29 22:50:15,119 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.067, Train_accy 92.08, Test_accy 75.52
2024-01-29 22:50:15,126 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:50:25,936 [l2p_self_training.py] => wrong labeled samples count: 571
2024-01-29 22:50:25,937 [l2p_self_training.py] => it label on oot samples count: 504
2024-01-29 22:50:25,937 [l2p_self_training.py] => 2506 unlabeled samples will be pseudo labeled
2024-01-29 22:50:25,937 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:50:25,938 [toolkit.py] => Pseudo Accuracy: 0.772146847565842
2024-01-29 22:50:44,259 [trainer.py] => No NME accuracy.
2024-01-29 22:50:44,259 [trainer.py] => CNN: {'total': 75.52, '00-09': 57.1, '10-19': 72.7, '20-29': 83.3, '30-39': 74.3, '40-49': 88.8, '50-59': 76.9, 'old': 75.24, 'new': 76.9}
2024-01-29 22:50:44,259 [trainer.py] => CNN top1 curve: [94.2, 85.05, 80.03, 79.08, 76.0, 75.52]
2024-01-29 22:50:44,259 [trainer.py] => CNN top5 curve: [99.7, 98.0, 97.93, 97.4, 97.04, 96.47]

Average Accuracy (CNN): 81.64666666666666
2024-01-29 22:50:44,259 [trainer.py] => Average Accuracy (CNN): 81.64666666666666 

2024-01-29 22:50:44,261 [trainer.py] => All params: 171816392
2024-01-29 22:50:44,262 [trainer.py] => Trainable params: 122980
2024-01-29 22:50:44,262 [l2p_self_training.py] => Learning on 60-70
3250
2024-01-29 22:50:44,312 [l2p_self_training.py] => train dataset length: 250
Task 6, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.134, Train_accy 94.40, Test_accy 71.26: 100%|██████████████| 5/5 [00:29<00:00,  5.88s/it]
2024-01-29 22:51:13,728 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.134, Train_accy 94.40, Test_accy 71.26
2024-01-29 22:51:13,734 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:51:23,886 [l2p_self_training.py] => wrong labeled samples count: 127
2024-01-29 22:51:23,886 [l2p_self_training.py] => it label on oot samples count: 119
2024-01-29 22:51:23,887 [l2p_self_training.py] => 2103 unlabeled samples will be pseudo labeled
2024-01-29 22:51:23,887 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:51:23,887 [toolkit.py] => Pseudo Accuracy: 0.9396100808368997
2024-01-29 22:51:23,894 [l2p_self_training.py] => train dataset length: 2353
Task 6, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.165, Train_accy 95.54, Test_accy 75.91: 100%|██████████████| 5/5 [01:17<00:00, 15.52s/it]
2024-01-29 22:52:41,510 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.165, Train_accy 95.54, Test_accy 75.91
2024-01-29 22:52:41,512 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:52:51,615 [l2p_self_training.py] => wrong labeled samples count: 333
2024-01-29 22:52:51,616 [l2p_self_training.py] => it label on oot samples count: 291
2024-01-29 22:52:51,616 [l2p_self_training.py] => 2454 unlabeled samples will be pseudo labeled
2024-01-29 22:52:51,616 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:52:51,616 [toolkit.py] => Pseudo Accuracy: 0.8643031784841075
2024-01-29 22:52:51,623 [l2p_self_training.py] => train dataset length: 2704
Task 6, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.107, Train_accy 93.42, Test_accy 75.21: 100%|██████████████| 5/5 [01:25<00:00, 17.04s/it]
2024-01-29 22:54:16,849 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.107, Train_accy 93.42, Test_accy 75.21
2024-01-29 22:54:16,856 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:54:27,025 [l2p_self_training.py] => wrong labeled samples count: 402
2024-01-29 22:54:27,026 [l2p_self_training.py] => it label on oot samples count: 354
2024-01-29 22:54:27,026 [l2p_self_training.py] => 2426 unlabeled samples will be pseudo labeled
2024-01-29 22:54:27,027 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:54:27,027 [toolkit.py] => Pseudo Accuracy: 0.8342951360263808
2024-01-29 22:54:48,294 [trainer.py] => No NME accuracy.
2024-01-29 22:54:48,295 [trainer.py] => CNN: {'total': 75.21, '00-09': 49.8, '10-19': 71.6, '20-29': 83.1, '30-39': 72.9, '40-49': 88.7, '50-59': 71.8, '60-69': 88.6, 'old': 72.98, 'new': 88.6}
2024-01-29 22:54:48,295 [trainer.py] => CNN top1 curve: [94.2, 85.05, 80.03, 79.08, 76.0, 75.52, 75.21]
2024-01-29 22:54:48,295 [trainer.py] => CNN top5 curve: [99.7, 98.0, 97.93, 97.4, 97.04, 96.47, 95.97]

Average Accuracy (CNN): 80.72714285714285
2024-01-29 22:54:48,295 [trainer.py] => Average Accuracy (CNN): 80.72714285714285 

2024-01-29 22:54:48,296 [trainer.py] => All params: 171816392
2024-01-29 22:54:48,297 [trainer.py] => Trainable params: 122980
2024-01-29 22:54:48,297 [l2p_self_training.py] => Learning on 70-80
3000
2024-01-29 22:54:48,351 [l2p_self_training.py] => train dataset length: 250
Task 7, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.067, Train_accy 92.40, Test_accy 68.53: 100%|██████████████| 5/5 [00:32<00:00,  6.49s/it]
2024-01-29 22:55:20,814 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.067, Train_accy 92.40, Test_accy 68.53
2024-01-29 22:55:20,820 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:55:30,218 [l2p_self_training.py] => wrong labeled samples count: 83
2024-01-29 22:55:30,219 [l2p_self_training.py] => it label on oot samples count: 62
2024-01-29 22:55:30,219 [l2p_self_training.py] => 1653 unlabeled samples will be pseudo labeled
2024-01-29 22:55:30,219 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:55:30,219 [toolkit.py] => Pseudo Accuracy: 0.9497882637628554
2024-01-29 22:55:30,225 [l2p_self_training.py] => train dataset length: 1903
Task 7, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.200, Train_accy 95.74, Test_accy 73.12: 100%|██████████████| 5/5 [01:10<00:00, 14.00s/it]
2024-01-29 22:56:40,247 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.200, Train_accy 95.74, Test_accy 73.12
2024-01-29 22:56:40,253 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:56:49,679 [l2p_self_training.py] => wrong labeled samples count: 295
2024-01-29 22:56:49,680 [l2p_self_training.py] => it label on oot samples count: 186
2024-01-29 22:56:49,680 [l2p_self_training.py] => 2242 unlabeled samples will be pseudo labeled
2024-01-29 22:56:49,680 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:56:49,681 [toolkit.py] => Pseudo Accuracy: 0.868421052631579
2024-01-29 22:56:49,691 [l2p_self_training.py] => train dataset length: 2492
Task 7, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.136, Train_accy 94.14, Test_accy 73.53: 100%|██████████████| 5/5 [01:23<00:00, 16.71s/it]
2024-01-29 22:58:13,262 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.136, Train_accy 94.14, Test_accy 73.53
2024-01-29 22:58:13,269 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:58:22,657 [l2p_self_training.py] => wrong labeled samples count: 299
2024-01-29 22:58:22,658 [l2p_self_training.py] => it label on oot samples count: 209
2024-01-29 22:58:22,658 [l2p_self_training.py] => 2238 unlabeled samples will be pseudo labeled
2024-01-29 22:58:22,658 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:58:22,659 [toolkit.py] => Pseudo Accuracy: 0.8663985701519213
2024-01-29 22:58:46,937 [trainer.py] => No NME accuracy.
2024-01-29 22:58:46,937 [trainer.py] => CNN: {'total': 73.53, '00-09': 51.3, '10-19': 71.5, '20-29': 84.1, '30-39': 71.8, '40-49': 87.9, '50-59': 72.8, '60-69': 85.0, '70-79': 63.8, 'old': 74.91, 'new': 63.8}
2024-01-29 22:58:46,937 [trainer.py] => CNN top1 curve: [94.2, 85.05, 80.03, 79.08, 76.0, 75.52, 75.21, 73.53]
2024-01-29 22:58:46,937 [trainer.py] => CNN top5 curve: [99.7, 98.0, 97.93, 97.4, 97.04, 96.47, 95.97, 95.89]

Average Accuracy (CNN): 79.82749999999999
2024-01-29 22:58:46,938 [trainer.py] => Average Accuracy (CNN): 79.82749999999999 

2024-01-29 22:58:46,941 [trainer.py] => All params: 171816392
2024-01-29 22:58:46,943 [trainer.py] => Trainable params: 122980
2024-01-29 22:58:46,943 [l2p_self_training.py] => Learning on 80-90
2750
2024-01-29 22:58:46,997 [l2p_self_training.py] => train dataset length: 250
Task 8, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.095, Train_accy 92.00, Test_accy 67.98: 100%|██████████████| 5/5 [00:35<00:00,  7.08s/it]
2024-01-29 22:59:22,411 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.095, Train_accy 92.00, Test_accy 67.98
2024-01-29 22:59:22,418 [l2p_self_training.py] => pseudo labeling start
2024-01-29 22:59:31,065 [l2p_self_training.py] => wrong labeled samples count: 78
2024-01-29 22:59:31,066 [l2p_self_training.py] => it label on oot samples count: 54
2024-01-29 22:59:31,066 [l2p_self_training.py] => 1855 unlabeled samples will be pseudo labeled
2024-01-29 22:59:31,066 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 22:59:31,067 [toolkit.py] => Pseudo Accuracy: 0.9579514824797843
2024-01-29 22:59:31,072 [l2p_self_training.py] => train dataset length: 2105
Task 8, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.135, Train_accy 94.16, Test_accy 72.24: 100%|██████████████| 5/5 [01:17<00:00, 15.53s/it]
2024-01-29 23:00:48,710 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.135, Train_accy 94.16, Test_accy 72.24
2024-01-29 23:00:48,716 [l2p_self_training.py] => pseudo labeling start
2024-01-29 23:00:57,338 [l2p_self_training.py] => wrong labeled samples count: 128
2024-01-29 23:00:57,338 [l2p_self_training.py] => it label on oot samples count: 80
2024-01-29 23:00:57,339 [l2p_self_training.py] => 2140 unlabeled samples will be pseudo labeled
2024-01-29 23:00:57,339 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 23:00:57,339 [toolkit.py] => Pseudo Accuracy: 0.9401869158878504
2024-01-29 23:00:57,345 [l2p_self_training.py] => train dataset length: 2390
Task 8, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.136, Train_accy 93.60, Test_accy 72.38: 100%|██████████████| 5/5 [01:24<00:00, 16.85s/it]
2024-01-29 23:02:21,615 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.136, Train_accy 93.60, Test_accy 72.38
2024-01-29 23:02:21,622 [l2p_self_training.py] => pseudo labeling start
2024-01-29 23:02:30,246 [l2p_self_training.py] => wrong labeled samples count: 171
2024-01-29 23:02:30,247 [l2p_self_training.py] => it label on oot samples count: 108
2024-01-29 23:02:30,247 [l2p_self_training.py] => 2204 unlabeled samples will be pseudo labeled
2024-01-29 23:02:30,247 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 23:02:30,248 [toolkit.py] => Pseudo Accuracy: 0.9224137931034483
2024-01-29 23:02:57,586 [trainer.py] => No NME accuracy.
2024-01-29 23:02:57,586 [trainer.py] => CNN: {'total': 72.38, '00-09': 45.2, '10-19': 70.2, '20-29': 83.1, '30-39': 70.6, '40-49': 86.3, '50-59': 72.2, '60-69': 85.9, '70-79': 62.2, '80-89': 75.7, 'old': 71.96, 'new': 75.7}
2024-01-29 23:02:57,586 [trainer.py] => CNN top1 curve: [94.2, 85.05, 80.03, 79.08, 76.0, 75.52, 75.21, 73.53, 72.38]
2024-01-29 23:02:57,586 [trainer.py] => CNN top5 curve: [99.7, 98.0, 97.93, 97.4, 97.04, 96.47, 95.97, 95.89, 95.29]

Average Accuracy (CNN): 78.99999999999999
2024-01-29 23:02:57,587 [trainer.py] => Average Accuracy (CNN): 78.99999999999999 

2024-01-29 23:02:57,590 [trainer.py] => All params: 171816392
2024-01-29 23:02:57,592 [trainer.py] => Trainable params: 122980
2024-01-29 23:02:57,592 [l2p_self_training.py] => Learning on 90-100
2500
2024-01-29 23:02:57,645 [l2p_self_training.py] => train dataset length: 250
Task 9, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.090, Train_accy 92.40, Test_accy 67.34: 100%|██████████████| 5/5 [00:38<00:00,  7.74s/it]
2024-01-29 23:03:36,334 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.090, Train_accy 92.40, Test_accy 67.34
2024-01-29 23:03:36,337 [l2p_self_training.py] => pseudo labeling start
2024-01-29 23:03:44,221 [l2p_self_training.py] => wrong labeled samples count: 13
2024-01-29 23:03:44,221 [l2p_self_training.py] => it label on oot samples count: 0
2024-01-29 23:03:44,222 [l2p_self_training.py] => 1758 unlabeled samples will be pseudo labeled
2024-01-29 23:03:44,222 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 23:03:44,222 [toolkit.py] => Pseudo Accuracy: 0.9926052332195677
2024-01-29 23:03:44,229 [l2p_self_training.py] => train dataset length: 2008
Task 9, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.120, Train_accy 93.53, Test_accy 70.72: 100%|██████████████| 5/5 [01:18<00:00, 15.68s/it]
2024-01-29 23:05:02,636 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.120, Train_accy 93.53, Test_accy 70.72
2024-01-29 23:05:02,643 [l2p_self_training.py] => pseudo labeling start
2024-01-29 23:05:10,518 [l2p_self_training.py] => wrong labeled samples count: 64
2024-01-29 23:05:10,519 [l2p_self_training.py] => it label on oot samples count: 0
2024-01-29 23:05:10,519 [l2p_self_training.py] => 2046 unlabeled samples will be pseudo labeled
2024-01-29 23:05:10,519 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 23:05:10,519 [toolkit.py] => Pseudo Accuracy: 0.9687194525904204
2024-01-29 23:05:10,526 [l2p_self_training.py] => train dataset length: 2296
Task 9, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.116, Train_accy 93.34, Test_accy 71.34: 100%|██████████████| 5/5 [01:25<00:00, 17.00s/it]
2024-01-29 23:06:35,552 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.116, Train_accy 93.34, Test_accy 71.34
2024-01-29 23:06:35,558 [l2p_self_training.py] => pseudo labeling start
2024-01-29 23:06:43,448 [l2p_self_training.py] => wrong labeled samples count: 67
2024-01-29 23:06:43,449 [l2p_self_training.py] => it label on oot samples count: 0
2024-01-29 23:06:43,449 [l2p_self_training.py] => 2052 unlabeled samples will be pseudo labeled
2024-01-29 23:06:43,449 [l2p_self_training.py] => pseudo labeling finish
2024-01-29 23:06:43,450 [toolkit.py] => Pseudo Accuracy: 0.9673489278752436
2024-01-29 23:07:13,640 [trainer.py] => No NME accuracy.
2024-01-29 23:07:13,641 [trainer.py] => CNN: {'total': 71.34, '00-09': 46.1, '10-19': 68.4, '20-29': 82.7, '30-39': 71.2, '40-49': 86.6, '50-59': 71.9, '60-69': 84.1, '70-79': 61.2, '80-89': 75.9, '90-99': 65.3, 'old': 72.01, 'new': 65.3}
2024-01-29 23:07:13,641 [trainer.py] => CNN top1 curve: [94.2, 85.05, 80.03, 79.08, 76.0, 75.52, 75.21, 73.53, 72.38, 71.34]
2024-01-29 23:07:13,641 [trainer.py] => CNN top5 curve: [99.7, 98.0, 97.93, 97.4, 97.04, 96.47, 95.97, 95.89, 95.29, 94.6]

Average Accuracy (CNN): 78.234
2024-01-29 23:07:13,641 [trainer.py] => Average Accuracy (CNN): 78.234 

Accuracy Matrix (CNN):
[[94.2 84.5 76.5 73.9 61.  57.1 49.8 51.3 45.2 46.1]
 [ 0.  85.6 77.3 77.8 71.7 72.7 71.6 71.5 70.2 68.4]
 [ 0.   0.  86.3 86.9 84.4 83.3 83.1 84.1 83.1 82.7]
 [ 0.   0.   0.  77.7 74.4 74.3 72.9 71.8 70.6 71.2]
 [ 0.   0.   0.   0.  88.5 88.8 88.7 87.9 86.3 86.6]
 [ 0.   0.   0.   0.   0.  76.9 71.8 72.8 72.2 71.9]
 [ 0.   0.   0.   0.   0.   0.  88.6 85.  85.9 84.1]
 [ 0.   0.   0.   0.   0.   0.   0.  63.8 62.2 61.2]
 [ 0.   0.   0.   0.   0.   0.   0.   0.  75.7 75.9]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.  65.3]]
2024-01-29 23:07:13,643 [trainer.py] => Forgetting (CNN): 10.033333333333331

######################################################################################
======================================================================================
######################################################################################
原l2p+Self-Training方法在有future oot数据时伪标签打的对错（分类头为 0:total classes）

(base) root@autodl-container-56644e8033-177721e5:~/autodl-tmp/LAMDA-PILOT# python main.py --config=exps/l2p_self_training.json
2024-02-02 21:45:13,597 [trainer.py] => config: exps/l2p_self_training.json
2024-02-02 21:45:13,597 [trainer.py] => prefix:  
2024-02-02 21:45:13,598 [trainer.py] => dataset: cifar224
2024-02-02 21:45:13,598 [trainer.py] => memory_size: 0
2024-02-02 21:45:13,598 [trainer.py] => memory_per_class: 0
2024-02-02 21:45:13,598 [trainer.py] => fixed_memory: False
2024-02-02 21:45:13,598 [trainer.py] => shuffle: True
2024-02-02 21:45:13,598 [trainer.py] => init_cls: 10
2024-02-02 21:45:13,598 [trainer.py] => increment: 10
2024-02-02 21:45:13,598 [trainer.py] => model_name: l2p_self_training
2024-02-02 21:45:13,598 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2024-02-02 21:45:13,598 [trainer.py] => get_original_backbone: True
2024-02-02 21:45:13,598 [trainer.py] => device: [device(type='cuda', index=0)]
2024-02-02 21:45:13,598 [trainer.py] => seed: 1993
2024-02-02 21:45:13,598 [trainer.py] => tuned_epoch: 5
2024-02-02 21:45:13,598 [trainer.py] => init_lr: 0.001875
2024-02-02 21:45:13,598 [trainer.py] => batch_size: 16
2024-02-02 21:45:13,598 [trainer.py] => weight_decay: 0
2024-02-02 21:45:13,598 [trainer.py] => min_lr: 1e-05
2024-02-02 21:45:13,598 [trainer.py] => optimizer: adam
2024-02-02 21:45:13,598 [trainer.py] => scheduler: constant
2024-02-02 21:45:13,598 [trainer.py] => reinit_optimizer: True
2024-02-02 21:45:13,598 [trainer.py] => global_pool: token
2024-02-02 21:45:13,598 [trainer.py] => head_type: prompt
2024-02-02 21:45:13,598 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2024-02-02 21:45:13,598 [trainer.py] => pretrained: True
2024-02-02 21:45:13,598 [trainer.py] => drop: 0.0
2024-02-02 21:45:13,598 [trainer.py] => drop_path: 0.0
2024-02-02 21:45:13,598 [trainer.py] => prompt_pool: True
2024-02-02 21:45:13,598 [trainer.py] => pool_size: 10
2024-02-02 21:45:13,598 [trainer.py] => length: 5
2024-02-02 21:45:13,598 [trainer.py] => top_k: 5
2024-02-02 21:45:13,598 [trainer.py] => initializer: uniform
2024-02-02 21:45:13,599 [trainer.py] => prompt_key: True
2024-02-02 21:45:13,599 [trainer.py] => prompt_key_init: uniform
2024-02-02 21:45:13,599 [trainer.py] => use_prompt_mask: False
2024-02-02 21:45:13,599 [trainer.py] => shared_prompt_pool: False
2024-02-02 21:45:13,599 [trainer.py] => shared_prompt_key: False
2024-02-02 21:45:13,599 [trainer.py] => batchwise_prompt: True
2024-02-02 21:45:13,599 [trainer.py] => embedding_key: cls
2024-02-02 21:45:13,599 [trainer.py] => predefined_key: 
2024-02-02 21:45:13,599 [trainer.py] => pull_constraint: True
2024-02-02 21:45:13,599 [trainer.py] => pull_constraint_coeff: 0.1
2024-02-02 21:45:13,599 [trainer.py] => semi_supervised_mode: True
2024-02-02 21:45:13,599 [trainer.py] => labeled_ratio: 0.05
2024-02-02 21:45:13,599 [trainer.py] => unlabeled_data_distribution_mode: future_oot
2024-02-02 21:45:13,599 [trainer.py] => confidence_threshold: 0.9
2024-02-02 21:45:13,599 [trainer.py] => max_self_training_iteration: 3
Files already downloaded and verified
Files already downloaded and verified
2024-02-02 21:45:15,132 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-02-02 21:45:16,660 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2024-02-02 21:45:16,660 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2024-02-02 21:45:19,029 [l2p_self_training.py] => 85,940,836 model total parameters.
2024-02-02 21:45:19,029 [l2p_self_training.py] => 122,980 model training parameters.
2024-02-02 21:45:19,029 [l2p_self_training.py] => prompt.prompt: 38400
2024-02-02 21:45:19,029 [l2p_self_training.py] => prompt.prompt_key: 7680
2024-02-02 21:45:19,030 [l2p_self_training.py] => head.weight: 76800
2024-02-02 21:45:19,030 [l2p_self_training.py] => head.bias: 100
2024-02-02 21:45:19,031 [trainer.py] => All params: 171816392
2024-02-02 21:45:19,032 [trainer.py] => Trainable params: 122980
2024-02-02 21:45:19,032 [l2p_self_training.py] => Learning on 0-10
4750
2024-02-02 21:45:19,312 [l2p_self_training.py] => train dataset length: 250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.515, Train_accy 84.00, Test_accy 89.50: 100%|███████████████| 5/5 [00:11<00:00,  2.25s/it]
2024-02-02 21:45:30,581 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.515, Train_accy 84.00, Test_accy 89.50
2024-02-02 21:45:30,588 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:45:45,604 [l2p_self_training.py] => wrong labeled samples count: 249
2024-02-02 21:45:45,605 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 21:45:45,605 [l2p_self_training.py] => it label on future sample count: 233
2024-02-02 21:45:45,605 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-02 21:45:45,606 [l2p_self_training.py] => 1274 unlabeled samples will be pseudo labeled
2024-02-02 21:45:45,606 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:45:45,606 [toolkit.py] => Pseudo Accuracy: 0.804552590266876
2024-02-02 21:45:45,613 [l2p_self_training.py] => train dataset length: 1524
Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.150, Train_accy 88.39, Test_accy 93.00: 100%|███████████████| 5/5 [00:40<00:00,  8.16s/it]
2024-02-02 21:46:26,437 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.150, Train_accy 88.39, Test_accy 93.00
2024-02-02 21:46:26,439 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:46:41,271 [l2p_self_training.py] => wrong labeled samples count: 792
2024-02-02 21:46:41,272 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 21:46:41,272 [l2p_self_training.py] => it label on future sample count: 731
2024-02-02 21:46:41,273 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-02 21:46:41,273 [l2p_self_training.py] => 2460 unlabeled samples will be pseudo labeled
2024-02-02 21:46:41,273 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:46:41,273 [toolkit.py] => Pseudo Accuracy: 0.6780487804878049
2024-02-02 21:46:41,278 [l2p_self_training.py] => train dataset length: 2710
Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.077, Train_accy 87.64, Test_accy 94.20: 100%|███████████████| 5/5 [01:08<00:00, 13.73s/it]
2024-02-02 21:47:49,920 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.077, Train_accy 87.64, Test_accy 94.20
2024-02-02 21:47:49,926 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:48:04,839 [l2p_self_training.py] => wrong labeled samples count: 1123
2024-02-02 21:48:04,843 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 21:48:04,843 [l2p_self_training.py] => it label on future sample count: 1055
2024-02-02 21:48:04,843 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-02 21:48:04,843 [l2p_self_training.py] => 2922 unlabeled samples will be pseudo labeled
2024-02-02 21:48:04,843 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:48:04,843 [toolkit.py] => Pseudo Accuracy: 0.6156741957563313
2024-02-02 21:48:08,262 [trainer.py] => No NME accuracy.
2024-02-02 21:48:08,262 [trainer.py] => CNN: {'total': 94.2, '00-09': 94.2, 'old': 0, 'new': 94.2}
2024-02-02 21:48:08,262 [trainer.py] => CNN top1 curve: [94.2]
2024-02-02 21:48:08,262 [trainer.py] => CNN top5 curve: [99.7]

Average Accuracy (CNN): 94.2
2024-02-02 21:48:08,262 [trainer.py] => Average Accuracy (CNN): 94.2 

2024-02-02 21:48:08,263 [trainer.py] => All params: 171816392
2024-02-02 21:48:08,265 [trainer.py] => Trainable params: 122980
2024-02-02 21:48:08,265 [l2p_self_training.py] => Learning on 10-20
4500
2024-02-02 21:48:08,326 [l2p_self_training.py] => train dataset length: 250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.028, Train_accy 91.60, Test_accy 74.35: 100%|██████████████| 5/5 [00:14<00:00,  2.85s/it]
2024-02-02 21:48:22,571 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.028, Train_accy 91.60, Test_accy 74.35
2024-02-02 21:48:22,578 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:48:36,646 [l2p_self_training.py] => wrong labeled samples count: 1273
2024-02-02 21:48:36,651 [l2p_self_training.py] => previous label on future samples count: 752
2024-02-02 21:48:36,651 [l2p_self_training.py] => it label on future sample count: 48
2024-02-02 21:48:36,651 [l2p_self_training.py] => previous label on it sample count: 471
2024-02-02 21:48:36,651 [l2p_self_training.py] => 1768 unlabeled samples will be pseudo labeled
2024-02-02 21:48:36,651 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:48:36,652 [toolkit.py] => Pseudo Accuracy: 0.27997737556561086
2024-02-02 21:48:36,658 [l2p_self_training.py] => train dataset length: 2018
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.330, Train_accy 79.78, Test_accy 70.20: 100%|███████████████| 5/5 [00:55<00:00, 11.11s/it]
2024-02-02 21:49:32,229 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.330, Train_accy 79.78, Test_accy 70.20
2024-02-02 21:49:32,236 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:49:46,261 [l2p_self_training.py] => wrong labeled samples count: 1429
2024-02-02 21:49:46,267 [l2p_self_training.py] => previous label on future samples count: 865
2024-02-02 21:49:46,267 [l2p_self_training.py] => it label on future sample count: 100
2024-02-02 21:49:46,267 [l2p_self_training.py] => previous label on it sample count: 449
2024-02-02 21:49:46,267 [l2p_self_training.py] => 2085 unlabeled samples will be pseudo labeled
2024-02-02 21:49:46,267 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:49:46,267 [toolkit.py] => Pseudo Accuracy: 0.3146282973621103
2024-02-02 21:49:46,279 [l2p_self_training.py] => train dataset length: 2335
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.199, Train_accy 84.33, Test_accy 66.60: 100%|███████████████| 5/5 [01:03<00:00, 12.61s/it]
2024-02-02 21:50:49,340 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.199, Train_accy 84.33, Test_accy 66.60
2024-02-02 21:50:49,346 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:51:03,425 [l2p_self_training.py] => wrong labeled samples count: 1488
2024-02-02 21:51:03,431 [l2p_self_training.py] => previous label on future samples count: 955
2024-02-02 21:51:03,431 [l2p_self_training.py] => it label on future sample count: 98
2024-02-02 21:51:03,431 [l2p_self_training.py] => previous label on it sample count: 421
2024-02-02 21:51:03,431 [l2p_self_training.py] => 2244 unlabeled samples will be pseudo labeled
2024-02-02 21:51:03,431 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:51:03,432 [toolkit.py] => Pseudo Accuracy: 0.33689839572192515
2024-02-02 21:51:09,894 [trainer.py] => No NME accuracy.
2024-02-02 21:51:09,895 [trainer.py] => CNN: {'total': 66.6, '00-09': 72.7, '10-19': 60.5, 'old': 72.7, 'new': 60.5}
2024-02-02 21:51:09,895 [trainer.py] => CNN top1 curve: [94.2, 66.6]
2024-02-02 21:51:09,895 [trainer.py] => CNN top5 curve: [99.7, 95.35]

Average Accuracy (CNN): 80.4
2024-02-02 21:51:09,895 [trainer.py] => Average Accuracy (CNN): 80.4 

2024-02-02 21:51:09,899 [trainer.py] => All params: 171816392
2024-02-02 21:51:09,902 [trainer.py] => Trainable params: 122980
2024-02-02 21:51:09,902 [l2p_self_training.py] => Learning on 20-30
4250
2024-02-02 21:51:09,971 [l2p_self_training.py] => train dataset length: 250
Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.102, Train_accy 92.40, Test_accy 55.50: 100%|██████████████| 5/5 [00:17<00:00,  3.49s/it]
2024-02-02 21:51:27,446 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.102, Train_accy 92.40, Test_accy 55.50
2024-02-02 21:51:27,453 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:51:40,757 [l2p_self_training.py] => wrong labeled samples count: 1553
2024-02-02 21:51:40,762 [l2p_self_training.py] => previous label on future samples count: 897
2024-02-02 21:51:40,763 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 21:51:40,763 [l2p_self_training.py] => previous label on it sample count: 656
2024-02-02 21:51:40,763 [l2p_self_training.py] => 1641 unlabeled samples will be pseudo labeled
2024-02-02 21:51:40,763 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:51:40,763 [toolkit.py] => Pseudo Accuracy: 0.05362583790371724
2024-02-02 21:51:40,767 [l2p_self_training.py] => train dataset length: 1891
Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.271, Train_accy 81.23, Test_accy 46.33: 100%|███████████████| 5/5 [00:55<00:00, 11.18s/it]
2024-02-02 21:52:36,673 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.271, Train_accy 81.23, Test_accy 46.33
2024-02-02 21:52:36,680 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:52:50,036 [l2p_self_training.py] => wrong labeled samples count: 1892
2024-02-02 21:52:50,043 [l2p_self_training.py] => previous label on future samples count: 1017
2024-02-02 21:52:50,043 [l2p_self_training.py] => it label on future sample count: 2
2024-02-02 21:52:50,043 [l2p_self_training.py] => previous label on it sample count: 872
2024-02-02 21:52:50,043 [l2p_self_training.py] => 2126 unlabeled samples will be pseudo labeled
2024-02-02 21:52:50,043 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:52:50,043 [toolkit.py] => Pseudo Accuracy: 0.11006585136406397
2024-02-02 21:52:50,048 [l2p_self_training.py] => train dataset length: 2376
Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.205, Train_accy 84.26, Test_accy 48.83: 100%|███████████████| 5/5 [01:07<00:00, 13.44s/it]
2024-02-02 21:53:57,247 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.205, Train_accy 84.26, Test_accy 48.83
2024-02-02 21:53:57,254 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:54:10,525 [l2p_self_training.py] => wrong labeled samples count: 1596
2024-02-02 21:54:10,530 [l2p_self_training.py] => previous label on future samples count: 985
2024-02-02 21:54:10,531 [l2p_self_training.py] => it label on future sample count: 8
2024-02-02 21:54:10,531 [l2p_self_training.py] => previous label on it sample count: 602
2024-02-02 21:54:10,531 [l2p_self_training.py] => 2025 unlabeled samples will be pseudo labeled
2024-02-02 21:54:10,531 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:54:10,531 [toolkit.py] => Pseudo Accuracy: 0.21185185185185185
2024-02-02 21:54:20,164 [trainer.py] => No NME accuracy.
2024-02-02 21:54:20,165 [trainer.py] => CNN: {'total': 48.83, '00-09': 72.2, '10-19': 32.8, '20-29': 41.5, 'old': 52.5, 'new': 41.5}
2024-02-02 21:54:20,165 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83]
2024-02-02 21:54:20,165 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2]

Average Accuracy (CNN): 69.87666666666667
2024-02-02 21:54:20,166 [trainer.py] => Average Accuracy (CNN): 69.87666666666667 

2024-02-02 21:54:20,170 [trainer.py] => All params: 171816392
2024-02-02 21:54:20,173 [trainer.py] => Trainable params: 122980
2024-02-02 21:54:20,174 [l2p_self_training.py] => Learning on 30-40
4000
2024-02-02 21:54:20,231 [l2p_self_training.py] => train dataset length: 250
Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.079, Train_accy 94.00, Test_accy 44.22: 100%|██████████████| 5/5 [00:20<00:00,  4.10s/it]
2024-02-02 21:54:40,756 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.079, Train_accy 94.00, Test_accy 44.22
2024-02-02 21:54:40,763 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:54:53,305 [l2p_self_training.py] => wrong labeled samples count: 1616
2024-02-02 21:54:53,310 [l2p_self_training.py] => previous label on future samples count: 835
2024-02-02 21:54:53,310 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 21:54:53,310 [l2p_self_training.py] => previous label on it sample count: 781
2024-02-02 21:54:53,310 [l2p_self_training.py] => 1788 unlabeled samples will be pseudo labeled
2024-02-02 21:54:53,310 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:54:53,310 [toolkit.py] => Pseudo Accuracy: 0.09619686800894854
2024-02-02 21:54:53,314 [l2p_self_training.py] => train dataset length: 2038
Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.252, Train_accy 82.78, Test_accy 36.95: 100%|███████████████| 5/5 [01:02<00:00, 12.49s/it]
2024-02-02 21:55:55,753 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.252, Train_accy 82.78, Test_accy 36.95
2024-02-02 21:55:55,759 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:56:08,317 [l2p_self_training.py] => wrong labeled samples count: 1809
2024-02-02 21:56:08,322 [l2p_self_training.py] => previous label on future samples count: 989
2024-02-02 21:56:08,322 [l2p_self_training.py] => it label on future sample count: 1
2024-02-02 21:56:08,323 [l2p_self_training.py] => previous label on it sample count: 816
2024-02-02 21:56:08,323 [l2p_self_training.py] => 2237 unlabeled samples will be pseudo labeled
2024-02-02 21:56:08,323 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:56:08,323 [toolkit.py] => Pseudo Accuracy: 0.19132767098793027
2024-02-02 21:56:08,328 [l2p_self_training.py] => train dataset length: 2487
Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.132, Train_accy 85.97, Test_accy 34.83: 100%|███████████████| 5/5 [01:12<00:00, 14.54s/it]
2024-02-02 21:57:21,012 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.132, Train_accy 85.97, Test_accy 34.83
2024-02-02 21:57:21,019 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:57:33,593 [l2p_self_training.py] => wrong labeled samples count: 1640
2024-02-02 21:57:33,599 [l2p_self_training.py] => previous label on future samples count: 943
2024-02-02 21:57:33,599 [l2p_self_training.py] => it label on future sample count: 2
2024-02-02 21:57:33,599 [l2p_self_training.py] => previous label on it sample count: 688
2024-02-02 21:57:33,599 [l2p_self_training.py] => 2186 unlabeled samples will be pseudo labeled
2024-02-02 21:57:33,600 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:57:33,600 [toolkit.py] => Pseudo Accuracy: 0.24977127172918573
2024-02-02 21:57:46,192 [trainer.py] => No NME accuracy.
2024-02-02 21:57:46,193 [trainer.py] => CNN: {'total': 34.83, '00-09': 60.3, '10-19': 26.7, '20-29': 9.2, '30-39': 43.1, 'old': 32.07, 'new': 43.1}
2024-02-02 21:57:46,193 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83]
2024-02-02 21:57:46,193 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08]

Average Accuracy (CNN): 61.114999999999995
2024-02-02 21:57:46,193 [trainer.py] => Average Accuracy (CNN): 61.114999999999995 

2024-02-02 21:57:46,194 [trainer.py] => All params: 171816392
2024-02-02 21:57:46,196 [trainer.py] => Trainable params: 122980
2024-02-02 21:57:46,196 [l2p_self_training.py] => Learning on 40-50
3750
2024-02-02 21:57:46,251 [l2p_self_training.py] => train dataset length: 250
Task 4, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.107, Train_accy 92.00, Test_accy 31.70: 100%|██████████████| 5/5 [00:23<00:00,  4.74s/it]
2024-02-02 21:58:09,965 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.107, Train_accy 92.00, Test_accy 31.70
2024-02-02 21:58:09,971 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:58:21,791 [l2p_self_training.py] => wrong labeled samples count: 2007
2024-02-02 21:58:21,799 [l2p_self_training.py] => previous label on future samples count: 800
2024-02-02 21:58:21,799 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 21:58:21,799 [l2p_self_training.py] => previous label on it sample count: 1207
2024-02-02 21:58:21,799 [l2p_self_training.py] => 2065 unlabeled samples will be pseudo labeled
2024-02-02 21:58:21,799 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:58:21,800 [toolkit.py] => Pseudo Accuracy: 0.028087167070217918
2024-02-02 21:58:21,805 [l2p_self_training.py] => train dataset length: 2315
Task 4, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.209, Train_accy 83.97, Test_accy 26.08: 100%|███████████████| 5/5 [01:12<00:00, 14.44s/it]
2024-02-02 21:59:34,019 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.209, Train_accy 83.97, Test_accy 26.08
2024-02-02 21:59:34,025 [l2p_self_training.py] => pseudo labeling start
2024-02-02 21:59:45,807 [l2p_self_training.py] => wrong labeled samples count: 1969
2024-02-02 21:59:45,814 [l2p_self_training.py] => previous label on future samples count: 790
2024-02-02 21:59:45,814 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 21:59:45,814 [l2p_self_training.py] => previous label on it sample count: 1179
2024-02-02 21:59:45,815 [l2p_self_training.py] => 2098 unlabeled samples will be pseudo labeled
2024-02-02 21:59:45,815 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 21:59:45,815 [toolkit.py] => Pseudo Accuracy: 0.061487130600571975
2024-02-02 21:59:45,821 [l2p_self_training.py] => train dataset length: 2348
Task 4, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.112, Train_accy 85.60, Test_accy 23.08: 100%|███████████████| 5/5 [01:12<00:00, 14.56s/it]
2024-02-02 22:00:58,598 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.112, Train_accy 85.60, Test_accy 23.08
2024-02-02 22:00:58,600 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:01:10,408 [l2p_self_training.py] => wrong labeled samples count: 2027
2024-02-02 22:01:10,415 [l2p_self_training.py] => previous label on future samples count: 870
2024-02-02 22:01:10,415 [l2p_self_training.py] => it label on future sample count: 2
2024-02-02 22:01:10,415 [l2p_self_training.py] => previous label on it sample count: 1154
2024-02-02 22:01:10,415 [l2p_self_training.py] => 2208 unlabeled samples will be pseudo labeled
2024-02-02 22:01:10,415 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:01:10,416 [toolkit.py] => Pseudo Accuracy: 0.08197463768115942
2024-02-02 22:01:26,044 [trainer.py] => No NME accuracy.
2024-02-02 22:01:26,045 [trainer.py] => CNN: {'total': 23.08, '00-09': 56.7, '10-19': 16.8, '20-29': 5.9, '30-39': 13.2, '40-49': 22.8, 'old': 23.15, 'new': 22.8}
2024-02-02 22:01:26,045 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08]
2024-02-02 22:01:26,045 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2]

Average Accuracy (CNN): 53.507999999999996
2024-02-02 22:01:26,045 [trainer.py] => Average Accuracy (CNN): 53.507999999999996 

2024-02-02 22:01:26,049 [trainer.py] => All params: 171816392
2024-02-02 22:01:26,052 [trainer.py] => Trainable params: 122980
2024-02-02 22:01:26,052 [l2p_self_training.py] => Learning on 50-60
3500
2024-02-02 22:01:26,200 [l2p_self_training.py] => train dataset length: 250
Task 5, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.063, Train_accy 89.20, Test_accy 22.25: 100%|██████████████| 5/5 [00:26<00:00,  5.29s/it]
2024-02-02 22:01:52,634 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.063, Train_accy 89.20, Test_accy 22.25
2024-02-02 22:01:52,636 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:02:03,672 [l2p_self_training.py] => wrong labeled samples count: 2022
2024-02-02 22:02:03,679 [l2p_self_training.py] => previous label on future samples count: 658
2024-02-02 22:02:03,679 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:02:03,680 [l2p_self_training.py] => previous label on it sample count: 1364
2024-02-02 22:02:03,680 [l2p_self_training.py] => 2033 unlabeled samples will be pseudo labeled
2024-02-02 22:02:03,680 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:02:03,680 [toolkit.py] => Pseudo Accuracy: 0.0054107230693556324
2024-02-02 22:02:03,685 [l2p_self_training.py] => train dataset length: 2283
Task 5, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.153, Train_accy 84.23, Test_accy 14.87: 100%|███████████████| 5/5 [01:13<00:00, 14.79s/it]
2024-02-02 22:03:17,649 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.153, Train_accy 84.23, Test_accy 14.87
2024-02-02 22:03:17,654 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:03:28,781 [l2p_self_training.py] => wrong labeled samples count: 1830
2024-02-02 22:03:28,788 [l2p_self_training.py] => previous label on future samples count: 630
2024-02-02 22:03:28,788 [l2p_self_training.py] => it label on future sample count: 3
2024-02-02 22:03:28,788 [l2p_self_training.py] => previous label on it sample count: 1197
2024-02-02 22:03:28,788 [l2p_self_training.py] => 1852 unlabeled samples will be pseudo labeled
2024-02-02 22:03:28,788 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:03:28,789 [toolkit.py] => Pseudo Accuracy: 0.011879049676025918
2024-02-02 22:03:28,793 [l2p_self_training.py] => train dataset length: 2102
Task 5, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.049, Train_accy 88.34, Test_accy 13.82: 100%|███████████████| 5/5 [01:09<00:00, 13.99s/it]
2024-02-02 22:04:38,755 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.049, Train_accy 88.34, Test_accy 13.82
2024-02-02 22:04:38,761 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:04:49,790 [l2p_self_training.py] => wrong labeled samples count: 1966
2024-02-02 22:04:49,796 [l2p_self_training.py] => previous label on future samples count: 675
2024-02-02 22:04:49,797 [l2p_self_training.py] => it label on future sample count: 4
2024-02-02 22:04:49,797 [l2p_self_training.py] => previous label on it sample count: 1285
2024-02-02 22:04:49,797 [l2p_self_training.py] => 2056 unlabeled samples will be pseudo labeled
2024-02-02 22:04:49,797 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:04:49,797 [toolkit.py] => Pseudo Accuracy: 0.04377431906614786
2024-02-02 22:05:08,453 [trainer.py] => No NME accuracy.
2024-02-02 22:05:08,454 [trainer.py] => CNN: {'total': 13.82, '00-09': 49.9, '10-19': 13.3, '20-29': 0.3, '30-39': 0.2, '40-49': 1.5, '50-59': 17.7, 'old': 13.04, 'new': 17.7}
2024-02-02 22:05:08,454 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08, 13.82]
2024-02-02 22:05:08,454 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2, 33.65]

Average Accuracy (CNN): 46.893333333333324
2024-02-02 22:05:08,454 [trainer.py] => Average Accuracy (CNN): 46.893333333333324 

2024-02-02 22:05:08,457 [trainer.py] => All params: 171816392
2024-02-02 22:05:08,460 [trainer.py] => Trainable params: 122980
2024-02-02 22:05:08,460 [l2p_self_training.py] => Learning on 60-70
3250
2024-02-02 22:05:08,517 [l2p_self_training.py] => train dataset length: 250
Task 6, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.108, Train_accy 93.20, Test_accy 15.76: 100%|██████████████| 5/5 [00:29<00:00,  5.95s/it]
2024-02-02 22:05:38,248 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.108, Train_accy 93.20, Test_accy 15.76
2024-02-02 22:05:38,254 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:05:48,485 [l2p_self_training.py] => wrong labeled samples count: 1450
2024-02-02 22:05:48,486 [l2p_self_training.py] => previous label on future samples count: 458
2024-02-02 22:05:48,486 [l2p_self_training.py] => it label on future sample count: 1
2024-02-02 22:05:48,486 [l2p_self_training.py] => previous label on it sample count: 991
2024-02-02 22:05:48,486 [l2p_self_training.py] => 1520 unlabeled samples will be pseudo labeled
2024-02-02 22:05:48,486 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:05:48,486 [toolkit.py] => Pseudo Accuracy: 0.046052631578947366
2024-02-02 22:05:48,488 [l2p_self_training.py] => train dataset length: 1770
Task 6, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.209, Train_accy 83.11, Test_accy 13.61: 100%|███████████████| 5/5 [01:05<00:00, 13.06s/it]
2024-02-02 22:06:53,790 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.209, Train_accy 83.11, Test_accy 13.61
2024-02-02 22:06:53,796 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:07:04,089 [l2p_self_training.py] => wrong labeled samples count: 1326
2024-02-02 22:07:04,094 [l2p_self_training.py] => previous label on future samples count: 427
2024-02-02 22:07:04,094 [l2p_self_training.py] => it label on future sample count: 3
2024-02-02 22:07:04,094 [l2p_self_training.py] => previous label on it sample count: 895
2024-02-02 22:07:04,094 [l2p_self_training.py] => 1532 unlabeled samples will be pseudo labeled
2024-02-02 22:07:04,094 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:07:04,095 [toolkit.py] => Pseudo Accuracy: 0.13446475195822455
2024-02-02 22:07:04,099 [l2p_self_training.py] => train dataset length: 1782
Task 6, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.041, Train_accy 87.60, Test_accy 14.30: 100%|███████████████| 5/5 [01:05<00:00, 13.06s/it]
2024-02-02 22:08:09,423 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.041, Train_accy 87.60, Test_accy 14.30
2024-02-02 22:08:09,429 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:08:19,689 [l2p_self_training.py] => wrong labeled samples count: 1143
2024-02-02 22:08:19,690 [l2p_self_training.py] => previous label on future samples count: 466
2024-02-02 22:08:19,690 [l2p_self_training.py] => it label on future sample count: 8
2024-02-02 22:08:19,690 [l2p_self_training.py] => previous label on it sample count: 666
2024-02-02 22:08:19,690 [l2p_self_training.py] => 1583 unlabeled samples will be pseudo labeled
2024-02-02 22:08:19,690 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:08:19,690 [toolkit.py] => Pseudo Accuracy: 0.2779532533164877
2024-02-02 22:08:41,356 [trainer.py] => No NME accuracy.
2024-02-02 22:08:41,357 [trainer.py] => CNN: {'total': 14.3, '00-09': 40.6, '10-19': 10.3, '20-29': 0.0, '30-39': 0.0, '40-49': 0.0, '50-59': 3.3, '60-69': 45.9, 'old': 9.03, 'new': 45.9}
2024-02-02 22:08:41,357 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08, 13.82, 14.3]
2024-02-02 22:08:41,357 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2, 33.65, 29.63]

Average Accuracy (CNN): 42.23714285714285
2024-02-02 22:08:41,357 [trainer.py] => Average Accuracy (CNN): 42.23714285714285 

2024-02-02 22:08:41,361 [trainer.py] => All params: 171816392
2024-02-02 22:08:41,363 [trainer.py] => Trainable params: 122980
2024-02-02 22:08:41,363 [l2p_self_training.py] => Learning on 70-80
3000
2024-02-02 22:08:41,409 [l2p_self_training.py] => train dataset length: 250
Task 7, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.062, Train_accy 92.40, Test_accy 14.75: 100%|██████████████| 5/5 [00:32<00:00,  6.53s/it]
2024-02-02 22:09:14,059 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.062, Train_accy 92.40, Test_accy 14.75
2024-02-02 22:09:14,066 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:09:23,629 [l2p_self_training.py] => wrong labeled samples count: 1433
2024-02-02 22:09:23,635 [l2p_self_training.py] => previous label on future samples count: 301
2024-02-02 22:09:23,635 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:09:23,635 [l2p_self_training.py] => previous label on it sample count: 1132
2024-02-02 22:09:23,635 [l2p_self_training.py] => 1445 unlabeled samples will be pseudo labeled
2024-02-02 22:09:23,635 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:09:23,636 [toolkit.py] => Pseudo Accuracy: 0.008304498269896194
2024-02-02 22:09:23,640 [l2p_self_training.py] => train dataset length: 1695
Task 7, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.202, Train_accy 82.18, Test_accy 13.50: 100%|███████████████| 5/5 [01:06<00:00, 13.27s/it]
2024-02-02 22:10:29,968 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.202, Train_accy 82.18, Test_accy 13.50
2024-02-02 22:10:29,975 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:10:39,595 [l2p_self_training.py] => wrong labeled samples count: 1435
2024-02-02 22:10:39,596 [l2p_self_training.py] => previous label on future samples count: 314
2024-02-02 22:10:39,596 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:10:39,597 [l2p_self_training.py] => previous label on it sample count: 1121
2024-02-02 22:10:39,597 [l2p_self_training.py] => 1470 unlabeled samples will be pseudo labeled
2024-02-02 22:10:39,597 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:10:39,597 [toolkit.py] => Pseudo Accuracy: 0.023809523809523808
2024-02-02 22:10:39,599 [l2p_self_training.py] => train dataset length: 1720
Task 7, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.092, Train_accy 86.45, Test_accy 12.50: 100%|███████████████| 5/5 [01:07<00:00, 13.41s/it]
2024-02-02 22:11:46,659 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.092, Train_accy 86.45, Test_accy 12.50
2024-02-02 22:11:46,666 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:11:56,263 [l2p_self_training.py] => wrong labeled samples count: 1528
2024-02-02 22:11:56,268 [l2p_self_training.py] => previous label on future samples count: 340
2024-02-02 22:11:56,269 [l2p_self_training.py] => it label on future sample count: 2
2024-02-02 22:11:56,269 [l2p_self_training.py] => previous label on it sample count: 1182
2024-02-02 22:11:56,269 [l2p_self_training.py] => 1613 unlabeled samples will be pseudo labeled
2024-02-02 22:11:56,269 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:11:56,269 [toolkit.py] => Pseudo Accuracy: 0.05269683818970862
2024-02-02 22:12:21,195 [trainer.py] => No NME accuracy.
2024-02-02 22:12:21,196 [trainer.py] => CNN: {'total': 12.5, '00-09': 49.4, '10-19': 17.8, '20-29': 0.0, '30-39': 0.0, '40-49': 0.0, '50-59': 1.2, '60-69': 13.2, '70-79': 18.4, 'old': 11.66, 'new': 18.4}
2024-02-02 22:12:21,196 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08, 13.82, 14.3, 12.5]
2024-02-02 22:12:21,196 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2, 33.65, 29.63, 31.32]

Average Accuracy (CNN): 38.519999999999996
2024-02-02 22:12:21,196 [trainer.py] => Average Accuracy (CNN): 38.519999999999996 

2024-02-02 22:12:21,199 [trainer.py] => All params: 171816392
2024-02-02 22:12:21,201 [trainer.py] => Trainable params: 122980
2024-02-02 22:12:21,201 [l2p_self_training.py] => Learning on 80-90
2750
2024-02-02 22:12:21,247 [l2p_self_training.py] => train dataset length: 250
Task 8, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.075, Train_accy 90.40, Test_accy 12.74: 100%|██████████████| 5/5 [00:35<00:00,  7.14s/it]
2024-02-02 22:12:56,973 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.075, Train_accy 90.40, Test_accy 12.74
2024-02-02 22:12:56,980 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:13:05,761 [l2p_self_training.py] => wrong labeled samples count: 1498
2024-02-02 22:13:05,767 [l2p_self_training.py] => previous label on future samples count: 163
2024-02-02 22:13:05,767 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:13:05,767 [l2p_self_training.py] => previous label on it sample count: 1335
2024-02-02 22:13:05,767 [l2p_self_training.py] => 1515 unlabeled samples will be pseudo labeled
2024-02-02 22:13:05,767 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:13:05,767 [toolkit.py] => Pseudo Accuracy: 0.011221122112211221
2024-02-02 22:13:05,771 [l2p_self_training.py] => train dataset length: 1765
Task 8, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.166, Train_accy 83.46, Test_accy 8.77: 100%|████████████████| 5/5 [01:11<00:00, 14.29s/it]
2024-02-02 22:14:17,210 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.166, Train_accy 83.46, Test_accy 8.77
2024-02-02 22:14:17,217 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:14:25,972 [l2p_self_training.py] => wrong labeled samples count: 1465
2024-02-02 22:14:25,978 [l2p_self_training.py] => previous label on future samples count: 155
2024-02-02 22:14:25,978 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:14:25,978 [l2p_self_training.py] => previous label on it sample count: 1310
2024-02-02 22:14:25,978 [l2p_self_training.py] => 1532 unlabeled samples will be pseudo labeled
2024-02-02 22:14:25,978 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:14:25,979 [toolkit.py] => Pseudo Accuracy: 0.043733681462140996
2024-02-02 22:14:25,985 [l2p_self_training.py] => train dataset length: 1782
Task 8, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.017, Train_accy 88.95, Test_accy 7.89: 100%|████████████████| 5/5 [01:11<00:00, 14.37s/it]
2024-02-02 22:15:37,836 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.017, Train_accy 88.95, Test_accy 7.89
2024-02-02 22:15:37,843 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:15:46,644 [l2p_self_training.py] => wrong labeled samples count: 1418
2024-02-02 22:15:46,649 [l2p_self_training.py] => previous label on future samples count: 173
2024-02-02 22:15:46,649 [l2p_self_training.py] => it label on future sample count: 1
2024-02-02 22:15:46,649 [l2p_self_training.py] => previous label on it sample count: 1243
2024-02-02 22:15:46,650 [l2p_self_training.py] => 1565 unlabeled samples will be pseudo labeled
2024-02-02 22:15:46,650 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:15:46,650 [toolkit.py] => Pseudo Accuracy: 0.0939297124600639
2024-02-02 22:16:14,540 [trainer.py] => No NME accuracy.
2024-02-02 22:16:14,541 [trainer.py] => CNN: {'total': 7.89, '00-09': 42.0, '10-19': 6.1, '20-29': 0.0, '30-39': 0.0, '40-49': 0.0, '50-59': 0.0, '60-69': 0.1, '70-79': 0.8, '80-89': 22.0, 'old': 6.12, 'new': 22.0}
2024-02-02 22:16:14,541 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08, 13.82, 14.3, 12.5, 7.89]
2024-02-02 22:16:14,541 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2, 33.65, 29.63, 31.32, 23.84]

Average Accuracy (CNN): 35.11666666666666
2024-02-02 22:16:14,541 [trainer.py] => Average Accuracy (CNN): 35.11666666666666 

2024-02-02 22:16:14,544 [trainer.py] => All params: 171816392
2024-02-02 22:16:14,547 [trainer.py] => Trainable params: 122980
2024-02-02 22:16:14,547 [l2p_self_training.py] => Learning on 90-100
2500
2024-02-02 22:16:14,613 [l2p_self_training.py] => train dataset length: 250
Task 9, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.063, Train_accy 90.40, Test_accy 8.76: 100%|███████████████| 5/5 [00:38<00:00,  7.75s/it]
2024-02-02 22:16:53,388 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.063, Train_accy 90.40, Test_accy 8.76
2024-02-02 22:16:53,395 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:17:01,483 [l2p_self_training.py] => wrong labeled samples count: 1403
2024-02-02 22:17:01,488 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 22:17:01,488 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:17:01,488 [l2p_self_training.py] => previous label on it sample count: 1403
2024-02-02 22:17:01,489 [l2p_self_training.py] => 1422 unlabeled samples will be pseudo labeled
2024-02-02 22:17:01,489 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:17:01,489 [toolkit.py] => Pseudo Accuracy: 0.013361462728551337
2024-02-02 22:17:01,495 [l2p_self_training.py] => train dataset length: 1672
Task 9, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.202, Train_accy 81.70, Test_accy 6.75: 100%|████████████████| 5/5 [01:11<00:00, 14.36s/it]
2024-02-02 22:18:13,301 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.202, Train_accy 81.70, Test_accy 6.75
2024-02-02 22:18:13,307 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:18:21,266 [l2p_self_training.py] => wrong labeled samples count: 1055
2024-02-02 22:18:21,270 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 22:18:21,270 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:18:21,270 [l2p_self_training.py] => previous label on it sample count: 1052
2024-02-02 22:18:21,271 [l2p_self_training.py] => 1165 unlabeled samples will be pseudo labeled
2024-02-02 22:18:21,271 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:18:21,271 [toolkit.py] => Pseudo Accuracy: 0.0944206008583691
2024-02-02 22:18:21,275 [l2p_self_training.py] => train dataset length: 1415
Task 9, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.066, Train_accy 86.22, Test_accy 5.61: 100%|████████████████| 5/5 [01:05<00:00, 13.19s/it]
2024-02-02 22:19:27,206 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.066, Train_accy 86.22, Test_accy 5.61
2024-02-02 22:19:27,212 [l2p_self_training.py] => pseudo labeling start
2024-02-02 22:19:35,191 [l2p_self_training.py] => wrong labeled samples count: 972
2024-02-02 22:19:35,195 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-02 22:19:35,195 [l2p_self_training.py] => it label on future sample count: 0
2024-02-02 22:19:35,195 [l2p_self_training.py] => previous label on it sample count: 971
2024-02-02 22:19:35,195 [l2p_self_training.py] => 1142 unlabeled samples will be pseudo labeled
2024-02-02 22:19:35,195 [l2p_self_training.py] => pseudo labeling finish
2024-02-02 22:19:35,196 [toolkit.py] => Pseudo Accuracy: 0.14886164623467601
2024-02-02 22:20:06,150 [trainer.py] => No NME accuracy.
2024-02-02 22:20:06,150 [trainer.py] => CNN: {'total': 5.61, '00-09': 30.5, '10-19': 1.6, '20-29': 0.0, '30-39': 0.0, '40-49': 0.0, '50-59': 0.0, '60-69': 0.0, '70-79': 0.0, '80-89': 0.5, '90-99': 23.5, 'old': 3.62, 'new': 23.5}
2024-02-02 22:20:06,150 [trainer.py] => CNN top1 curve: [94.2, 66.6, 48.83, 34.83, 23.08, 13.82, 14.3, 12.5, 7.89, 5.61]
2024-02-02 22:20:06,151 [trainer.py] => CNN top5 curve: [99.7, 95.35, 79.2, 60.08, 49.2, 33.65, 29.63, 31.32, 23.84, 18.08]

Average Accuracy (CNN): 32.166
2024-02-02 22:20:06,151 [trainer.py] => Average Accuracy (CNN): 32.166 

Accuracy Matrix (CNN):
[[94.2 72.7 72.2 60.3 56.7 49.9 40.6 49.4 42.  30.5]
 [ 0.  60.5 32.8 26.7 16.8 13.3 10.3 17.8  6.1  1.6]
 [ 0.   0.  41.5  9.2  5.9  0.3  0.   0.   0.   0. ]
 [ 0.   0.   0.  43.1 13.2  0.2  0.   0.   0.   0. ]
 [ 0.   0.   0.   0.  22.8  1.5  0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.  17.7  3.3  1.2  0.   0. ]
 [ 0.   0.   0.   0.   0.   0.  45.9 13.2  0.1  0. ]
 [ 0.   0.   0.   0.   0.   0.   0.  18.4  0.8  0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.  22.   0.5]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.  23.5]]
2024-02-02 22:20:06,153 [trainer.py] => Forgetting (CNN): 37.05555555555556

######################################################################################
======================================================================================
######################################################################################
原l2p+Self-Training方法在有future oot数据时伪标签打的对错（分类头为 0:total classes）

(lhz-torch-2.0) lhz@csgpu-SYS-4029GP-TRT:~/code/LAMDA-PILOT$ python main.py --config=./exps/l2p_self_training.json
2024-02-28 10:24:32,628 [trainer.py] => config: ./exps/l2p_self_training.json
2024-02-28 10:24:32,629 [trainer.py] => prefix:  
2024-02-28 10:24:32,629 [trainer.py] => dataset: cifar224
2024-02-28 10:24:32,629 [trainer.py] => memory_size: 0
2024-02-28 10:24:32,629 [trainer.py] => memory_per_class: 0
2024-02-28 10:24:32,629 [trainer.py] => fixed_memory: False
2024-02-28 10:24:32,629 [trainer.py] => shuffle: True
2024-02-28 10:24:32,629 [trainer.py] => init_cls: 10
2024-02-28 10:24:32,629 [trainer.py] => increment: 10
2024-02-28 10:24:32,629 [trainer.py] => model_name: l2p_self_training
2024-02-28 10:24:32,629 [trainer.py] => backbone_type: vit_base_patch16_224_l2p
2024-02-28 10:24:32,629 [trainer.py] => get_original_backbone: True
2024-02-28 10:24:32,629 [trainer.py] => device: [device(type='cuda', index=5)]
2024-02-28 10:24:32,629 [trainer.py] => seed: 1993
2024-02-28 10:24:32,629 [trainer.py] => tuned_epoch: 5
2024-02-28 10:24:32,629 [trainer.py] => init_lr: 0.001875
2024-02-28 10:24:32,629 [trainer.py] => batch_size: 16
2024-02-28 10:24:32,629 [trainer.py] => weight_decay: 0
2024-02-28 10:24:32,629 [trainer.py] => min_lr: 1e-05
2024-02-28 10:24:32,629 [trainer.py] => optimizer: adam
2024-02-28 10:24:32,629 [trainer.py] => scheduler: constant
2024-02-28 10:24:32,629 [trainer.py] => reinit_optimizer: True
2024-02-28 10:24:32,629 [trainer.py] => global_pool: token
2024-02-28 10:24:32,629 [trainer.py] => head_type: prompt
2024-02-28 10:24:32,629 [trainer.py] => freeze: ['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed']
2024-02-28 10:24:32,629 [trainer.py] => pretrained: True
2024-02-28 10:24:32,629 [trainer.py] => drop: 0.0
2024-02-28 10:24:32,629 [trainer.py] => drop_path: 0.0
2024-02-28 10:24:32,629 [trainer.py] => prompt_pool: True
2024-02-28 10:24:32,630 [trainer.py] => pool_size: 10
2024-02-28 10:24:32,630 [trainer.py] => length: 5
2024-02-28 10:24:32,630 [trainer.py] => top_k: 5
2024-02-28 10:24:32,630 [trainer.py] => initializer: uniform
2024-02-28 10:24:32,630 [trainer.py] => prompt_key: True
2024-02-28 10:24:32,630 [trainer.py] => prompt_key_init: uniform
2024-02-28 10:24:32,630 [trainer.py] => use_prompt_mask: False
2024-02-28 10:24:32,630 [trainer.py] => shared_prompt_pool: False
2024-02-28 10:24:32,630 [trainer.py] => shared_prompt_key: False
2024-02-28 10:24:32,630 [trainer.py] => batchwise_prompt: True
2024-02-28 10:24:32,630 [trainer.py] => embedding_key: cls
2024-02-28 10:24:32,630 [trainer.py] => predefined_key: 
2024-02-28 10:24:32,630 [trainer.py] => pull_constraint: True
2024-02-28 10:24:32,630 [trainer.py] => pull_constraint_coeff: 0.1
2024-02-28 10:24:32,630 [trainer.py] => semi_supervised_mode: True
2024-02-28 10:24:32,630 [trainer.py] => labeled_ratio: 0.05
2024-02-28 10:24:32,630 [trainer.py] => unlabeled_data_distribution_mode: future_oot
2024-02-28 10:24:32,630 [trainer.py] => confidence_threshold: 0.9
2024-02-28 10:24:32,630 [trainer.py] => max_self_training_iteration: 3
Files already downloaded and verified
Files already downloaded and verified
2024-02-28 10:24:34,480 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-02-28 10:24:36,295 [vision_transformer_l2p.py] => Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 222, 768])
2024-02-28 10:24:36,295 [vision_transformer_l2p.py] => Position embedding grid-size from [14, 14] to (14, 14)
2024-02-28 10:24:38,550 [l2p_self_training.py] => 85,940,836 model total parameters.
2024-02-28 10:24:38,550 [l2p_self_training.py] => 122,980 model training parameters.
2024-02-28 10:24:38,551 [l2p_self_training.py] => prompt.prompt: 38400
2024-02-28 10:24:38,551 [l2p_self_training.py] => prompt.prompt_key: 7680
2024-02-28 10:24:38,551 [l2p_self_training.py] => head.weight: 76800
2024-02-28 10:24:38,551 [l2p_self_training.py] => head.bias: 100
2024-02-28 10:24:38,552 [trainer.py] => All params: 171816392
2024-02-28 10:24:38,553 [trainer.py] => Trainable params: 122980
2024-02-28 10:24:38,553 [l2p_self_training.py] => Learning on 0-10
4750
2024-02-28 10:24:39,225 [l2p_self_training.py] => train dataset length: 250
Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.515, Train_accy 84.00, Test_accy 89.50: 100%|███████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.88s/it]
2024-02-28 10:24:58,640 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 0 => Loss 0.515, Train_accy 84.00, Test_accy 89.50
2024-02-28 10:24:58,642 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:25:26,412 [l2p_self_training.py] => wrong labeled samples count: 249
2024-02-28 10:25:26,414 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 10:25:26,414 [l2p_self_training.py] => it label on future sample count: 233
2024-02-28 10:25:26,414 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-28 10:25:26,414 [l2p_self_training.py] => 1274 unlabeled samples will be pseudo labeled
2024-02-28 10:25:26,414 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:25:26,415 [toolkit.py] => Pseudo Accuracy: 0.804552590266876
[0 1 2 3 4 5 6 7 8 9]
2024-02-28 10:25:26,418 [l2p_self_training.py] => train dataset length: 1524
Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.150, Train_accy 88.39, Test_accy 93.00: 100%|███████████████████████████████████████████████████| 5/5 [01:15<00:00, 15.14s/it]
2024-02-28 10:26:42,107 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 1 => Loss 0.150, Train_accy 88.39, Test_accy 93.00
2024-02-28 10:26:42,110 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:27:09,913 [l2p_self_training.py] => wrong labeled samples count: 792
2024-02-28 10:27:09,915 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 10:27:09,915 [l2p_self_training.py] => it label on future sample count: 731
2024-02-28 10:27:09,915 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-28 10:27:09,915 [l2p_self_training.py] => 2460 unlabeled samples will be pseudo labeled
2024-02-28 10:27:09,915 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:27:09,915 [toolkit.py] => Pseudo Accuracy: 0.6780487804878049
[0 1 2 3 4 5 6 7 8 9]
2024-02-28 10:27:09,921 [l2p_self_training.py] => train dataset length: 2710
Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.076, Train_accy 87.53, Test_accy 94.20: 100%|███████████████████████████████████████████████████| 5/5 [02:09<00:00, 25.90s/it]
2024-02-28 10:29:19,436 [l2p_self_training.py] => Task 0, Epoch 5/5, Self_training_Iteration: 2 => Loss 0.076, Train_accy 87.53, Test_accy 94.20
2024-02-28 10:29:19,439 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:29:47,403 [l2p_self_training.py] => wrong labeled samples count: 1127
2024-02-28 10:29:47,405 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 10:29:47,405 [l2p_self_training.py] => it label on future sample count: 1058
2024-02-28 10:29:47,405 [l2p_self_training.py] => previous label on it sample count: 0
2024-02-28 10:29:47,405 [l2p_self_training.py] => 2925 unlabeled samples will be pseudo labeled
2024-02-28 10:29:47,405 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:29:47,405 [toolkit.py] => Pseudo Accuracy: 0.6147008547008547
[0 1 2 3 4 5 6 7 8 9]
2024-02-28 10:29:53,517 [trainer.py] => No NME accuracy.
2024-02-28 10:29:53,517 [trainer.py] => CNN: {'total': 94.2, '00-09': 94.2, 'old': 0, 'new': 94.2}
2024-02-28 10:29:53,517 [trainer.py] => CNN top1 curve: [94.2]
2024-02-28 10:29:53,518 [trainer.py] => CNN top5 curve: [99.7]

Average Accuracy (CNN): 94.2
2024-02-28 10:29:53,518 [trainer.py] => Average Accuracy (CNN): 94.2 

2024-02-28 10:29:53,520 [trainer.py] => All params: 171816392
2024-02-28 10:29:53,521 [trainer.py] => Trainable params: 122980
2024-02-28 10:29:53,522 [l2p_self_training.py] => Learning on 10-20
4500
2024-02-28 10:29:53,597 [l2p_self_training.py] => train dataset length: 250
Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.027, Train_accy 91.20, Test_accy 74.30: 100%|██████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.03s/it]
2024-02-28 10:30:18,736 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.027, Train_accy 91.20, Test_accy 74.30
2024-02-28 10:30:18,740 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:30:45,089 [l2p_self_training.py] => wrong labeled samples count: 1265
2024-02-28 10:30:45,094 [l2p_self_training.py] => previous label on future samples count: 743
2024-02-28 10:30:45,094 [l2p_self_training.py] => it label on future sample count: 48
2024-02-28 10:30:45,094 [l2p_self_training.py] => previous label on it sample count: 471
2024-02-28 10:30:45,094 [l2p_self_training.py] => 1763 unlabeled samples will be pseudo labeled
2024-02-28 10:30:45,094 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:30:45,095 [toolkit.py] => Pseudo Accuracy: 0.2824730572887124
[10 11 12 13 14 15 16 17 18 19]
2024-02-28 10:30:45,096 [l2p_self_training.py] => train dataset length: 799
Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.073, Train_accy 91.99, Test_accy 79.85: 100%|██████████████████████████████████████████████████| 5/5 [00:49<00:00,  9.99s/it]
2024-02-28 10:31:35,040 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.073, Train_accy 91.99, Test_accy 79.85
2024-02-28 10:31:35,042 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:32:01,439 [l2p_self_training.py] => wrong labeled samples count: 1187
2024-02-28 10:32:01,441 [l2p_self_training.py] => previous label on future samples count: 682
2024-02-28 10:32:01,442 [l2p_self_training.py] => it label on future sample count: 145
2024-02-28 10:32:01,442 [l2p_self_training.py] => previous label on it sample count: 343
2024-02-28 10:32:01,442 [l2p_self_training.py] => 2022 unlabeled samples will be pseudo labeled
2024-02-28 10:32:01,442 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:32:01,442 [toolkit.py] => Pseudo Accuracy: 0.41295746785361026
[10 11 12 13 14 15 16 17 18 19]
2024-02-28 10:32:01,446 [l2p_self_training.py] => train dataset length: 1247
Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.048, Train_accy 92.14, Test_accy 82.75: 100%|██████████████████████████████████████████████████| 5/5 [01:09<00:00, 13.88s/it]
2024-02-28 10:33:10,851 [l2p_self_training.py] => Task 1, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.048, Train_accy 92.14, Test_accy 82.75
2024-02-28 10:33:10,853 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:33:37,220 [l2p_self_training.py] => wrong labeled samples count: 921
2024-02-28 10:33:37,222 [l2p_self_training.py] => previous label on future samples count: 456
2024-02-28 10:33:37,222 [l2p_self_training.py] => it label on future sample count: 266
2024-02-28 10:33:37,222 [l2p_self_training.py] => previous label on it sample count: 151
2024-02-28 10:33:37,223 [l2p_self_training.py] => 2120 unlabeled samples will be pseudo labeled
2024-02-28 10:33:37,223 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:33:37,224 [toolkit.py] => Pseudo Accuracy: 0.565566037735849
[10 11 12 13 14 15 16 17 18 19]
2024-02-28 10:33:49,036 [trainer.py] => No NME accuracy.
2024-02-28 10:33:49,037 [trainer.py] => CNN: {'total': 82.75, '00-09': 90.0, '10-19': 75.5, 'old': 90.0, 'new': 75.5}
2024-02-28 10:33:49,037 [trainer.py] => CNN top1 curve: [94.2, 82.75]
2024-02-28 10:33:49,037 [trainer.py] => CNN top5 curve: [99.7, 97.15]

Average Accuracy (CNN): 88.475
2024-02-28 10:33:49,037 [trainer.py] => Average Accuracy (CNN): 88.475 

2024-02-28 10:33:49,040 [trainer.py] => All params: 171816392
2024-02-28 10:33:49,041 [trainer.py] => Trainable params: 122980
2024-02-28 10:33:49,041 [l2p_self_training.py] => Learning on 20-30
4250
2024-02-28 10:33:49,199 [l2p_self_training.py] => train dataset length: 250
Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.112, Train_accy 92.00, Test_accy 73.23: 100%|██████████████████████████████████████████████████| 5/5 [00:30<00:00,  6.07s/it]
2024-02-28 10:34:19,547 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.112, Train_accy 92.00, Test_accy 73.23
2024-02-28 10:34:19,550 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:34:44,408 [l2p_self_training.py] => wrong labeled samples count: 1109
2024-02-28 10:34:44,410 [l2p_self_training.py] => previous label on future samples count: 634
2024-02-28 10:34:44,411 [l2p_self_training.py] => it label on future sample count: 1
2024-02-28 10:34:44,411 [l2p_self_training.py] => previous label on it sample count: 474
2024-02-28 10:34:44,411 [l2p_self_training.py] => 1582 unlabeled samples will be pseudo labeled
2024-02-28 10:34:44,411 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:34:44,411 [toolkit.py] => Pseudo Accuracy: 0.29898862199747156
[20 21 22 23 24 25 26 28 29]
2024-02-28 10:34:44,413 [l2p_self_training.py] => train dataset length: 724
Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.161, Train_accy 95.17, Test_accy 78.37: 100%|██████████████████████████████████████████████████| 5/5 [00:51<00:00, 10.33s/it]
2024-02-28 10:35:36,086 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.161, Train_accy 95.17, Test_accy 78.37
2024-02-28 10:35:36,089 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:36:00,820 [l2p_self_training.py] => wrong labeled samples count: 1074
2024-02-28 10:36:00,824 [l2p_self_training.py] => previous label on future samples count: 667
2024-02-28 10:36:00,824 [l2p_self_training.py] => it label on future sample count: 14
2024-02-28 10:36:00,824 [l2p_self_training.py] => previous label on it sample count: 391
2024-02-28 10:36:00,824 [l2p_self_training.py] => 2033 unlabeled samples will be pseudo labeled
2024-02-28 10:36:00,824 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:36:00,825 [toolkit.py] => Pseudo Accuracy: 0.4717166748647319
[20 21 22 23 24 25 26 27 28 29]
2024-02-28 10:36:00,830 [l2p_self_training.py] => train dataset length: 1225
Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.116, Train_accy 93.88, Test_accy 80.77: 100%|██████████████████████████████████████████████████| 5/5 [01:14<00:00, 14.81s/it]
2024-02-28 10:37:14,870 [l2p_self_training.py] => Task 2, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.116, Train_accy 93.88, Test_accy 80.77
2024-02-28 10:37:14,875 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:37:39,708 [l2p_self_training.py] => wrong labeled samples count: 921
2024-02-28 10:37:39,711 [l2p_self_training.py] => previous label on future samples count: 542
2024-02-28 10:37:39,711 [l2p_self_training.py] => it label on future sample count: 50
2024-02-28 10:37:39,712 [l2p_self_training.py] => previous label on it sample count: 325
2024-02-28 10:37:39,712 [l2p_self_training.py] => 2281 unlabeled samples will be pseudo labeled
2024-02-28 10:37:39,712 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:37:39,712 [toolkit.py] => Pseudo Accuracy: 0.5962297238053486
[20 21 22 23 24 25 26 28 29]
2024-02-28 10:37:57,355 [trainer.py] => No NME accuracy.
2024-02-28 10:37:57,355 [trainer.py] => CNN: {'total': 80.77, '00-09': 90.2, '10-19': 76.2, '20-29': 75.9, 'old': 83.2, 'new': 75.9}
2024-02-28 10:37:57,356 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77]
2024-02-28 10:37:57,356 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07]

Average Accuracy (CNN): 85.90666666666665
2024-02-28 10:37:57,356 [trainer.py] => Average Accuracy (CNN): 85.90666666666665 

2024-02-28 10:37:57,357 [trainer.py] => All params: 171816392
2024-02-28 10:37:57,358 [trainer.py] => Trainable params: 122980
2024-02-28 10:37:57,358 [l2p_self_training.py] => Learning on 30-40
4000
2024-02-28 10:37:57,416 [l2p_self_training.py] => train dataset length: 250
Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.086, Train_accy 93.20, Test_accy 70.78: 100%|██████████████████████████████████████████████████| 5/5 [00:36<00:00,  7.25s/it]
2024-02-28 10:38:33,684 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.086, Train_accy 93.20, Test_accy 70.78
2024-02-28 10:38:33,687 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:38:57,027 [l2p_self_training.py] => wrong labeled samples count: 1264
2024-02-28 10:38:57,029 [l2p_self_training.py] => previous label on future samples count: 477
2024-02-28 10:38:57,029 [l2p_self_training.py] => it label on future sample count: 7
2024-02-28 10:38:57,029 [l2p_self_training.py] => previous label on it sample count: 780
2024-02-28 10:38:57,029 [l2p_self_training.py] => 1682 unlabeled samples will be pseudo labeled
2024-02-28 10:38:57,029 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:38:57,029 [toolkit.py] => Pseudo Accuracy: 0.24851367419738407
[33 34 35 36 38 39]
2024-02-28 10:38:57,031 [l2p_self_training.py] => train dataset length: 675
Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.173, Train_accy 94.37, Test_accy 72.88: 100%|██████████████████████████████████████████████████| 5/5 [00:55<00:00, 11.06s/it]
2024-02-28 10:39:52,339 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.173, Train_accy 94.37, Test_accy 72.88
2024-02-28 10:39:52,341 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:40:15,697 [l2p_self_training.py] => wrong labeled samples count: 1116
2024-02-28 10:40:15,700 [l2p_self_training.py] => previous label on future samples count: 439
2024-02-28 10:40:15,700 [l2p_self_training.py] => it label on future sample count: 29
2024-02-28 10:40:15,700 [l2p_self_training.py] => previous label on it sample count: 645
2024-02-28 10:40:15,700 [l2p_self_training.py] => 1853 unlabeled samples will be pseudo labeled
2024-02-28 10:40:15,700 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:40:15,701 [toolkit.py] => Pseudo Accuracy: 0.397733405288721
[33 34 35 36 37 38 39]
2024-02-28 10:40:15,704 [l2p_self_training.py] => train dataset length: 1019
Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.131, Train_accy 94.50, Test_accy 74.95: 100%|██████████████████████████████████████████████████| 5/5 [01:10<00:00, 14.11s/it]
2024-02-28 10:41:26,246 [l2p_self_training.py] => Task 3, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.131, Train_accy 94.50, Test_accy 74.95
2024-02-28 10:41:26,250 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:41:49,556 [l2p_self_training.py] => wrong labeled samples count: 925
2024-02-28 10:41:49,558 [l2p_self_training.py] => previous label on future samples count: 384
2024-02-28 10:41:49,558 [l2p_self_training.py] => it label on future sample count: 43
2024-02-28 10:41:49,558 [l2p_self_training.py] => previous label on it sample count: 494
2024-02-28 10:41:49,558 [l2p_self_training.py] => 1796 unlabeled samples will be pseudo labeled
2024-02-28 10:41:49,558 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:41:49,558 [toolkit.py] => Pseudo Accuracy: 0.4849665924276169
[30 31 32 33 34 35 36 37 38 39]
2024-02-28 10:42:12,856 [trainer.py] => No NME accuracy.
2024-02-28 10:42:12,856 [trainer.py] => CNN: {'total': 74.95, '00-09': 89.4, '10-19': 76.3, '20-29': 76.1, '30-39': 58.0, 'old': 80.6, 'new': 58.0}
2024-02-28 10:42:12,857 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95]
2024-02-28 10:42:12,857 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58]

Average Accuracy (CNN): 83.16749999999999
2024-02-28 10:42:12,857 [trainer.py] => Average Accuracy (CNN): 83.16749999999999 

2024-02-28 10:42:12,858 [trainer.py] => All params: 171816392
2024-02-28 10:42:12,859 [trainer.py] => Trainable params: 122980
2024-02-28 10:42:12,859 [l2p_self_training.py] => Learning on 40-50
3750
2024-02-28 10:42:12,916 [l2p_self_training.py] => train dataset length: 250
Task 4, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.136, Train_accy 94.00, Test_accy 69.10: 100%|██████████████████████████████████████████████████| 5/5 [00:42<00:00,  8.47s/it]
2024-02-28 10:42:55,267 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.136, Train_accy 94.00, Test_accy 69.10
2024-02-28 10:42:55,270 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:43:17,138 [l2p_self_training.py] => wrong labeled samples count: 768
2024-02-28 10:43:17,141 [l2p_self_training.py] => previous label on future samples count: 350
2024-02-28 10:43:17,141 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 10:43:17,141 [l2p_self_training.py] => previous label on it sample count: 418
2024-02-28 10:43:17,141 [l2p_self_training.py] => 1011 unlabeled samples will be pseudo labeled
2024-02-28 10:43:17,141 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:43:17,141 [toolkit.py] => Pseudo Accuracy: 0.2403560830860534
[40 41 42 43 44 46 47 48 49]
2024-02-28 10:43:17,143 [l2p_self_training.py] => train dataset length: 493
Task 4, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.203, Train_accy 95.74, Test_accy 71.38: 100%|██████████████████████████████████████████████████| 5/5 [00:52<00:00, 10.54s/it]
2024-02-28 10:44:09,839 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.203, Train_accy 95.74, Test_accy 71.38
2024-02-28 10:44:09,842 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:44:31,708 [l2p_self_training.py] => wrong labeled samples count: 778
2024-02-28 10:44:31,710 [l2p_self_training.py] => previous label on future samples count: 344
2024-02-28 10:44:31,710 [l2p_self_training.py] => it label on future sample count: 9
2024-02-28 10:44:31,710 [l2p_self_training.py] => previous label on it sample count: 423
2024-02-28 10:44:31,710 [l2p_self_training.py] => 1377 unlabeled samples will be pseudo labeled
2024-02-28 10:44:31,711 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:44:31,711 [toolkit.py] => Pseudo Accuracy: 0.43500363108206247
[40 41 42 43 44 46 47 48 49]
2024-02-28 10:44:31,715 [l2p_self_training.py] => train dataset length: 860
Task 4, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.151, Train_accy 95.35, Test_accy 73.46: 100%|██████████████████████████████████████████████████| 5/5 [01:09<00:00, 13.83s/it]
2024-02-28 10:45:40,870 [l2p_self_training.py] => Task 4, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.151, Train_accy 95.35, Test_accy 73.46
2024-02-28 10:45:40,872 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:46:02,859 [l2p_self_training.py] => wrong labeled samples count: 547
2024-02-28 10:46:02,861 [l2p_self_training.py] => previous label on future samples count: 303
2024-02-28 10:46:02,861 [l2p_self_training.py] => it label on future sample count: 24
2024-02-28 10:46:02,861 [l2p_self_training.py] => previous label on it sample count: 209
2024-02-28 10:46:02,861 [l2p_self_training.py] => 1476 unlabeled samples will be pseudo labeled
2024-02-28 10:46:02,862 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:46:02,862 [toolkit.py] => Pseudo Accuracy: 0.6294037940379403
[40 41 42 43 44 45 46 47 48 49]
2024-02-28 10:46:32,014 [trainer.py] => No NME accuracy.
2024-02-28 10:46:32,014 [trainer.py] => CNN: {'total': 73.46, '00-09': 86.7, '10-19': 77.2, '20-29': 76.7, '30-39': 59.1, '40-49': 67.6, 'old': 74.92, 'new': 67.6}
2024-02-28 10:46:32,015 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46]
2024-02-28 10:46:32,015 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92]

Average Accuracy (CNN): 81.22599999999998
2024-02-28 10:46:32,015 [trainer.py] => Average Accuracy (CNN): 81.22599999999998 

2024-02-28 10:46:32,017 [trainer.py] => All params: 171816392
2024-02-28 10:46:32,019 [trainer.py] => Trainable params: 122980
2024-02-28 10:46:32,020 [l2p_self_training.py] => Learning on 50-60
3500
2024-02-28 10:46:32,096 [l2p_self_training.py] => train dataset length: 250
Task 5, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.057, Train_accy 89.20, Test_accy 68.67: 100%|██████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.66s/it]
2024-02-28 10:47:20,395 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.057, Train_accy 89.20, Test_accy 68.67
2024-02-28 10:47:20,399 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:47:41,107 [l2p_self_training.py] => wrong labeled samples count: 598
2024-02-28 10:47:41,108 [l2p_self_training.py] => previous label on future samples count: 262
2024-02-28 10:47:41,108 [l2p_self_training.py] => it label on future sample count: 9
2024-02-28 10:47:41,108 [l2p_self_training.py] => previous label on it sample count: 327
2024-02-28 10:47:41,108 [l2p_self_training.py] => 933 unlabeled samples will be pseudo labeled
2024-02-28 10:47:41,108 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:47:41,109 [toolkit.py] => Pseudo Accuracy: 0.3590568060021436
[50 51 52 53 55 56 57 58 59]
2024-02-28 10:47:41,115 [l2p_self_training.py] => train dataset length: 594
Task 5, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.234, Train_accy 96.46, Test_accy 71.33: 100%|██████████████████████████████████████████████████| 5/5 [01:04<00:00, 12.83s/it]
2024-02-28 10:48:45,252 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.234, Train_accy 96.46, Test_accy 71.33
2024-02-28 10:48:45,256 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:49:05,711 [l2p_self_training.py] => wrong labeled samples count: 496
2024-02-28 10:49:05,712 [l2p_self_training.py] => previous label on future samples count: 239
2024-02-28 10:49:05,712 [l2p_self_training.py] => it label on future sample count: 16
2024-02-28 10:49:05,712 [l2p_self_training.py] => previous label on it sample count: 238
2024-02-28 10:49:05,712 [l2p_self_training.py] => 1233 unlabeled samples will be pseudo labeled
2024-02-28 10:49:05,712 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:49:05,712 [toolkit.py] => Pseudo Accuracy: 0.5977291159772912
[50 51 52 53 54 55 56 57 58 59]
2024-02-28 10:49:05,718 [l2p_self_training.py] => train dataset length: 1006
Task 5, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.145, Train_accy 94.53, Test_accy 72.08: 100%|██████████████████████████████████████████████████| 5/5 [01:21<00:00, 16.38s/it]
2024-02-28 10:50:27,599 [l2p_self_training.py] => Task 5, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.145, Train_accy 94.53, Test_accy 72.08
2024-02-28 10:50:27,601 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:50:48,104 [l2p_self_training.py] => wrong labeled samples count: 426
2024-02-28 10:50:48,105 [l2p_self_training.py] => previous label on future samples count: 232
2024-02-28 10:50:48,105 [l2p_self_training.py] => it label on future sample count: 48
2024-02-28 10:50:48,106 [l2p_self_training.py] => previous label on it sample count: 130
2024-02-28 10:50:48,106 [l2p_self_training.py] => 1514 unlabeled samples will be pseudo labeled
2024-02-28 10:50:48,106 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:50:48,106 [toolkit.py] => Pseudo Accuracy: 0.7186261558784677
[50 51 52 53 54 55 56 57 58 59]
2024-02-28 10:51:22,801 [trainer.py] => No NME accuracy.
2024-02-28 10:51:22,801 [trainer.py] => CNN: {'total': 72.08, '00-09': 80.6, '10-19': 77.9, '20-29': 76.8, '30-39': 58.8, '40-49': 63.7, '50-59': 74.7, 'old': 71.56, 'new': 74.7}
2024-02-28 10:51:22,801 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46, 72.08]
2024-02-28 10:51:22,801 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92, 95.17]

Average Accuracy (CNN): 79.70166666666665
2024-02-28 10:51:22,801 [trainer.py] => Average Accuracy (CNN): 79.70166666666665 

2024-02-28 10:51:22,803 [trainer.py] => All params: 171816392
2024-02-28 10:51:22,804 [trainer.py] => Trainable params: 122980
2024-02-28 10:51:22,804 [l2p_self_training.py] => Learning on 60-70
3250
2024-02-28 10:51:22,871 [l2p_self_training.py] => train dataset length: 250
Task 6, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.107, Train_accy 93.60, Test_accy 69.31: 100%|██████████████████████████████████████████████████| 5/5 [00:53<00:00, 10.64s/it]
2024-02-28 10:52:16,082 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.107, Train_accy 93.60, Test_accy 69.31
2024-02-28 10:52:16,084 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:52:35,124 [l2p_self_training.py] => wrong labeled samples count: 554
2024-02-28 10:52:35,127 [l2p_self_training.py] => previous label on future samples count: 208
2024-02-28 10:52:35,127 [l2p_self_training.py] => it label on future sample count: 1
2024-02-28 10:52:35,127 [l2p_self_training.py] => previous label on it sample count: 345
2024-02-28 10:52:35,127 [l2p_self_training.py] => 1033 unlabeled samples will be pseudo labeled
2024-02-28 10:52:35,127 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:52:35,127 [toolkit.py] => Pseudo Accuracy: 0.46369796708615685
[60 61 62 63 64 65 66 67 68]
2024-02-28 10:52:35,129 [l2p_self_training.py] => train dataset length: 730
Task 6, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.152, Train_accy 95.21, Test_accy 70.79: 100%|██████████████████████████████████████████████████| 5/5 [01:15<00:00, 15.04s/it]
2024-02-28 10:53:50,310 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.152, Train_accy 95.21, Test_accy 70.79
2024-02-28 10:53:50,314 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:54:09,560 [l2p_self_training.py] => wrong labeled samples count: 497
2024-02-28 10:54:09,562 [l2p_self_training.py] => previous label on future samples count: 206
2024-02-28 10:54:09,563 [l2p_self_training.py] => it label on future sample count: 6
2024-02-28 10:54:09,563 [l2p_self_training.py] => previous label on it sample count: 283
2024-02-28 10:54:09,563 [l2p_self_training.py] => 1250 unlabeled samples will be pseudo labeled
2024-02-28 10:54:09,563 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:54:09,563 [toolkit.py] => Pseudo Accuracy: 0.6024
[60 61 62 63 64 65 66 67 68]
2024-02-28 10:54:09,566 [l2p_self_training.py] => train dataset length: 1011
Task 6, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.156, Train_accy 95.15, Test_accy 71.17: 100%|██████████████████████████████████████████████████| 5/5 [01:27<00:00, 17.58s/it]
2024-02-28 10:55:37,444 [l2p_self_training.py] => Task 6, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.156, Train_accy 95.15, Test_accy 71.17
2024-02-28 10:55:37,447 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:55:56,723 [l2p_self_training.py] => wrong labeled samples count: 424
2024-02-28 10:55:56,724 [l2p_self_training.py] => previous label on future samples count: 163
2024-02-28 10:55:56,724 [l2p_self_training.py] => it label on future sample count: 31
2024-02-28 10:55:56,724 [l2p_self_training.py] => previous label on it sample count: 217
2024-02-28 10:55:56,724 [l2p_self_training.py] => 1527 unlabeled samples will be pseudo labeled
2024-02-28 10:55:56,724 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:55:56,724 [toolkit.py] => Pseudo Accuracy: 0.722331368696791
[60 61 62 63 64 65 66 67 68]
2024-02-28 10:56:37,275 [trainer.py] => No NME accuracy.
2024-02-28 10:56:37,275 [trainer.py] => CNN: {'total': 71.17, '00-09': 77.9, '10-19': 77.4, '20-29': 77.0, '30-39': 59.4, '40-49': 64.0, '50-59': 71.6, '60-69': 70.9, 'old': 71.22, 'new': 70.9}
2024-02-28 10:56:37,275 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46, 72.08, 71.17]
2024-02-28 10:56:37,275 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92, 95.17, 94.79]

Average Accuracy (CNN): 78.48285714285713
2024-02-28 10:56:37,275 [trainer.py] => Average Accuracy (CNN): 78.48285714285713 

2024-02-28 10:56:37,277 [trainer.py] => All params: 171816392
2024-02-28 10:56:37,278 [trainer.py] => Trainable params: 122980
2024-02-28 10:56:37,279 [l2p_self_training.py] => Learning on 70-80
3000
2024-02-28 10:56:37,352 [l2p_self_training.py] => train dataset length: 250
Task 7, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.073, Train_accy 93.60, Test_accy 65.92: 100%|██████████████████████████████████████████████████| 5/5 [00:58<00:00, 11.79s/it]
2024-02-28 10:57:36,295 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.073, Train_accy 93.60, Test_accy 65.92
2024-02-28 10:57:36,297 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:57:53,853 [l2p_self_training.py] => wrong labeled samples count: 501
2024-02-28 10:57:53,855 [l2p_self_training.py] => previous label on future samples count: 136
2024-02-28 10:57:53,855 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 10:57:53,855 [l2p_self_training.py] => previous label on it sample count: 365
2024-02-28 10:57:53,855 [l2p_self_training.py] => 593 unlabeled samples will be pseudo labeled
2024-02-28 10:57:53,855 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:57:53,855 [toolkit.py] => Pseudo Accuracy: 0.1551433389544688
[70 71 72 73 74 75 77]
2024-02-28 10:57:53,857 [l2p_self_training.py] => train dataset length: 342
Task 7, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.206, Train_accy 94.74, Test_accy 67.44: 100%|██████████████████████████████████████████████████| 5/5 [01:03<00:00, 12.61s/it]
2024-02-28 10:58:56,899 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.206, Train_accy 94.74, Test_accy 67.44
2024-02-28 10:58:56,903 [l2p_self_training.py] => pseudo labeling start
2024-02-28 10:59:14,475 [l2p_self_training.py] => wrong labeled samples count: 406
2024-02-28 10:59:14,477 [l2p_self_training.py] => previous label on future samples count: 136
2024-02-28 10:59:14,477 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 10:59:14,477 [l2p_self_training.py] => previous label on it sample count: 268
2024-02-28 10:59:14,477 [l2p_self_training.py] => 699 unlabeled samples will be pseudo labeled
2024-02-28 10:59:14,477 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 10:59:14,478 [toolkit.py] => Pseudo Accuracy: 0.41917024320457796
[70 71 72 73 74 75 76 77 78 79]
2024-02-28 10:59:14,479 [l2p_self_training.py] => train dataset length: 545
Task 7, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.186, Train_accy 95.23, Test_accy 68.65: 100%|██████████████████████████████████████████████████| 5/5 [01:12<00:00, 14.59s/it]
2024-02-28 11:00:27,407 [l2p_self_training.py] => Task 7, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.186, Train_accy 95.23, Test_accy 68.65
2024-02-28 11:00:27,410 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:00:44,961 [l2p_self_training.py] => wrong labeled samples count: 364
2024-02-28 11:00:44,963 [l2p_self_training.py] => previous label on future samples count: 124
2024-02-28 11:00:44,963 [l2p_self_training.py] => it label on future sample count: 7
2024-02-28 11:00:44,963 [l2p_self_training.py] => previous label on it sample count: 232
2024-02-28 11:00:44,963 [l2p_self_training.py] => 876 unlabeled samples will be pseudo labeled
2024-02-28 11:00:44,963 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:00:44,963 [toolkit.py] => Pseudo Accuracy: 0.5844748858447488
[70 71 72 73 74 75 77 78 79]
2024-02-28 11:01:31,198 [trainer.py] => No NME accuracy.
2024-02-28 11:01:31,198 [trainer.py] => CNN: {'total': 68.65, '00-09': 79.8, '10-19': 77.7, '20-29': 76.3, '30-39': 60.9, '40-49': 63.8, '50-59': 69.1, '60-69': 67.4, '70-79': 54.2, 'old': 70.71, 'new': 54.2}
2024-02-28 11:01:31,198 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46, 72.08, 71.17, 68.65]
2024-02-28 11:01:31,198 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92, 95.17, 94.79, 94.06]

Average Accuracy (CNN): 77.25374999999998
2024-02-28 11:01:31,198 [trainer.py] => Average Accuracy (CNN): 77.25374999999998 

2024-02-28 11:01:31,200 [trainer.py] => All params: 171816392
2024-02-28 11:01:31,201 [trainer.py] => Trainable params: 122980
2024-02-28 11:01:31,201 [l2p_self_training.py] => Learning on 80-90
2750
2024-02-28 11:01:31,251 [l2p_self_training.py] => train dataset length: 250
Task 8, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.055, Train_accy 90.80, Test_accy 64.39: 100%|██████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.01s/it]
2024-02-28 11:02:36,297 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.055, Train_accy 90.80, Test_accy 64.39
2024-02-28 11:02:36,302 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:02:52,365 [l2p_self_training.py] => wrong labeled samples count: 648
2024-02-28 11:02:52,367 [l2p_self_training.py] => previous label on future samples count: 46
2024-02-28 11:02:52,367 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 11:02:52,367 [l2p_self_training.py] => previous label on it sample count: 602
2024-02-28 11:02:52,367 [l2p_self_training.py] => 809 unlabeled samples will be pseudo labeled
2024-02-28 11:02:52,367 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:02:52,367 [toolkit.py] => Pseudo Accuracy: 0.19901112484548825
[81 82 83 85 86 87 88]
2024-02-28 11:02:52,368 [l2p_self_training.py] => train dataset length: 411
Task 8, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.160, Train_accy 94.16, Test_accy 66.01: 100%|██████████████████████████████████████████████████| 5/5 [01:11<00:00, 14.37s/it]
2024-02-28 11:04:04,197 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.160, Train_accy 94.16, Test_accy 66.01
2024-02-28 11:04:04,199 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:04:20,202 [l2p_self_training.py] => wrong labeled samples count: 574
2024-02-28 11:04:20,204 [l2p_self_training.py] => previous label on future samples count: 34
2024-02-28 11:04:20,205 [l2p_self_training.py] => it label on future sample count: 1
2024-02-28 11:04:20,205 [l2p_self_training.py] => previous label on it sample count: 538
2024-02-28 11:04:20,205 [l2p_self_training.py] => 930 unlabeled samples will be pseudo labeled
2024-02-28 11:04:20,205 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:04:20,205 [toolkit.py] => Pseudo Accuracy: 0.3827956989247312
[81 82 83 84 85 86 87 88]
2024-02-28 11:04:20,208 [l2p_self_training.py] => train dataset length: 608
Task 8, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.191, Train_accy 95.07, Test_accy 66.37: 100%|██████████████████████████████████████████████████| 5/5 [01:20<00:00, 16.12s/it]
2024-02-28 11:05:40,793 [l2p_self_training.py] => Task 8, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.191, Train_accy 95.07, Test_accy 66.37
2024-02-28 11:05:40,796 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:05:56,878 [l2p_self_training.py] => wrong labeled samples count: 481
2024-02-28 11:05:56,879 [l2p_self_training.py] => previous label on future samples count: 52
2024-02-28 11:05:56,879 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 11:05:56,879 [l2p_self_training.py] => previous label on it sample count: 429
2024-02-28 11:05:56,880 [l2p_self_training.py] => 1035 unlabeled samples will be pseudo labeled
2024-02-28 11:05:56,880 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:05:56,880 [toolkit.py] => Pseudo Accuracy: 0.5352657004830917
[80 81 82 83 84 85 86 87 88 89]
2024-02-28 11:06:48,728 [trainer.py] => No NME accuracy.
2024-02-28 11:06:48,728 [trainer.py] => CNN: {'total': 66.37, '00-09': 78.2, '10-19': 77.9, '20-29': 76.4, '30-39': 61.1, '40-49': 63.6, '50-59': 70.2, '60-69': 68.9, '70-79': 51.9, '80-89': 49.1, 'old': 68.53, 'new': 49.1}
2024-02-28 11:06:48,728 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46, 72.08, 71.17, 68.65, 66.37]
2024-02-28 11:06:48,728 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92, 95.17, 94.79, 94.06, 93.31]

Average Accuracy (CNN): 76.04444444444442
2024-02-28 11:06:48,728 [trainer.py] => Average Accuracy (CNN): 76.04444444444442 

2024-02-28 11:06:48,730 [trainer.py] => All params: 171816392
2024-02-28 11:06:48,731 [trainer.py] => Trainable params: 122980
2024-02-28 11:06:48,731 [l2p_self_training.py] => Learning on 90-100
2500
2024-02-28 11:06:48,779 [l2p_self_training.py] => train dataset length: 250
Task 9, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.044, Train_accy 88.80, Test_accy 64.34: 100%|██████████████████████████████████████████████████| 5/5 [01:11<00:00, 14.24s/it]
2024-02-28 11:07:59,996 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 0 => Loss -0.044, Train_accy 88.80, Test_accy 64.34
2024-02-28 11:08:00,000 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:08:14,657 [l2p_self_training.py] => wrong labeled samples count: 282
2024-02-28 11:08:14,659 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 11:08:14,659 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 11:08:14,659 [l2p_self_training.py] => previous label on it sample count: 282
2024-02-28 11:08:14,660 [l2p_self_training.py] => 604 unlabeled samples will be pseudo labeled
2024-02-28 11:08:14,660 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:08:14,660 [toolkit.py] => Pseudo Accuracy: 0.5331125827814569
[90 91 92 93 94 95 96 97 98]
2024-02-28 11:08:14,663 [l2p_self_training.py] => train dataset length: 572
Task 9, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.209, Train_accy 96.33, Test_accy 65.01: 100%|██████████████████████████████████████████████████| 5/5 [01:25<00:00, 17.01s/it]
2024-02-28 11:09:39,736 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 1 => Loss -0.209, Train_accy 96.33, Test_accy 65.01
2024-02-28 11:09:39,739 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:09:54,333 [l2p_self_training.py] => wrong labeled samples count: 250
2024-02-28 11:09:54,334 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 11:09:54,334 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 11:09:54,334 [l2p_self_training.py] => previous label on it sample count: 248
2024-02-28 11:09:54,334 [l2p_self_training.py] => 867 unlabeled samples will be pseudo labeled
2024-02-28 11:09:54,334 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:09:54,335 [toolkit.py] => Pseudo Accuracy: 0.7116493656286044
[90 91 93 94 95 96 97 98 99]
2024-02-28 11:09:54,338 [l2p_self_training.py] => train dataset length: 869
Task 9, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.181, Train_accy 95.28, Test_accy 65.68: 100%|██████████████████████████████████████████████████| 5/5 [01:37<00:00, 19.58s/it]
2024-02-28 11:11:32,220 [l2p_self_training.py] => Task 9, Epoch 5/5, Self_training_Iteration: 2 => Loss -0.181, Train_accy 95.28, Test_accy 65.68
2024-02-28 11:11:32,222 [l2p_self_training.py] => pseudo labeling start
2024-02-28 11:11:46,840 [l2p_self_training.py] => wrong labeled samples count: 176
2024-02-28 11:11:46,841 [l2p_self_training.py] => previous label on future samples count: 0
2024-02-28 11:11:46,841 [l2p_self_training.py] => it label on future sample count: 0
2024-02-28 11:11:46,841 [l2p_self_training.py] => previous label on it sample count: 169
2024-02-28 11:11:46,841 [l2p_self_training.py] => 1132 unlabeled samples will be pseudo labeled
2024-02-28 11:11:46,842 [l2p_self_training.py] => pseudo labeling finish
2024-02-28 11:11:46,842 [toolkit.py] => Pseudo Accuracy: 0.8445229681978799
[90 91 92 93 94 95 96 97 98 99]
2024-02-28 11:12:44,129 [trainer.py] => No NME accuracy.
2024-02-28 11:12:44,129 [trainer.py] => CNN: {'total': 65.68, '00-09': 72.8, '10-19': 78.4, '20-29': 76.8, '30-39': 61.5, '40-49': 63.1, '50-59': 69.0, '60-69': 67.6, '70-79': 52.0, '80-89': 48.8, '90-99': 66.8, 'old': 65.56, 'new': 66.8}
2024-02-28 11:12:44,130 [trainer.py] => CNN top1 curve: [94.2, 82.75, 80.77, 74.95, 73.46, 72.08, 71.17, 68.65, 66.37, 65.68]
2024-02-28 11:12:44,130 [trainer.py] => CNN top5 curve: [99.7, 97.15, 97.07, 96.58, 95.92, 95.17, 94.79, 94.06, 93.31, 92.71]

Average Accuracy (CNN): 75.008
2024-02-28 11:12:44,130 [trainer.py] => Average Accuracy (CNN): 75.008 

Accuracy Matrix (CNN):
[[94.2 90.  90.2 89.4 86.7 80.6 77.9 79.8 78.2 72.8]
 [ 0.  75.5 76.2 76.3 77.2 77.9 77.4 77.7 77.9 78.4]
 [ 0.   0.  75.9 76.1 76.7 76.8 77.  76.3 76.4 76.8]
 [ 0.   0.   0.  58.  59.1 58.8 59.4 60.9 61.1 61.5]
 [ 0.   0.   0.   0.  67.6 63.7 64.  63.8 63.6 63.1]
 [ 0.   0.   0.   0.   0.  74.7 71.6 69.1 70.2 69. ]
 [ 0.   0.   0.   0.   0.   0.  70.9 67.4 68.9 67.6]
 [ 0.   0.   0.   0.   0.   0.   0.  54.2 51.9 52. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.  49.1 48.8]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.  66.8]]
2024-02-28 11:12:44,131 [trainer.py] => Forgetting (CNN): 4.177777777777781
