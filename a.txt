Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensive experiments and in-depth analyses strongly demonstrate the effectiveness of our proposed model and training framework.1
n recent years, multimodal machine translation
(MMT) has achieved great progress and thus received increasing attention. Current studies on
MMT mainly focus on the text machine translation with scene images (Elliott et al., 2016; Calixto
et al., 2017a; Elliott and Kádár, 2017; Libovický
et al., 2018; Ive et al., 2019; Zhang et al., 2020;
Sulubacak et al., 2020). However, a more common
requirement for MMT in real-world applications
is text image translation (TIT) (Ma et al., 2022),
which aims to translate the source texts embedded
in the image to target translations. Due to its wide
Current studies on TIT face two main bottlenecks. First, this task lacks a publicly available TIT
dataset. Second, the common practice is to adopt
a cascaded translation system, where the texts embedded in the input image are firstly recognized
by an optical character recognition (OCR) model,
and then the recognition results are fed into a textonly neural machine translation (NMT) model for
translation. However, such a method tends to suffer from the problem of OCR error propagation,
and thus often generates unsatisfactory translations.
As shown in Figure 1, “富锦消防” ("fu jin xiao
fang”) in the image is incorrectly recognized as
“富锦消阳” (“fu jin xiao yang”). Consequently,
the text-only NMT model incorrectly translates
it into “Fujin Xiaoyang”. Furthermore, we use
the commonly-used PaddleOCR2 to handle several
OCR benchmark datasets. As reported in Table 1,
we observe that the highest recognition accuracy
at the image level is less than 67% and that at the
sentence level is not higher than 81%. It can be said
that OCR errors are very common, thus they have a
serious negative impact on subsequent translation.
In this paper, we first manually annotate a
Chinese-English TIT dataset named OCRMT30K,
providing convenience for subsequent studies. This
dataset is developed based on five Chinese OCR
datasets, including about 30,000 image-text pairs.
Besides, we propose a TIT model with a multimodal codebook to alleviate the OCR error propagation problem. The basic intuition behind our
model is that when humans observe the incorrectly
recognized text in an image, they can still associate
the image with relevant or correct texts, which can
provide useful supplementary information for translation. Figure 3 shows the basic architecture of our
model, which mainly consists of four modules: 1)
a text encoder that converts the input text into a
hidden state sequence; 2) an image encoder encoding the input image as a visual vector sequence;
3) a multimodal codebook. This module can be
described as a vocabulary comprising latent codes,
each of which represents a cluster. It is trained to
map the input images and ground-truth texts into
the shared semantic space of latent codes. During
inference, this module is fed with the input image
and then outputs latent codes containing the text
information related to ground-truth texts. 4) a text
decoder that is fed with the combined representation of the recognized text and the outputted latent
codes, and then generates the final translation.
Moreover, we propose a multi-stage training
framework for our TIT model, which can fully
exploit additional bilingual texts and OCR data for
model training. Specifically, our framework consists of four stages. First, we use a large-scale
bilingual corpus to pretrain the text encoder and
text decoder. Second, we pretrain the newly added
multimodal codebook on a large-scale monolingual
corpus. Third, we further introduce an image encoder that includes a pretrained vision Transformer
with fixed parameters to extract visual features, and
continue to train the multimodal codebook. Additionally, we introduce an image-text alignment task
to enhance the ability of the multimodal codebook
in associating images with related texts. Finally,
we finetune the entire model on the OCRMT30K
dataset. Particularly, we maintain the image-text
alignment task at this stage to reduce the gap between the third and fourth training stages.
Our main contributions are as follows:
• We release an OCRMT30K dataset, which is
the first Chinese-English TIT dataset, prompting the subsequent studies.
• We present a TIT model with a multimodal
codebook, which can leverage the input image to generate the information of relevant or
correct texts, providing useful information for
the subsequent translation.
• We propose a multi-stage training framework
for our model, which effectively leverages additional bilingual texts and OCR data to enhance the model training.
• Extensive experiments and analyses demonstrate the effectiveness of our model and training framework.