Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensive experiments and in-depth analyses strongly demonstrate the effectiveness of our proposed model and training framework.1n recent years, multimodal machine translation(MMT) has achieved great progress and thus received increasing attention. Current studies onMMT mainly focus on the text machine translation with scene images (Elliott et al., 2016; Calixtoet al., 2017a; Elliott and Kádár, 2017; Libovickýet al., 2018; Ive et al., 2019; Zhang et al., 2020;Sulubacak et al., 2020). However, a more commonrequirement for MMT in real-world applicationsis text image translation (TIT) (Ma et al., 2022),which aims to translate the source texts embeddedin the image to target translations. Due to its wideCurrent studies on TIT face two main bottlenecks. First, this task lacks a publicly available TITdataset. Second, the common practice is to adopta cascaded translation system, where the texts embedded in the input image are firstly recognizedby an optical character recognition (OCR) model,and then the recognition results are fed into a textonly neural machine translation (NMT) model fortranslation. However, such a method tends to suffer from the problem of OCR error propagation,and thus often generates unsatisfactory translations.As shown in Figure 1, “富锦消防” ("fu jin xiaofang”) in the image is incorrectly recognized as“富锦消阳” (“fu jin xiao yang”). Consequently,the text-only NMT model incorrectly translatesit into “Fujin Xiaoyang”. Furthermore, we usethe commonly-used PaddleOCR2 to handle severalOCR benchmark datasets. As reported in Table 1,we observe that the highest recognition accuracyat the image level is less than 67% and that at thesentence level is not higher than 81%. It can be saidthat OCR errors are very common, thus they have aserious negative impact on subsequent translation.In this paper, we first manually annotate aChinese-English TIT dataset named OCRMT30K,providing convenience for subsequent studies. Thisdataset is developed based on five Chinese OCRdatasets, including about 30,000 image-text pairs.Besides, we propose a TIT model with a multimodal codebook to alleviate the OCR error propagation problem. The basic intuition behind ourmodel is that when humans observe the incorrectlyrecognized text in an image, they can still associatethe image with relevant or correct texts, which canprovide useful supplementary information for translation. Figure 3 shows the basic architecture of ourmodel, which mainly consists of four modules: 1)a text encoder that converts the input text into ahidden state sequence; 2) an image encoder encoding the input image as a visual vector sequence;3) a multimodal codebook. This module can bedescribed as a vocabulary comprising latent codes,each of which represents a cluster. It is trained tomap the input images and ground-truth texts intothe shared semantic space of latent codes. Duringinference, this module is fed with the input imageand then outputs latent codes containing the textinformation related to ground-truth texts. 4) a textdecoder that is fed with the combined representation of the recognized text and the outputted latentcodes, and then generates the final translation.Moreover, we propose a multi-stage trainingframework for our TIT model, which can fullyexploit additional bilingual texts and OCR data formodel training. Specifically, our framework consists of four stages. First, we use a large-scalebilingual corpus to pretrain the text encoder andtext decoder. Second, we pretrain the newly addedmultimodal codebook on a large-scale monolingualcorpus. Third, we further introduce an image encoder that includes a pretrained vision Transformerwith fixed parameters to extract visual features, andcontinue to train the multimodal codebook. Additionally, we introduce an image-text alignment taskto enhance the ability of the multimodal codebookin associating images with related texts. Finally,we finetune the entire model on the OCRMT30Kdataset. Particularly, we maintain the image-textalignment task at this stage to reduce the gap between the third and fourth training stages.Our main contributions are as follows:• We release an OCRMT30K dataset, which isthe first Chinese-English TIT dataset, prompting the subsequent studies.• We present a TIT model with a multimodalcodebook, which can leverage the input image to generate the information of relevant orcorrect texts, providing useful information forthe subsequent translation.• We propose a multi-stage training frameworkfor our model, which effectively leverages additional bilingual texts and OCR data to enhance the model training.• Extensive experiments and analyses demonstrate the effectiveness of our model and training framework.